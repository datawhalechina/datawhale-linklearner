# ************************************************************
# Sequel Ace SQL dump
# 版本号： 20016
#
# https://sequel-ace.com/
# https://github.com/Sequel-Ace/Sequel-Ace
#
# 主机: 127.0.0.1 (MySQL 8.0.27)
# 数据库: datawhale
# 生成时间: 2021-12-19 13:50:37 +0000
# ************************************************************


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
SET NAMES utf8mb4;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE='NO_AUTO_VALUE_ON_ZERO', SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;


# 转储表 activity
# ------------------------------------------------------------

CREATE TABLE `activity` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '活动id',
  `name` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '活动名称',
  `description` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '活动描述',
  `registration_link` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '报名链接',
  `registration_end_time` timestamp NULL DEFAULT NULL COMMENT '报名截止时间',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='活动表';

LOCK TABLES `activity` WRITE;
/*!40000 ALTER TABLE `activity` DISABLE KEYS */;

INSERT INTO `activity` (`id`, `name`, `description`, `registration_link`, `registration_end_time`, `create_time`, `modify_time`)
VALUES
	(1,'一起来啃南瓜书','快来参加吧','http://www.shadowingszy.top','2021-12-07 00:00:00','2021-12-05 00:00:00','2021-12-10 13:11:26'),
	(2,'校招面试常见问题盘点','快来参加吧','http://www.shadowingszy.top','2021-12-10 00:00:00','2021-12-05 01:00:00','2021-12-10 13:11:42'),
	(3,'深度学习入门','快来参加吧','http://www.shadowingszy.top','2021-12-13 00:00:00','2021-12-05 01:00:00','2021-12-10 13:11:49'),
	(4,'服务端开发组队学习','快来参加吧','http://www.shadowingszy.top','2021-11-10 00:00:00','2021-11-05 01:00:00','2021-12-10 13:11:59'),
	(5,'前端开发组队学习','快来参加吧','http://www.shadowingszy.top','2021-12-15 00:00:00','2021-11-05 01:00:00','2021-12-10 13:12:04');

/*!40000 ALTER TABLE `activity` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 banner
# ------------------------------------------------------------

CREATE TABLE `banner` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '轮播图id',
  `image_url` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '轮播图片链接',
  `description` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '轮播图描述',
  `status` int DEFAULT NULL COMMENT '轮播图状态，10可用，20不可用',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='轮播图内容表';

LOCK TABLES `banner` WRITE;
/*!40000 ALTER TABLE `banner` DISABLE KEYS */;

INSERT INTO `banner` (`id`, `image_url`, `description`, `status`, `create_time`, `modify_time`)
VALUES
	(1,'http://www.shadowingszy.top/homepage/assets/images/image1.jpg','2021年最后一次组队学习，共11个内容。',10,'2021-12-05 00:00:00','2021-12-19 20:16:11'),
	(2,'http://www.shadowingszy.top/homepage/assets/images/image2.jpg','测试banner2',10,'2021-12-05 00:00:00','2021-12-05 18:34:19');

/*!40000 ALTER TABLE `banner` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 knowledge
# ------------------------------------------------------------

CREATE TABLE `knowledge` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '知识体系id',
  `name` varchar(512) DEFAULT NULL COMMENT '知识体系名称',
  `content` text CHARACTER SET utf8mb4 COLLATE utf8mb4_bin COMMENT '知识体系内容，json格式',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='知识体系表';

LOCK TABLES `knowledge` WRITE;
/*!40000 ALTER TABLE `knowledge` DISABLE KEYS */;

INSERT INTO `knowledge` (`id`, `name`, `content`, `create_time`, `modify_time`)
VALUES
	(1,'数据分析',X'5B0A20207B0A20202020226E616D65223A2022E695B0E68DAEE58886E69E90E59FBAE7A180222C0A2020202022636F6E74656E74223A205B0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D2C0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D2C0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D0A202020205D0A20207D2C0A20207B0A20202020226E616D65223A2022E695B0E68DAEE58886E69E90E8BF9BE998B6222C0A2020202022636F6E74656E74223A205B0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D2C0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D2C0A2020202020207B0A2020202020202020227469746C65223A2022E69687E5AD97E4BB8BE7BB8D222C0A202020202020202022636F6E74657874223A2022E8BF99E698AFE58685E5AEB9222C0A202020202020202022696D67223A2022220A2020202020207D0A202020205D0A20207D2C0A20207B0A20202020226E616D65223A2022E6AF95E4B89AE4BA86222C0A2020202022636F6E74656E74223A205B5D0A20207D0A5D','2021-12-19 16:28:21','2021-12-19 20:51:16'),
	(2,'深度学习',X'5B7B226E616D65223A22E6B7B1E5BAA6E5ADA6E4B9A0E59FBAE7A180222C22636F6E74656E74223A5B7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D2C7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D2C7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D5D7D2C7B226E616D65223A22E6B7B1E5BAA6E5ADA6E4B9A0E8BF9BE998B6222C22636F6E74656E74223A5B7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D2C7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D2C7B227469746C65223A22E69687E5AD97E4BB8BE7BB8D222C22636F6E74657874223A22E8BF99E698AFE58685E5AEB9222C22696D67223A22227D5D7D5D','2021-12-19 16:28:21','2021-12-19 20:05:40');

/*!40000 ALTER TABLE `knowledge` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 learn
# ------------------------------------------------------------

CREATE TABLE `learn` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '学习id',
  `name` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '学习名称',
  `description` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '学习描述',
  `image_url` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '学习封面图',
  `like` int DEFAULT NULL COMMENT '学习点赞数',
  `license` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '开源协议',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='学习表';

LOCK TABLES `learn` WRITE;
/*!40000 ALTER TABLE `learn` DISABLE KEYS */;

INSERT INTO `learn` (`id`, `name`, `description`, `image_url`, `like`, `license`, `create_time`, `modify_time`)
VALUES
	(1,'南瓜书PumpkinBook','人工智能领域中文的开山之作、周志华“西瓜书”《机器学习》伴侣书，Datawhale开源协作学习笔记“南瓜书”，机器学习初学小白提升数学基础能力的练习书！','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book.jpeg',50,'MIT','2021-12-05 00:00:00','2021-12-19 20:02:39'),
	(2,'Joyful-Pandas','本教程共有十章，可分为三大模块：基础知识、四类操作、四类数据，涵盖了pandas的所有核心操作与特性。','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas.jpeg',20,'MIT','2021-12-05 00:00:00','2021-12-19 20:02:48'),
	(3,'EasyRL','李宏毅老师的《深度强化学习》是强化学习领域经典的中文视频之一。对于想入门强化学习又想看中文讲解的人来说绝对是非常推荐的。','http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl.jpeg',10,'MIT','2021-12-05 00:00:00','2021-12-19 20:03:00');

/*!40000 ALTER TABLE `learn` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 learn_detail
# ------------------------------------------------------------

CREATE TABLE `learn_detail` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '学习章节id',
  `learn_id` int DEFAULT NULL COMMENT '学习id',
  `title` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '章节标题',
  `content` mediumtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci COMMENT '章节内容，md5格式',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='学习章节内容表';

LOCK TABLES `learn_detail` WRITE;
/*!40000 ALTER TABLE `learn_detail` DISABLE KEYS */;

INSERT INTO `learn_detail` (`id`, `learn_id`, `title`, `content`, `create_time`, `modify_time`)
VALUES
	(1,1,'chapter1','## 1.1\n$$E_{o t e}\\left(\\mathfrak{L}_{a} | X, f\\right)=\\sum_{h} \\sum_{\\boldsymbol{x} \\in \\mathcal{X}-X} P(\\boldsymbol{x}) \\mathbb{I}(h(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) P\\left(h | X, \\mathfrak{L}_{a}\\right)$$\n[解析]：参见公式(1.2)\n\n## 1.2\n$$\\begin{aligned}\n\\sum_{f}E_{ote}(\\mathfrak{L}_a\\vert X,f) &= \\sum_f\\sum_h\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x})\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x}))P(h\\vert X,\\mathfrak{L}_a) \\\\\n&=\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x}) \\sum_hP(h\\vert X,\\mathfrak{L}_a)\\sum_f\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x})) \\\\\n&=\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x}) \\sum_hP(h\\vert X,\\mathfrak{L}_a)\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert} \\\\\n&=\\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert}\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x}) \\sum_hP(h\\vert X,\\mathfrak{L}_a) \\\\\n&=2^{\\vert \\mathcal{X} \\vert-1}\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x}) \\cdot 1\\\\\n\\end{aligned}$$\n[解析]：第1步到第2步：\n$$\\begin{aligned}\n&\\sum_f\\sum_h\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x})\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x}))P(h\\vert X,\\mathfrak{L}_a) \\\\\n&=\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x})\\sum_f\\sum_h\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x}))P(h\\vert X,\\mathfrak{L}_a) \\\\\n&=\\sum_{\\boldsymbol{x}\\in\\mathcal{X}-X}P(\\boldsymbol{x}) \\sum_hP(h\\vert X,\\mathfrak{L}_a)\\sum_f\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x})) \\\\\n\\end{aligned}$$\n第2步到第3步：首先要知道此时我们对$f$的假设是任何能将样本映射到{0,1}的函数且服从均匀分布，也就是说不止一个$f$且每个$f$出现的概率相等，例如样本空间只有两个样本时：$ \\mathcal{X}=\\{\\boldsymbol{x}_1,\\boldsymbol{x}_2\\},\\vert \\mathcal{X} \\vert=2$，那么所有的真实目标函数$f$为：\n$$\\begin{aligned}\nf_1:f_1(\\boldsymbol{x}_1)=0,f_1(\\boldsymbol{x}_2)=0;\\\\\nf_2:f_2(\\boldsymbol{x}_1)=0,f_2(\\boldsymbol{x}_2)=1;\\\\\nf_3:f_3(\\boldsymbol{x}_1)=1,f_3(\\boldsymbol{x}_2)=0;\\\\\nf_4:f_4(\\boldsymbol{x}_1)=1,f_4(\\boldsymbol{x}_2)=1;\n\\end{aligned}$$\n一共$2^{\\vert \\mathcal{X} \\vert}=2^2=4$个真实目标函数。所以此时通过算法$\\mathfrak{L}_a$学习出来的模型$h(\\boldsymbol{x})$对每个样本无论预测值为0还是1必然有一半的$f$与之预测值相等，例如，现在学出来的模型$h(\\boldsymbol{x})$对$\\boldsymbol{x}_1$的预测值为1，也即$h(\\boldsymbol{x}_1)=1$，那么有且只有$f_3$和$f_4$与$h(\\boldsymbol{x})$的预测值相等，也就是有且只有一半的$f$与它预测值相等，所以$\\sum_f\\mathbb{I}(h(\\boldsymbol{x})\\neq f(\\boldsymbol{x})) = \\cfrac{1}{2}2^{\\vert \\mathcal{X} \\vert}$；第3步一直到最后显然成立。值得一提的是，在这里我们假设真实的目标函数$f$为“任何能将样本映射到{0,1}的函数且服从均匀分布”，但是实际情形并非如此，通常我们只认为能高度拟合已有样本数据的函数才是真实目标函数，例如，现在已有的样本数据为$\\{(\\boldsymbol{x}_1,0),(\\boldsymbol{x}_2,1)\\}$，那么此时$f_2$才是我们认为的真实目标函数，由于没有收集到或者压根不存在$\\{(\\boldsymbol{x}_1,0),(\\boldsymbol{x}_2,0)\\},\\{(\\boldsymbol{x}_1,1),(\\boldsymbol{x}_2,0)\\},\\{(\\boldsymbol{x}_1,1),(\\boldsymbol{x}_2,1)\\}$这类样本，所以$f_1,f_3,f_4$都不算是真实目标函数。这也就是西瓜书公式(1.3)下面的第3段话举的“骑自行车”的例子所想表达的内容。','2021-12-10 12:43:49','2021-12-19 13:30:57'),
	(2,1,'chapter2','## 2.20\n$$\\text{AUC}=\\frac{1}{2}\\sum_{i=1}^{m-1}(x_{i+1} - x_i)\\cdot(y_i + y_{i+1})$$\n[解析]：在解释$\\text{AUC}$公式之前，我们需要先弄清楚$\\text{ROC}$曲线的具体绘制过程，下面我们就举个例子，按照西瓜书图2.4下方给出的绘制方法来讲解一下$\\text{ROC}$曲线的具体绘制过程。假设我们已经训练得到一个学习器$f(s)$，现在用该学习器来对我们的8个测试样本（4个正例，4个反例，也即$m^+=m^-=4$）进行预测，假设预测结果为：\n$$(s_1,0.77,+),(s_2,0.62,-),(s_3,0.58,+),(s_4,0.47,+),(s_5,0.47,-),(s_6,0.33,-),(s_7,0.23,+),(s_8,0.15,-)$$\n其中，$+$和$-$分别表示为正例和为反例，里面的数字表示学习器$f(s)$预测该样本为正例的概率，例如对于反例$s_2$来说，当前学习器$f(s)$预测它是正例的概率为$0.62$。根据西瓜书上给出的绘制方法可知，首先需要对所有测试样本按照学习器给出的预测结果进行排序（上面给出的预测结果已经按照预测值从大到小排好），接着将分类阈值设为一个不可能取到的最大值，显然这时候所有样本预测为正例的概率都一定小于分类阈值，那么预测为正例的样本个数为0，相应的真正例率和假正例率也都为0，所以此时我们可以在坐标$(0,0)$处打一个点。接下来我们需要把分类阈值从大到小依次设为每个样本的预测值，也就是依次设为$0.77、0.62、0.58、0.47、0.33、0.23、0.15$，然后每次计算真正例率和假正例率，再在相应的坐标上打一个点，最后再将各个点用直线串连起来即可得到$\\text{ROC}$曲线。需要注意的是，在统计预测结果时，预测值等于分类阈值的样本也算作预测为正例。例如，当分类阈值为$0.77$时，测试样本$s_1$被预测为正例，由于它的真实标记也是正例，所以此时$s_1$是一个真正例。为了便于绘图，我们将$x$轴（假正例率轴）的“步长”定为$\\frac{1}{m^-}$，$y$轴（真正例率轴）的“步长”定为$\\frac{1}{m^+}$，这样的话，根据真正例率和假正例率的定义可知，每次变动分类阈值时，若新增$i$个假正例，那么相应的$x$轴坐标也就增加$\\frac{i}{m^-}$，同理，若新增$j$个真正例，那么相应的$y$轴坐标也就增加$\\frac{j}{m^+}$。按照以上讲述的绘制流程，最终我们可以绘制出如下图所示的$\\text{ROC}$曲线\n\n<center><img src=\"http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book/roc.png\" width= \"300\"/></center>\n\n在这里我们为了能在解析公式(2.21)时复用此图所以没有写上具体地数值，转而用其数学符号代替。其中绿色线段表示在分类阈值变动的过程中只新增了真正例，红色线段表示只新增了假正例，蓝色线段表示既新增了真正例也新增了假正例。根据$\\text{AUC}$值的定义可知，此时的$\\text{AUC}$值其实就是所有红色线段和蓝色线段与$x$轴围成的面积之和。观察上图可知，红色线段与$x$轴围成的图形恒为矩形，蓝色线段与$x$轴围成的图形恒为梯形，但是由于梯形面积公式既能算梯形面积，也能算矩形面积，所以无论是红色线段还是蓝色线段，其与$x$轴围成的面积都能用梯形公式来计算，也即\n$$\\frac{1}{2}\\cdot(x_{i+1} - x_i)\\cdot(y_i + y_{i+1})$$\n其中，$(x_{i+1} - x_i)$表示“高”，$y_i$表示“上底”，$y_{i+1}$表示“下底”。那么\n$$\\sum_{i=1}^{m-1}\\left[\\frac{1}{2}\\cdot(x_{i+1} - x_i)\\cdot(y_i + y_{i+1})\\right]$$\n表示的便是对所有红色线段和蓝色线段与$x$轴围成的面积进行求和，此即为$\\text{AUC}$\n\n## 2.21\n$$\\ell_{rank}=\\frac{1}{m^+m^-}\\sum_{\\boldsymbol{x}^+ \\in D^+}\\sum_{\\boldsymbol{x}^- \\in D^-}\\left(\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{2}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right)$$\n[解析]：按照我们上述对公式(2.20)的解析思路，$\\ell_{rank}$可以看作是所有绿色线段和蓝色线段与$y$轴围成的面积之和，但是公式(2.21)很难一眼看出其面积的具体计算方式，因此我们需要将公式(2.21)进行恒等变形\n$$\\begin{aligned}\n\\ell_{rank}&=\\frac{1}{m^+m^-}\\sum_{\\boldsymbol{x}^+ \\in D^+}\\sum_{\\boldsymbol{x}^- \\in D^-}\\left(\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{2}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right) \\\\\n&=\\frac{1}{m^+m^-}\\sum_{\\boldsymbol{x}^+ \\in D^+}\\left[\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{2}\\cdot\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right] \\\\\n&=\\sum_{\\boldsymbol{x}^+ \\in D^+}\\left[\\frac{1}{m^+}\\cdot\\frac{1}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{2}\\cdot\\frac{1}{m^+}\\cdot\\frac{1}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right] \\\\\n&=\\sum_{\\boldsymbol{x}^+ \\in D^+}\\frac{1}{2}\\cdot\\frac{1}{m^+}\\cdot\\left[\\frac{2}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right] \\\\\n\\end{aligned}$$\n根据公式(2.20)中给出的$\\text{ROC}$曲线图可知，在变动分类阈值的过程当中，如果有新增真正例，那么相应地就会增加一条绿色线段或蓝色线段，所以上式中的$\\sum\\limits_{\\boldsymbol{x}^+ \\in D^+}$可以看作是在遍历所有绿色和蓝色线段，那么相应地$\\sum\\limits_{\\boldsymbol{x}^+ \\in D^+}$后面的那一项便是在求绿色线段或者蓝色线段与$y$轴围成的面积，也即\n$$\\frac{1}{2}\\cdot\\frac{1}{m^+}\\cdot\\left[\\frac{2}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\frac{1}{m^-}\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right]$$\n同公式(2.20)中的求解思路一样，不论是绿色线段还是蓝色线段，其与$y$轴围成的图形面积都可以用梯形公式来进行计算，所以上式表示的依旧是一个梯形的面积求解公式。其中$\\frac{1}{m^+}$即为梯形的“高”，中括号中的那一项便是“上底+下底”，下面我们来分别推导一下“上底”（较短的那个底）和“下底”。由于在绘制$\\text{ROC}$曲线的过程中，每新增一个假正例时$x$坐标也就新增一个单位，所以对于“上底”，也就是绿色或者蓝色线段的下端点到$y$轴的距离，它就等于$\\frac{1}{m^-}$乘以预测值比$\\boldsymbol{x^+}$大的假正例的个数，也即\n$$\\frac{1}{m^-}\\sum\\limits_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)$$\n而对于“下底”，它就等于$\\frac{1}{m^-}$乘以预测值大于等于$\\boldsymbol{x^+}$的假正例的个数，也即\n$$\\frac{1}{m^-}\\left(\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)<f(\\boldsymbol{x}^-)\\right)+\\sum_{\\boldsymbol{x}^- \\in D^-}\\mathbb{I}\\left(f(\\boldsymbol{x}^+)=f(\\boldsymbol{x}^-)\\right)\\right)$$\n\n## 2.27\n\n$$\\overline{\\epsilon}=\\max \\epsilon\\quad \\text { s.t. } \\sum_{i= \\epsilon_{0} \\times m+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) \\epsilon^{i}(1-\\epsilon)^{m-i}<\\alpha$$\n\n[推导]：截至2018年12月，第一版第30次印刷，公式（2.27）应当勘误修正为\n$$\\overline{\\epsilon}=\\min \\epsilon\\quad\\text { s.t. } \\sum_{i=\\epsilon\\times m+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) \\epsilon_0^{i}(1-\\epsilon_0)^{m-i}<\\alpha$$\n具体推导过程如下：由西瓜书中的上下文可知，对$\\epsilon\\leq\\epsilon_0$进行假设检验，等价于附录①中所述的对$p\\leq p_0$进行假设检验，所以在西瓜书中求解最大错误率$\\overline{\\epsilon}$等价于在附录①中求解事件最大发生频率$\\frac{\\overline{C}}{m}$。由附录①可知\n$$\\overline{C}=\\min C\\quad\\text { s.t. } \\sum_{i=C+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i}(1-p_0)^{m-i}<\\alpha$$\n所以\n$$\\frac{\\overline{C}}{m}=\\min \\frac{C}{m}\\quad\\text { s.t. } \\sum_{i=C+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i}(1-p_0)^{m-i}<\\alpha$$\n将上式中的$\\frac{\\overline{C}}{m},\\frac{C}{m},p_0$等价替换为$\\overline{\\epsilon},\\epsilon,\\epsilon_0$可得\n$$\\overline{\\epsilon}=\\min \\epsilon\\quad\\text { s.t. } \\sum_{i=\\epsilon\\times m+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) \\epsilon_0^{i}(1-\\epsilon_0)^{m-i}<\\alpha$$\n\n## 2.41\n\n$$\\begin{aligned} \nE(f ; D)=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-y_{D}\\right)^{2}\\right] \\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})+\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] \\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] \\\\ &+\\mathbb{E}_{D}\\left[+2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)\\right] \\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] \\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y+y-y_{D}\\right)^{2}\\right] \\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(y-y_{D}\\right)^{2}\\right]\\\\ &+2 \\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y\\right)\\left(y-y_{D}\\right)\\right]\\\\\n=& \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\left(\\bar{f}(\\boldsymbol{x})-y\\right)^{2}+\\mathbb{E}_{D}\\left[\\left(y_{D}-y\\right)^{2}\\right] \\end{aligned}$$\n\n[解析]：\n- 第1-2步：减一个$\\bar{f}(\\boldsymbol{x})$再加一个$\\bar{f}(\\boldsymbol{x})$，属于简单的恒等变形；\n- 第2-3步：首先将中括号里面的式子展开\n$$\\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}+\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}+2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)\\right]$$\n然后根据期望的运算性质：$\\mathbb{E}[X+Y]=\\mathbb{E}[X]+\\mathbb{E}[Y]$可将上式化为\n$$ \\mathbb{E}_{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)^{2}\\right]+\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] +\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)\\right]$$\n- 第3-4步：再次利用期望的运算性质将第3步得到的式子的最后一项展开\n$$\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)\\right] = \\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot\\bar{f}(\\boldsymbol{x})\\right] - \\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot y_{D}\\right]$$\n	- 首先计算展开后得到的第一项\n$$\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot\\bar{f}(\\boldsymbol{x})\\right] = \\mathbb{E}_{D}\\left[2f(\\boldsymbol{x} ; D)\\cdot\\bar{f}(\\boldsymbol{x})-2\\bar{f}(\\boldsymbol{x})\\cdot\\bar{f}(\\boldsymbol{x})\\right]$$\n由于$\\bar{f}(\\boldsymbol{x})$是常量，所以由期望的运算性质：$\\mathbb{E}[AX+B]=A\\mathbb{E}[X]+B$（其中$A,B$均为常量）可得\n$$\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot\\bar{f}(\\boldsymbol{x})\\right] = 2\\bar{f}(\\boldsymbol{x})\\cdot\\mathbb{E}_{D}\\left[f(\\boldsymbol{x} ; D)\\right]-2\\bar{f}(\\boldsymbol{x})\\cdot\\bar{f}(\\boldsymbol{x})$$\n由公式（2.37）可知：$\\mathbb{E}_{D}\\left[f(\\boldsymbol{x} ; D)\\right]=\\bar{f}(\\boldsymbol{x})$，所以\n$$\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot\\bar{f}(\\boldsymbol{x})\\right] = 2\\bar{f}(\\boldsymbol{x})\\cdot\\bar{f}(\\boldsymbol{x})-2\\bar{f}(\\boldsymbol{x})\\cdot\\bar{f}(\\boldsymbol{x})=0$$\n	- 接着计算展开后得到的第二项\n$$\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot y_{D}\\right]=2\\mathbb{E}_{D}\\left[f(\\boldsymbol{x} ; D)\\cdot y_{D}\\right]-2\\bar{f}(\\boldsymbol{x})\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right]$$\n由于噪声和$f$无关，所以$f(\\boldsymbol{x} ; D)$和$y_D$是两个相互独立的随机变量，所以根据期望的运算性质：$\\mathbb{E}[XY]=\\mathbb{E}[X]\\mathbb{E}[Y]$（其中$X$和$Y$为相互独立的随机变量）可得\n$$\\begin{aligned} \n\\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot y_{D}\\right]&=2\\mathbb{E}_{D}\\left[f(\\boldsymbol{x} ; D)\\cdot y_{D}\\right]-2\\bar{f}(\\boldsymbol{x})\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right] \\\\\n&=2\\mathbb{E}_{D}\\left[f(\\boldsymbol{x} ; D)\\right]\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right]-2\\bar{f}(\\boldsymbol{x})\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right] \\\\\n&=2\\bar{f}(\\boldsymbol{x})\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right]-2\\bar{f}(\\boldsymbol{x})\\cdot \\mathbb{E}_{D}\\left[y_{D}\\right] \\\\\n&= 0\n\\end{aligned}$$\n所以\n$$\\begin{aligned} \\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)\\right] &= \\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot\\bar{f}(\\boldsymbol{x})\\right] - \\mathbb{E}_{D}\\left[2\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})\\right)\\cdot y_{D}\\right] \\\\\n&= 0+0 \\\\\n&=0\n\\end{aligned}$$\n- 第4-5步：同第1-2步一样，减一个$y$再加一个$y$，属于简单的恒等变形；\n- 第5-6步：同第2-3步一样，将最后一项利用期望的运算性质进行展开；\n- 第6-7步：因为$\\bar{f}(\\boldsymbol{x})$和$y$均为常量，所以根据期望的运算性质可知，第6步中的第2项可化为\n$$\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y\\right)^{2}\\right]=\\left(\\bar{f}(\\boldsymbol{x})-y\\right)^{2}$$\n同理，第6步中的最后一项可化为\n$$2\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y\\right)\\left(y-y_{D}\\right)\\right]=2\\left(\\bar{f}(\\boldsymbol{x})-y\\right)\\mathbb{E}_{D}\\left[\\left(y-y_{D}\\right)\\right]$$\n由于此时假设噪声的期望为零，也即$\\mathbb{E}_{D}\\left[\\left(y-y_{D}\\right)\\right]=0$，所以\n$$2\\mathbb{E}_{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y\\right)\\left(y-y_{D}\\right)\\right]=2\\left(\\bar{f}(\\boldsymbol{x})-y\\right)\\cdot 0=0$$\n\n## 附录\n### ①二项分布参数$p$的检验<sup>[1]</sup>\n设某事件发生的概率为$p$，$p$未知，作$m$次独立试验，每次观察该事件是否发生，以$X$记该事件发生的次数，则$X$服从二项分布$B(m,p)$，现根据$X$检验如下假设：\n$$\\begin{aligned}\nH_0:p\\leq p_0\\\\\nH_1:p > p_0\n\\end{aligned}$$\n由二项分布本身的特性可知：$p$越小，$X$取到较小值的概率越大。因此，对于上述假设，一个直观上合理的检验为\n$$\\varphi:\\text{当}X\\leq C\\text{时接受}H_0,\\text{否则就拒绝}H_0$$\n其中，$C\\in N$表示事件最大发生次数。此检验对应的功效函数为\n$$\\begin{aligned}\n\\beta_{\\varphi}(p)&=P(X>C)\\\\\n&=1-P(X\\leq C) \\\\\n&=1-\\sum_{i=0}^{C}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p^{i} (1-p)^{m-i} \\\\\n&=\\sum_{i=C+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p^{i} (1-p)^{m-i} \\\\\n\\end{aligned}$$\n由于“$p$越小，$X$取到较小值的概率越大”可以等价表示为：$P(X\\leq C)$是关于$p$的减函数（更为严格的数学证明参见参考文献[1]中第二章习题7），所以$\\beta_{\\varphi}(p)=P(X>C)=1-P(X\\leq C)$是关于$p$的增函数，那么当$p\\leq p_0$时，$\\beta_{\\varphi}(p_0)$即为$\\beta_{\\varphi}(p)$的上确界。又因为，根据参考文献[1]中5.1.3的定义1.2可知，检验水平$\\alpha$默认取最小可能的水平，所以在给定检验水平$\\alpha$时，可以通过如下方程解得满足检验水平$\\alpha$的整数$C$：\n$$\\alpha =\\sup \\left\\{\\beta_{\\varphi}(p)\\right\\}$$\n显然，当$p\\leq p_0$时：\n$$\\begin{aligned}\n\\alpha &=\\sup \\left\\{\\beta_{\\varphi}(p)\\right\\} \\\\\n&=\\beta_{\\varphi}(p_0) \\\\\n&=\\sum_{i=C+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i} (1-p_0)^{m-i}\n\\end{aligned}$$\n对于此方程，通常不一定正好解得一个整数$C$使得方程成立，较常见的情况是存在这样一个$\\overline{C}$使得\n$$\\begin{aligned}\n\\sum_{i=\\overline{C}+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i} (1-p_0)^{m-i}<\\alpha \\\\\n\\sum_{i=\\overline{C}}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i} (1-p_0)^{m-i}>\\alpha\n\\end{aligned}$$\n此时，$C$只能取$\\overline{C}$或者$\\overline{C}+1$，若$C$取$\\overline{C}$，则相当于升高了检验水平$\\alpha$，若$C$取$\\overline{C}+1$则相当于降低了检验水平$\\alpha$，具体如何取舍需要结合实际情况，但是通常为了减小犯第一类错误的概率，会倾向于令$C$取$\\overline{C}+1$。下面考虑如何求解$\\overline{C}$：易证$\\beta_{\\varphi}(p_0)$是关于$C$的减函数，所以再结合上述关于$\\overline{C}$的两个不等式易推得\n$$\\overline{C}=\\min C\\quad\\text { s.t. } \\sum_{i=C+1}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) p_0^{i}(1-p_0)^{m-i}<\\alpha$$\n\n## 参考文献\n[1]陈希孺编著.概率论与数理统计[M].中国科学技术大学出版社,2009.','2021-12-10 12:43:49','2021-12-19 14:50:28'),
	(3,1,'chapter3','## 3.5\n$$\\cfrac{\\partial E_{(w, b)}}{\\partial w}=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)$$\n[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n$$\\begin{aligned}\n\\cfrac{\\partial E_{(w, b)}}{\\partial w}&=\\cfrac{\\partial}{\\partial w} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n&= \\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial w} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-x_i)\\right] \\\\\n&= \\sum_{i=1}^{m}\\left[2\\cdot\\left(w x_{i}^2-y_i x_i +bx_i\\right)\\right] \\\\\n&= 2\\cdot\\left(w\\sum_{i=1}^{m} x_{i}^2-\\sum_{i=1}^{m}y_i x_i +b\\sum_{i=1}^{m}x_i\\right) \\\\\n&=2\\left(w \\sum_{i=1}^{m} x_{i}^{2}-\\sum_{i=1}^{m}\\left(y_{i}-b\\right) x_{i}\\right)\n\\end{aligned}$$\n\n## 3.6\n$$\\cfrac{\\partial E_{(w, b)}}{\\partial b}=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)$$\n[推导]：已知$E_{(w, b)}=\\sum\\limits_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}$，所以\n$$\\begin{aligned}\n\\cfrac{\\partial E_{(w, b)}}{\\partial b}&=\\cfrac{\\partial}{\\partial b} \\left[\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n&=\\sum_{i=1}^{m}\\cfrac{\\partial}{\\partial b} \\left[\\left(y_{i}-w x_{i}-b\\right)^{2}\\right] \\\\\n&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(y_{i}-w x_{i}-b\\right)\\cdot (-1)\\right] \\\\\n&=\\sum_{i=1}^{m}\\left[2\\cdot\\left(b-y_{i}+w x_{i}\\right)\\right] \\\\\n&=2\\cdot\\left[\\sum_{i=1}^{m}b-\\sum_{i=1}^{m}y_{i}+\\sum_{i=1}^{m}w x_{i}\\right] \\\\\n&=2\\left(m b-\\sum_{i=1}^{m}\\left(y_{i}-w x_{i}\\right)\\right)\n\\end{aligned}$$\n\n## 3.7\n$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n[推导]：令公式(3.5)等于0\n$$ 0 = w\\sum_{i=1}^{m}x_i^2-\\sum_{i=1}^{m}(y_i-b)x_i $$\n$$ w\\sum_{i=1}^{m}x_i^2 = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}bx_i $$\n由于令公式(3.6)等于0可得$b=\\cfrac{1}{m}\\sum_{i=1}^{m}(y_i-wx_i)$，又因为$\\cfrac{1}{m}\\sum_{i=1}^{m}y_i=\\bar{y}$，$\\cfrac{1}{m}\\sum_{i=1}^{m}x_i=\\bar{x}$，则$b=\\bar{y}-w\\bar{x}$，代入上式可得\n$$\\begin{aligned}	 \nw\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\sum_{i=1}^{m}(\\bar{y}-w\\bar{x})x_i \\\\\nw\\sum_{i=1}^{m}x_i^2 & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i+w\\bar{x}\\sum_{i=1}^{m}x_i \\\\\nw(\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i) & = \\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i \\\\\nw & = \\cfrac{\\sum_{i=1}^{m}y_ix_i-\\bar{y}\\sum_{i=1}^{m}x_i}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i}\n\\end{aligned}$$\n由于$\\bar{y}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}y_i\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i$，$\\bar{x}\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}\\sum_{i=1}^{m}x_i\\sum_{i=1}^{m}x_i=\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2$，代入上式即可得公式(3.7)\n$$ w=\\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2} $$\n如果要想用Python来实现上式的话，上式中的求和运算只能用循环来实现，但是如果我们能将上式给向量化，也就是转换成矩阵（向量）运算的话，那么我们就可以利用诸如NumPy这种专门加速矩阵运算的类库来进行编写。下面我们就尝试将上式进行向量化，将$\\cfrac{1}{m}(\\sum_{i=1}^{m}x_i)^2=\\bar{x}\\sum_{i=1}^{m}x_i$代入分母可得\n$$\\begin{aligned}	  \nw & = \\cfrac{\\sum_{i=1}^{m}y_i(x_i-\\bar{x})}{\\sum_{i=1}^{m}x_i^2-\\bar{x}\\sum_{i=1}^{m}x_i} \\\\\n& = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x})}\n\\end{aligned}$$\n又因为$\\bar{y}\\sum_{i=1}^{m}x_i=\\bar{x}\\sum_{i=1}^{m}y_i=\\sum_{i=1}^{m}\\bar{y}x_i=\\sum_{i=1}^{m}\\bar{x}y_i=m\\bar{x}\\bar{y}=\\sum_{i=1}^{m}\\bar{x}\\bar{y}$，$\\sum_{i=1}^{m}x_i\\bar{x}=\\bar{x}\\sum_{i=1}^{m}x_i=\\bar{x}\\cdot m \\cdot\\frac{1}{m}\\cdot\\sum_{i=1}^{m}x_i=m\\bar{x}^2=\\sum_{i=1}^{m}\\bar{x}^2$，则上式可化为\n$$\\begin{aligned}\nw & = \\cfrac{\\sum_{i=1}^{m}(y_ix_i-y_i\\bar{x}-x_i\\bar{y}+\\bar{x}\\bar{y})}{\\sum_{i=1}^{m}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}^2)} \\\\\n& = \\cfrac{\\sum_{i=1}^{m}(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^{m}(x_i-\\bar{x})^2} \n\\end{aligned}$$\n若令$\\boldsymbol{x}=(x_1,x_2,...,x_m)^T$，$\\boldsymbol{x}_{d}=(x_1-\\bar{x},x_2-\\bar{x},...,x_m-\\bar{x})^T$为去均值后的$\\boldsymbol{x}$，$\\boldsymbol{y}=(y_1,y_2,...,y_m)^T$，$\\boldsymbol{y}_{d}=(y_1-\\bar{y},y_2-\\bar{y},...,y_m-\\bar{y})^T$为去均值后的$\\boldsymbol{y}$，其中$\\boldsymbol{x}$、$\\boldsymbol{x}_{d}$、$\\boldsymbol{y}$、$\\boldsymbol{y}_{d}$均为m行1列的列向量，代入上式可得\n$$w=\\cfrac{\\boldsymbol{x}_{d}^T\\boldsymbol{y}_{d}}{\\boldsymbol{x}_d^T\\boldsymbol{x}_{d}}$$\n\n## 3.9\n$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n[推导]：公式(3.4)是最小二乘法运用在一元线性回归上的情形，那么对于多元线性回归来说，我们可以类似得到\n$$\\begin{aligned}\n	\\left(\\boldsymbol{w}^{*}, b^{*}\\right)&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}\\right)^{2} \\\\\n	&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)\\right)^{2}\\\\\n	&=\\underset{(\\boldsymbol{w}, b)}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\left(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}_{i}+b\\right)\\right)^{2}\n\\end{aligned}$$\n为便于讨论，我们令$\\hat{\\boldsymbol{w}}=(\\boldsymbol{w};b)=(w_1;...;w_d;b)\\in\\mathbb{R}^{(d+1)\\times 1},\\hat{\\boldsymbol{x}}_i=(x_1;...;x_d;1)\\in\\mathbb{R}^{(d+1)\\times 1}$，那么上式可以简化为\n$$\\begin{aligned}\n	\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{w}}^\\mathrm{T}\\hat{\\boldsymbol{x}}_{i}\\right)^{2} \\\\\n	&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\sum_{i=1}^{m}\\left(y_{i}-\\hat{\\boldsymbol{x}}_{i}^\\mathrm{T}\\hat{\\boldsymbol{w}}\\right)^{2} \\\\\n\\end{aligned}$$\n根据向量内积的定义可知，上式可以写成如下向量内积的形式\n$$\\begin{aligned}\n	\\hat{\\boldsymbol{w}}^{*}&=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min } \\begin{bmatrix}\n	y_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} & \\cdots & y_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n	\\end{bmatrix}\n	\\begin{bmatrix}\n	y_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n	\\vdots \\\\\n	y_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n	\\end{bmatrix} \\\\\n\\end{aligned}$$\n其中\n$$\n\\begin{aligned}\n\\begin{bmatrix}\n	y_{1}-\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n	\\vdots \\\\\n	y_{m}-\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n\\end{bmatrix}&=\\begin{bmatrix}\n	y_{1} \\\\\n	\\vdots \\\\\n	y_{m}\n\\end{bmatrix}-\\begin{bmatrix}\n	\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T}\\hat{\\boldsymbol{w}} \\\\\n	\\vdots \\\\\n	\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\\hat{\\boldsymbol{w}}\n\\end{bmatrix}\\\\\n&=\\boldsymbol{y}-\\begin{bmatrix}\n	\\hat{\\boldsymbol{x}}_{1}^\\mathrm{T} \\\\\n	\\vdots \\\\\n	\\hat{\\boldsymbol{x}}_{m}^\\mathrm{T}\n\\end{bmatrix}\\cdot\\hat{\\boldsymbol{w}}\\\\\n&=\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol{w}}\n\\end{aligned}\n$$\n所以\n$$\\hat{\\boldsymbol{w}}^{*}=\\underset{\\hat{\\boldsymbol{w}}}{\\arg \\min }(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X} \\hat{\\boldsymbol{w}})$$\n\n## 3.10\n$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$\n[推导]：将$E_{\\hat{\\boldsymbol w}}=(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})^{\\mathrm{T}}(\\boldsymbol{y}-\\mathbf{X}\\hat{\\boldsymbol w})$展开可得\n$$E_{\\hat{\\boldsymbol w}}= \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}-\\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}-\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+\\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}$$\n对$\\hat{\\boldsymbol w}$求导可得\n$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= \\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\boldsymbol{y}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}-\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}}{\\partial \\hat{\\boldsymbol w}}+\\cfrac{\\partial \\hat{\\boldsymbol w}^{\\mathrm{T}}\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}\\hat{\\boldsymbol w}}{\\partial \\hat{\\boldsymbol w}}$$\n由矩阵微分公式$\\cfrac{\\partial\\boldsymbol{a}^{\\mathrm{T}}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\boldsymbol{a}}{\\partial\\boldsymbol{x}}=\\boldsymbol{a},\\cfrac{\\partial\\boldsymbol{x}^{\\mathrm{T}}\\mathbf{A}\\boldsymbol{x}}{\\partial\\boldsymbol{x}}=(\\mathbf{A}+\\mathbf{A}^{\\mathrm{T}})\\boldsymbol{x}$可得\n$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}= 0-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}-\\mathbf{X}^{\\mathrm{T}}\\boldsymbol{y}+(\\mathbf{X}^{\\mathrm{T}}\\mathbf{X}+\\mathbf{X}^{\\mathrm{T}}\\mathbf{X})\\hat{\\boldsymbol w}$$\n$$\\cfrac{\\partial E_{\\hat{\\boldsymbol w}}}{\\partial \\hat{\\boldsymbol w}}=2\\mathbf{X}^{\\mathrm{T}}(\\mathbf{X}\\hat{\\boldsymbol w}-\\boldsymbol{y})$$\n\n## 3.27\n$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}(-y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i+\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})) $$\n[推导]：将公式(3.26)代入公式(3.25)可得\n$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\ln\\left(y_ip_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})+(1-y_i)p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right) $$\n其中$ p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}},p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}} $，代入上式可得\n$$\\begin{aligned} \n\\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left(\\cfrac{y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right) \\\\\n&=\\sum_{i=1}^{m}\\left(\\ln(y_ie^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}+1-y_i)-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n\\end{aligned}$$\n由于$ y_i $=0或1，则\n$$ \\ell(\\boldsymbol{\\beta}) =\n\\begin{cases} \n\\sum_{i=1}^{m}(-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})),  & y_i=0 \\\\\n\\sum_{i=1}^{m}(\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})), & y_i=1\n\\end{cases} $$\n两式综合可得\n$$ \\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) $$\n由于此式仍为极大似然估计的似然函数，所以最大化似然函数等价于最小化似然函数的相反数，也即在似然函数前添加负号即可得公式(3.27)。值得一提的是，若将公式(3.26)这个似然项改写为$p(y_i|\\boldsymbol x_i;\\boldsymbol w,b)=[p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}$，再将其代入公式(3.25)可得\n$$\\begin{aligned}\n \\ell(\\boldsymbol{\\beta})&=\\sum_{i=1}^{m}\\ln\\left([p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{y_i}[p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})]^{1-y_i}\\right) \\\\\n&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)+(1-y_i)\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n&=\\sum_{i=1}^{m} \\left \\{ y_i\\left[\\ln\\left(p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)-\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right]+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right\\} \\\\\n&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(\\cfrac{p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}{p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})}\\right)+\\ln\\left(p_0(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})\\right)\\right] \\\\\n&=\\sum_{i=1}^{m}\\left[y_i\\ln\\left(e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}\\right)+\\ln\\left(\\cfrac{1}{1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i}}\\right)\\right] \\\\\n&=\\sum_{i=1}^{m}\\left(y_i\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i-\\ln(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}}\\hat{\\boldsymbol x}_i})\\right) \n\\end{aligned}$$\n显然，此种方式更易推导出公式(3.27)\n\n## 3.30\n$$\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}=-\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta}))$$\n[解析]：此式可以进行向量化，令$p_1(\\hat{\\boldsymbol x}_i;\\boldsymbol{\\beta})=\\hat{y}_i$，代入上式得\n$$\\begin{aligned}\n\\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} &= -\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(y_i-\\hat{y}_i) \\\\\n& =\\sum_{i=1}^{m}\\hat{\\boldsymbol x}_i(\\hat{y}_i-y_i) \\\\\n& ={\\mathbf{X}^{\\mathrm{T}}}(\\hat{\\boldsymbol y}-\\boldsymbol{y}) \\\\\n& ={\\mathbf{X}^{\\mathrm{T}}}(p_1(\\mathbf{X};\\boldsymbol{\\beta})-\\boldsymbol{y}) \\\\\n\\end{aligned}$$\n\n## 3.32\n$$J=\\cfrac{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w}$$\n[推导]：\n$$\\begin{aligned}\n	J &= \\cfrac{\\|\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{0}-\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{1}\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n	&= \\cfrac{\\|(\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{0}-\\boldsymbol w^{\\mathrm{T}}\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n	&= \\cfrac{\\|(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w\\|_2^2}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n	&= \\cfrac{\\left[(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w\\right]^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w} \\\\\n	&= \\cfrac{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol w}{\\boldsymbol w^{\\mathrm{T}}(\\boldsymbol{\\Sigma}_{0}+\\boldsymbol{\\Sigma}_{1})\\boldsymbol w}\n\\end{aligned}$$\n\n## 3.37\n$$\\mathbf{S}_b\\boldsymbol w=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n[推导]：由公式(3.36)可得拉格朗日函数为\n$$L(\\boldsymbol w,\\lambda)=-\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_b\\boldsymbol w+\\lambda(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_w\\boldsymbol w-1)$$\n对$\\boldsymbol w$求偏导可得\n$$\\begin{aligned}\n\\cfrac{\\partial L(\\boldsymbol w,\\lambda)}{\\partial \\boldsymbol w} &= -\\cfrac{\\partial(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_b\\boldsymbol w)}{\\partial \\boldsymbol w}+\\lambda \\cfrac{\\partial(\\boldsymbol w^{\\mathrm{T}}\\mathbf{S}_w\\boldsymbol w-1)}{\\partial \\boldsymbol w} \\\\\n&= -(\\mathbf{S}_b+\\mathbf{S}_b^{\\mathrm{T}})\\boldsymbol w+\\lambda(\\mathbf{S}_w+\\mathbf{S}_w^{\\mathrm{T}})\\boldsymbol w\n\\end{aligned}$$\n由于$\\mathbf{S}_b=\\mathbf{S}_b^{\\mathrm{T}},\\mathbf{S}_w=\\mathbf{S}_w^{\\mathrm{T}}$，所以\n$$\\cfrac{\\partial L(\\boldsymbol w,\\lambda)}{\\partial \\boldsymbol w} = -2\\mathbf{S}_b\\boldsymbol w+2\\lambda\\mathbf{S}_w\\boldsymbol w$$\n令上式等于0即可得\n$$-2\\mathbf{S}_b\\boldsymbol w+2\\lambda\\mathbf{S}_w\\boldsymbol w=0$$\n$$\\mathbf{S}_b\\boldsymbol w=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n$$(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol{w}=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n若令$(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})^{\\mathrm{T}}\\boldsymbol{w}=\\gamma$，则\n$$\\gamma(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})=\\lambda\\mathbf{S}_w\\boldsymbol w$$\n$$\\boldsymbol{w}=\\frac{\\gamma}{\\lambda}\\mathbf{S}_{w}^{-1}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n由于最终要求解的$\\boldsymbol{w}$不关心其大小，只关心其方向，所以其大小可以任意取值。由于$\\boldsymbol{\\mu}_{0}$和$\\boldsymbol{\\mu}_{1}$的大小是固定的，所以$\\gamma$这个标量的大小只受$\\boldsymbol{w}$的大小影响，因此可以调整$\\boldsymbol{w}$的大小使得$\\gamma=\\lambda$，西瓜书中所说的“不妨令$\\mathbf{S}_b\\boldsymbol w=\\lambda(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$”也等价于令$\\gamma=\\lambda$，因此，此时$\\frac{\\gamma}{\\lambda}=1$，求解出的$\\boldsymbol{w}$即为公式(3.39)\n\n## 3.38\n$$\\mathbf{S}_b\\boldsymbol{w}=\\lambda(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n[推导]：参见公式(3.37)\n\n## 3.39\n$$\\boldsymbol{w}=\\mathbf{S}_{w}^{-1}(\\boldsymbol{\\mu}_{0}-\\boldsymbol{\\mu}_{1})$$\n[推导]：参见公式(3.37)\n\n## 3.43\n$$\\begin{aligned}\n\\mathbf{S}_b &= \\mathbf{S}_t - \\mathbf{S}_w \\\\\n&= \\sum_{i=1}^N m_i(\\boldsymbol\\mu_i-\\boldsymbol\\mu)(\\boldsymbol\\mu_i-\\boldsymbol\\mu)^{\\mathrm{T}}\n\\end{aligned}$$\n[推导]：由公式(3.40)、公式(3.41)、公式(3.42)可得：\n$$\\begin{aligned}\n\\mathbf{S}_b &= \\mathbf{S}_t - \\mathbf{S}_w \\\\\n&= \\sum_{i=1}^m(\\boldsymbol x_i-\\boldsymbol\\mu)(\\boldsymbol x_i-\\boldsymbol\\mu)^{\\mathrm{T}}-\\sum_{i=1}^N\\sum_{\\boldsymbol x\\in X_i}(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x-\\boldsymbol\\mu_i)^{\\mathrm{T}} \\\\\n&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left((\\boldsymbol x-\\boldsymbol\\mu)(\\boldsymbol x-\\boldsymbol\\mu)^{\\mathrm{T}}-(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x-\\boldsymbol\\mu_i)^{\\mathrm{T}}\\right)\\right) \\\\\n&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left((\\boldsymbol x-\\boldsymbol\\mu)(\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu^{\\mathrm{T}})-(\\boldsymbol x-\\boldsymbol\\mu_i)(\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i^{\\mathrm{T}})\\right)\\right) \\\\\n&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left(\\boldsymbol x\\boldsymbol x^{\\mathrm{T}} - \\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol x\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right)\\right) \\\\\n&= \\sum_{i=1}^N\\left(\\sum_{\\boldsymbol x\\in X_i}\\left(- \\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right)\\right) \\\\\n&= \\sum_{i=1}^N\\left(-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol x\\boldsymbol\\mu^{\\mathrm{T}}-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu\\boldsymbol x^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol x\\boldsymbol\\mu_i^{\\mathrm{T}}+\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu_i\\boldsymbol x^{\\mathrm{T}}-\\sum_{\\boldsymbol x\\in X_i}\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n&= \\sum_{i=1}^N\\left(-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-m_i\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n&= \\sum_{i=1}^N\\left(-m_i\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-m_i\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+m_i\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+m_i\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n&= \\sum_{i=1}^Nm_i\\left(-\\boldsymbol\\mu_i\\boldsymbol\\mu^{\\mathrm{T}}-\\boldsymbol\\mu\\boldsymbol\\mu_i^{\\mathrm{T}}+\\boldsymbol\\mu\\boldsymbol\\mu^{\\mathrm{T}}+\\boldsymbol\\mu_i\\boldsymbol\\mu_i^{\\mathrm{T}}\\right) \\\\\n&= \\sum_{i=1}^N m_i(\\boldsymbol\\mu_i-\\boldsymbol\\mu)(\\boldsymbol\\mu_i-\\boldsymbol\\mu)^{\\mathrm{T}}\n\\end{aligned}$$\n\n## 3.44\n$$\\max\\limits_{\\mathbf{W}}\\cfrac{\n\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})}{\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})}$$\n[解析]：此式是公式(3.35)的推广形式，证明如下：\n设$\\mathbf{W}=(\\boldsymbol w_1,\\boldsymbol w_2,...,\\boldsymbol w_i,...,\\boldsymbol w_{N-1})\\in\\mathbb{R}^{d\\times(N-1)}$，其中$\\boldsymbol w_i\\in\\mathbb{R}^{d\\times 1}$为$d$行1列的列向量，则\n$$\\left\\{\n\\begin{aligned}\n\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})&=\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_b \\boldsymbol w_i \\\\\n\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})&=\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_w \\boldsymbol w_i\n\\end{aligned}\n\\right.$$\n所以公式(3.44)可变形为\n$$\\max\\limits_{\\mathbf{W}}\\cfrac{\n\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_b \\boldsymbol w_i}{\\sum_{i=1}^{N-1}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf{S}_w \\boldsymbol w_i}$$\n对比公式(3.35)易知上式即公式(3.35)的推广形式\n\n## 3.45\n$$\\mathbf{S}_b\\mathbf{W}=\\lambda\\mathbf{S}_w\\mathbf{W}$$\n[推导]：同公式(3.35)一样，我们在此处也固定公式(3.44)的分母为1，那么公式(3.44)此时等价于如下优化问题\n$$\\begin{array}{cl}\\underset{\\boldsymbol{w}}{\\min} & -\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W}) \\\\ \n\\text { s.t. } & \\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})=1\\end{array}$$\n根据拉格朗日乘子法可知，上述优化问题的拉格朗日函数为\n$$L(\\mathbf{W},\\lambda)=-\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})+\\lambda(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})-1)$$\n根据矩阵微分公式$\\cfrac{\\partial}{\\partial \\mathbf{X}} \\text { tr }(\\mathbf{X}^{\\mathrm{T}}  \\mathbf{B} \\mathbf{X})=(\\mathbf{B}+\\mathbf{B}^{\\mathrm{T}})\\mathbf{X}$对上式关于$\\mathbf{W}$求偏导可得\n$$\\begin{aligned}\n\\cfrac{\\partial L(\\mathbf{W},\\lambda)}{\\partial \\mathbf{W}} &= -\\cfrac{\\partial\\left(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_b \\mathbf{W})\\right)}{\\partial \\mathbf{W}}+\\lambda \\cfrac{\\partial\\left(\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}\\mathbf{S}_w \\mathbf{W})-1\\right)}{\\partial \\mathbf{W}} \\\\\n&= -(\\mathbf{S}_b+\\mathbf{S}_b^{\\mathrm{T}})\\mathbf{W}+\\lambda(\\mathbf{S}_w+\\mathbf{S}_w^{\\mathrm{T}})\\mathbf{W}\n\\end{aligned}$$\n由于$\\mathbf{S}_b=\\mathbf{S}_b^{\\mathrm{T}},\\mathbf{S}_w=\\mathbf{S}_w^{\\mathrm{T}}$，所以\n$$\\cfrac{\\partial L(\\mathbf{W},\\lambda)}{\\partial \\mathbf{W}} = -2\\mathbf{S}_b\\mathbf{W}+2\\lambda\\mathbf{S}_w\\mathbf{W}$$\n令上式等于$\\mathbf{0}$即可得\n$$-2\\mathbf{S}_b\\mathbf{W}+2\\lambda\\mathbf{S}_w\\mathbf{W}=\\mathbf{0}$$\n$$\\mathbf{S}_b\\mathbf{W}=\\lambda\\mathbf{S}_w\\mathbf{W}$$','2021-12-10 12:43:49','2021-12-19 13:37:02'),
	(4,1,'chapter4','## 4.1\n$$\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|}p_k\\log_{2}{p_k}$$\n[解析]：证明$0\\leq\\operatorname{Ent}(D)\\leq\\log_{2}|\\mathcal{Y}|$：\n已知集合$D$的信息熵的定义为\n$$\\operatorname{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y}|} p_{k} \\log _{2} p_{k}$$\n其中，$|\\mathcal{Y}|$表示样本类别总数，$p_k$表示第$k$类样本所占的比例，且$0 \\leq p_k \\leq 1,\\sum_{k=1}^{n}p_k=1$。若令$|\\mathcal{Y}|=n,p_k=x_k$，那么信息熵$\\operatorname{Ent}(D)$就可以看作一个$n$元实值函数，也即\n$$\\operatorname{Ent}(D)=f(x_1,...,x_n)=-\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k} $$\n其中，$0 \\leq x_k \\leq 1,\\sum_{k=1}^{n}x_k=1$，下面考虑求该多元函数的最值。首先我们先来求最大值，如果不考虑约束$0 \\leq x_k \\leq 1$，仅考虑$\\sum_{k=1}^{n}x_k=1$的话，对$f(x_1,...,x_n)$求最大值等价于如下最小化问题\n$$\\begin{array}{ll}{\n\\operatorname{min}} & {\\sum\\limits_{k=1}^{n} x_{k} \\log _{2} x_{k} } \\\\ \n{\\text { s.t. }} & {\\sum\\limits_{k=1}^{n}x_k=1} \n\\end{array}$$\n显然，在$0 \\leq x_k \\leq 1$时，此问题为凸优化问题，而对于凸优化问题来说，能令其拉格朗日函数的一阶偏导数等于0的点即为最优解。根据拉格朗日乘子法可知，该优化问题的拉格朗日函数为\n$$L(x_1,...,x_n,\\lambda)=\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)$$\n其中，$\\lambda$为拉格朗日乘子。对$L(x_1,...,x_n,\\lambda)$分别关于$x_1,...,x_n,\\lambda$求一阶偏导数，并令偏导数等于0可得\n$$\\begin{aligned}\n\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_1}&=\\cfrac{\\partial }{\\partial x_1}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n&=\\log _{2} x_{1}+x_1\\cdot \\cfrac{1}{x_1\\ln2}+\\lambda=0 \\\\\n&=\\log _{2} x_{1}+\\cfrac{1}{\\ln2}+\\lambda=0 \\\\\n&\\Rightarrow \\lambda=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}\\\\\n\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_2}&=\\cfrac{\\partial }{\\partial x_2}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n&\\Rightarrow \\lambda=-\\log _{2} x_{2}-\\cfrac{1}{\\ln2}\\\\\n\\vdots\\\\\n\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial x_n}&=\\cfrac{\\partial }{\\partial x_n}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n&\\Rightarrow \\lambda=-\\log _{2} x_{n}-\\cfrac{1}{\\ln2}\\\\\n\\cfrac{\\partial L(x_1,...,x_n,\\lambda)}{\\partial \\lambda}&=\\cfrac{\\partial }{\\partial \\lambda}\\left[\\sum_{k=1}^{n} x_{k} \\log _{2} x_{k}+\\lambda(\\sum_{k=1}^{n}x_k-1)\\right]=0\\\\\n&\\Rightarrow \\sum_{k=1}^{n}x_k=1\\\\\n\\end{aligned}$$\n整理一下可得\n$$\\left\\{ \\begin{array}{lr}\n\\lambda=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}=-\\log _{2} x_{2}-\\cfrac{1}{\\ln2}=...=-\\log _{2} x_{n}-\\cfrac{1}{\\ln2} \\\\\n\\sum\\limits_{k=1}^{n}x_k=1\n\\end{array}\\right.$$\n由以上两个方程可以解得\n$$x_1=x_2=...=x_n=\\cfrac{1}{n}$$\n又因为$x_k$还需满足约束$0 \\leq x_k \\leq 1$，显然$0 \\leq\\cfrac{1}{n}\\leq 1$，所以$x_1=x_2=...=x_n=\\cfrac{1}{n}$是满足所有约束的最优解，也即为当前最小化问题的最小值点，同时也是$f(x_1,...,x_n)$的最大值点。将$x_1=x_2=...=x_n=\\cfrac{1}{n}$代入$f(x_1,...,x_n)$中可得\n$$f(\\cfrac{1}{n},...,\\cfrac{1}{n})=-\\sum_{k=1}^{n} \\cfrac{1}{n} \\log _{2} \\cfrac{1}{n}=-n\\cdot\\cfrac{1}{n} \\log _{2} \\cfrac{1}{n}=\\log _{2} n$$\n所以$f(x_1,...,x_n)$在满足约束$0 \\leq x_k \\leq 1,\\sum_{k=1}^{n}x_k=1$时的最大值为$\\log _{2} n$。求完最大值后下面我们再来求最小值，如果不考虑约束$\\sum_{k=1}^{n}x_k=1$，仅考虑$0 \\leq x_k \\leq 1$的话，$f(x_1,...,x_n)$可以看做是$n$个互不相关的一元函数的加和，也即\n$$f(x_1,...,x_n)=\\sum_{k=1}^{n} g(x_k) $$\n其中，$g(x_k)=-x_{k} \\log _{2} x_{k},0 \\leq x_k \\leq 1$。那么当$g(x_1),g(x_2),...,g(x_n)$分别取到其最小值时，$f(x_1,...,x_n)$也就取到了最小值。所以接下来考虑分别求$g(x_1),g(x_2),...,g(x_n)$各自的最小值，由于$g(x_1),g(x_2),...,g(x_n)$的定义域和函数表达式均相同，所以只需求出$g(x_1)$的最小值也就求出了$g(x_2),...,g(x_n)$的最小值。下面考虑求$g(x_1)$的最小值，首先对$g(x_1)$关于$x_1$求一阶和二阶导数\n$$g^{\\prime}(x_1)=\\cfrac{d(-x_{1} \\log _{2} x_{1})}{d x_1}=-\\log _{2} x_{1}-x_1\\cdot \\cfrac{1}{x_1\\ln2}=-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}$$\n$$g^{\\prime\\prime}(x_1)=\\cfrac{d\\left(g^{\\prime}(x_1)\\right)}{d x_1}=\\cfrac{d\\left(-\\log _{2} x_{1}-\\cfrac{1}{\\ln2}\\right)}{d x_1}=-\\cfrac{1}{x_{1}\\ln2}$$\n显然，当$0 \\leq x_k \\leq 1$时$g^{\\prime\\prime}(x_1)=-\\cfrac{1}{x_{1}\\ln2}$恒小于0，所以$g(x_1)$是一个在其定义域范围内开口向下的凹函数，那么其最小值必然在边界取，于是分别取$x_1=0$和$x_1=1$，代入$g(x_1)$可得\n$$g(0)=-0\\log _{2} 0=0$$\n$$g(1)=-1\\log _{2} 1=0$$\n所以，$g(x_1)$的最小值为0，同理可得$g(x_2),...,g(x_n)$的最小值也为0，那么$f(x_1,...,x_n)$的最小值此时也为0。但是，此时是不考虑约束$\\sum_{k=1}^{n}x_k=1$，仅考虑$0 \\leq x_k \\leq 1$时取到的最小值，若考虑约束$\\sum_{k=1}^{n}x_k=1$的话，那么$f(x_1,...,x_n)$的最小值一定大于等于0。如果令某个$x_k=1$，那么根据约束$\\sum_{k=1}^{n}x_k=1$可知$x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，将其代入$f(x_1,...,x_n)$可得\n$$f(0,0,...,0,1,0,...,0)=-0 \\log _{2}0-0 \\log _{2}0...-0 \\log _{2}0-1 \\log _{2}1-0 \\log _{2}0...-0 \\log _{2}0=0 $$\n所以$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$一定是$f(x_1,...,x_n)$在满足约束$\\sum_{k=1}^{n}x_k=1$和$0 \\leq x_k \\leq 1$的条件下的最小值点，其最小值为0。<br>\n综上可知，当$f(x_1,...,x_n)$取到最大值时：$x_1=x_2=...=x_n=\\cfrac{1}{n}$，此时样本集合纯度最低；当$f(x_1,...,x_n)$取到最小值时：$x_k=1,x_1=x_2=...=x_{k-1}=x_{k+1}=...=x_n=0$，此时样本集合纯度最高。\n## 4.2\n$$\\operatorname{Gain}(D,a) = \\operatorname{Ent}(D) - \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}\\operatorname{Ent}({D^v})$$\n[解析]：这个是信息增益的定义公式，在信息论中信息增益也称为互信息（参见附录①），其表示已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度。所以在这里，这个公式可以理解为在属性$a$的取值已知后，样本类别这个随机变量的不确定性减小的程度。若根据某个属性计算得到的信息增益越大，则说明在知道其取值后样本集的不确定性减小的程度越大，也即为书上所说的“纯度提升”越大。\n\n## 4.6\n$$\\operatorname{Gini\\_index}(D,a) = \\sum_{v=1}^{V}\\frac{|D^v|}{|D|}\\operatorname{Gini}(D^v)$$\n[解析]：这个是数据集$D$中属性$a$的基尼指数的定义，它表示在属性$a$的取值已知的条件下，数据集$D$按照属性$a$的所有可能取值划分后的纯度，不过在构造CART分类树时并不会严格按照此公式来选择最优划分属性，主要是因为CART分类树是一颗二叉树，如果用上面的公式去选出最优划分属性，无法进一步选出最优划分属性的最优划分点。CART分类树的构造算法如下：\n- 首先，对每个属性$a$的每个可能取值$v$，将数据集$D$分为$a=v$和$a\\neq v$两部分来计算基尼指数，即\n$$\\operatorname{Gini\\_index}(D,a) = \\frac{|D^{a=v}|}{|D|}\\operatorname{Gini}(D^{a=v})+\\frac{|D^{a\\neq v}|}{|D|}\\operatorname{Gini}(D^{a\\neq v})$$\n- 然后，选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点；\n- 最后，重复以上两步，直至满足停止条件。\n\n下面以西瓜书中表4.2中西瓜数据集2.0为例来构造CART分类树，其中第一个最优划分属性和最优划分点的计算过程如下：\n以属性“色泽”为例，它有3个可能的取值：$\\{\\text{青绿}，\\text{乌黑}，\\text{浅白}\\}$， 若使用该属性的属性值是否等于“青绿”对数据集$D$进行划分，则可得到2个子集，分别记为$D^1(\\text{色泽}=\\text{青绿}),D^2(\\text{色泽}\\not=\\text{青绿})$。子集$D^1$包含编号$\\{1,4,6,10,13,17\\}$共6个样例，其中正例占$p_1=\\frac{3}{6}$，反例占$p_2=\\frac{3}{6}$；子集$D^2$包含编号$\\{2,3,5,7,8,9,11,12,14,15,16\\}$共11个样例，其中正例占$p_1=\\frac{5}{11}$，反例占$p_2=\\frac{6}{11}$，根据公式（4.5）可计算出用“色泽=青绿”划分之后得到基尼指数为\n$$\\operatorname{Gini\\_index}(D,\\text{色泽}=\\text{青绿}) = \\frac{6}{17}\\times\\left(1-(\\frac{3}{6})^2-(\\frac{3}{6})^2\\right)+\\frac{11}{17}\\times\\left(1-(\\frac{5}{11})^2-(\\frac{6}{11})^2\\right) \n= 0.497$$\n类似的，可以计算出以下不同属性取不同值的基尼指数\n$$\\begin{aligned}\n\\operatorname{Gini\\_index}(D,\\text{色泽}=\\text{乌黑}) = \\frac{6}{17}\\times\\left(1-(\\frac{4}{6})^2-(\\frac{2}{6})^2\\right)+\\frac{11}{17}\\times\\left(1-(\\frac{4}{11})^2-(\\frac{7}{11})^2\\right) \n= 0.456\\\\\n\\operatorname{Gini\\_index}(D,\\text{色泽}=\\text{浅白}) = \\frac{5}{17}\\times\\left(1-(\\frac{1}{5})^2-(\\frac{4}{5})^2\\right)+\\frac{12}{17}\\times\\left(1-(\\frac{7}{12})^2-(\\frac{5}{12})^2\\right) \n= 0.426\n\\end{aligned}$$\n$$\\begin{aligned}\n\\operatorname{Gini\\_index}(D,\\text{根蒂}=\\text{蜷缩}) = 0.456\\\\\n\\operatorname{Gini\\_index}(D,\\text{根蒂}=\\text{稍蜷}) = 0.496\\\\\n\\operatorname{Gini\\_index}(D,\\text{根蒂}=\\text{硬挺}) = 0.439\\\\\n\\operatorname{Gini\\_index}(D,\\text{敲声}=\\text{浊响}) = 0.450\\\\\n\\operatorname{Gini\\_index}(D,\\text{敲声}=\\text{沉闷}) = 0.494\\\\\n\\operatorname{Gini\\_index}(D,\\text{敲声}=\\text{清脆}) = 0.439\\\\\n\\operatorname{Gini\\_index}(D,\\text{纹理}=\\text{清晰}) = 0.286\\\\\n\\operatorname{Gini\\_index}(D,\\text{纹理}=\\text{稍稀}) = 0.437\\\\\n\\operatorname{Gini\\_index}(D,\\text{纹理}=\\text{模糊}) = 0.403\\\\\n\\operatorname{Gini\\_index}(D,\\text{脐部}=\\text{凹陷}) = 0.415\\\\\n\\operatorname{Gini\\_index}(D,\\text{脐部}=\\text{稍凹}) = 0.497\\\\\n\\operatorname{Gini\\_index}(D,\\text{脐部}=\\text{平坦}) = 0.362\\\\\n\\operatorname{Gini\\_index}(D,\\text{触感}=\\text{硬挺}) = 0.494\\\\\n\\operatorname{Gini\\_index}(D,\\text{触感}=\\text{软粘}) = 0.494\n\\end{aligned}$$ \n特别地，对于属性“触感”，由于它的可取值个数为2，所以其实只需计算其中一个取值的基尼指数即可。根据上面的计算结果可知$\\operatorname{Gini\\_index}(D,\\text{纹理}=\\text{清晰}) = 0.286$最小，所以选择属性“纹理”为最优划分属性并生成根节点，接着以“纹理=清晰”为最优划分点生成$D^1(\\text{纹理}=\\text{清晰}),D^2(\\text{纹理}\\not=\\text{清晰})$两个子节点，对于两个子节点分别重复上述步骤继续生成下一层子节点，直至满足停止条件。以上便是CART分类树的构建过程，从构建过程中可以看出，CART分类树最终构造出来的是一颗二叉树。CART决策树除了能处理分类问题以外，它还可以处理回归问题，附录②中给出了CART回归树的构造算法。\n\n## 4.7\n$$T_a={\\lbrace{\\frac{a^i+a^{i+1}}{2}|1\\leq{i}\\leq{n-1}}\\rbrace}$$\n[解析]：这个公式所表达的思想很简单，就是以每两个相邻取值的中点作为划分点，下面以西瓜书中表4.3中西瓜数据集3.0为例来说明此公式的用法。对于“密度”这个连续属性，已观测到的可能取值为$\\{0.243,0.245,0.343,0.360,0.403,0.437,0.481,0.556,0.593,0.608,0.634,0.639,0.657,0.666,0.697,0.719,0.774\\}$共17个值，根据公式（4.7）可知，此时$i$依次取1到16，那么“密度”这个属性的候选划分点集合为\n$T_{a} = \\{\\frac{(0.243+0.245)}{2}, \\frac{(0.245+0.343)}{2},\\frac{(0.343+0.360)}{2},\\frac{(0.360+0.403)}{2},\\frac{(0.403+0.437)}{2},\\frac{(0.437+0.481)}{2},\\frac{(0.481+0.556)}{2},\\\\\\frac{(0.556+0.593)}{2},\\frac{(0.593+0.608)}{2},\\frac{(0.608+0.634)}{2},\\frac{(0.634+0.639)}{2},\\frac{(0.639+0.657)}{2},\\frac{(0.657+0.666)}{2},\\frac{(0.666+0.697)}{2},\\frac{(0.697+0.719)}{2},\\frac{(0.719+0.774)}{2}\\}$\n\n## 4.8\n$$\\begin{aligned}\n\\operatorname{Gain}(D,a) &= \\max_{t\\in{T_a}}\\operatorname{Gain}(D,a,t)\\\\\n&=\n\\max_{t\\in{T_a}}\\operatorname{Ent}(D)-\\sum_{\\lambda\\in\\{-,+\\}}\\frac{|D_t^{\\lambda}|}\n{|D|}\\operatorname{Ent}(D_t^{\\lambda})\n\\end{aligned}$$\n[解析]：此公式是公式（4.2）用于离散化后的连续属性的版本，其中$T_a$由公式（4.7）计算得来，$\\lambda\\in\\{-,+\\}$表示属性$a$的取值分别小于等于和大于候选划分点$t$时的情形，也即当$\\lambda=-$时：$D^{\\lambda}_t=D^{a\\leq t}_t$，当$\\lambda=+$时：$D^{\\lambda}_t=D^{a> t}_t$。\n           \n## 附录\n### ①互信息<sup>[1]</sup>\n在解释互信息之前，需要先解释一下什么是条件熵。条件熵表示的是在已知一个随机变量的条件下，另一个随机变量的不确定性。具体地，假设有随机变量$X$和$Y$，且它们服从以下联合概率分布\n$$P(X = x_{i},Y = y_{j}) = p_{ij}\\quad i = 1,2,....,n;j = 1,2,...,m$$\n那么在已知$X$的条件下，随机变量$Y$的条件熵为\n$$\\operatorname{Ent}(Y|X) =  \\sum_{i=1}^np_i \\operatorname{Ent}(Y|X = x_i)$$\n其中，$ p_i = P(X = x_i) ，i =1,2,...,n$。互信息定义为信息熵和条件熵的差，它表示的是已知一个随机变量的信息后使得另一个随机变量的不确定性减少的程度。具体地，假设有随机变量$X$和$Y$，那么在已知$X$的信息后，$Y$的不确定性减少的程度为\n$$\\operatorname{I}(Y;X) = \\operatorname{Ent}(Y) - \\operatorname{Ent}(Y|X)$$\n此即为互信息的数学定义。\n\n### ②CART回归树<sup>[1]</sup>\n假设给定数据集\n$$D = {(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2)...,(\\boldsymbol{x}_N,y_N)}$$\n其中$\\boldsymbol{x}\\in \\mathbb{R}^d$为$d$维特征向量，$y\\in \\mathbb{R}$是连续型随机变量，这是一个标准的回归问题的数据集。若把每个属性视为坐标空间中的一个坐标轴，则$d$个属性就构成了一个$d$维的特征空间，而每个$d$维特征向量$\\boldsymbol{x}$就对应了$d$维的特征空间中的一个数据点。CART回归树的目标是将特征空间划分成若干个子空间，每个子空间都有一个固定的输出值，也就是凡是落在同一个子空间内的数据点$\\boldsymbol{x}_i$，他们所对应的输出值$y_i$恒相等，且都为该子空间的输出值。那么如何划分出若干个子空间呢？这里采用一种启发式的方法：\n- 任意选择一个属性$a$，遍历其所有可能取值，根据如下公式找出属性$a$最优划分点$v^*$：\n$$v^* = \\arg\\min_{v}\\left[\\min_{c_1}\\sum_{\\boldsymbol {x}_i\\in{R_1(a,v)}}{(y_i - c_1)}^2 + \\min_{c_2}\\sum_{\\boldsymbol {x}_i\\in{R_2(a,v)}}{(y_i - c_2)}^2 \\right]$$\n其中，$R_1(a,v)=\\{\\boldsymbol {x}|\\boldsymbol {x}\\in D^{a\\leq v}\\},R_2(a,v)=\\{\\boldsymbol {x}|\\boldsymbol {x}\\in D^{a > v}\\}$，$c_1$和$c_2$分别为集合$R_1(a,v)$和$R_2(a,v)$中的样本$\\boldsymbol {x}_i$对应的输出值$y_i$的均值，也即\n$$c_1=\\operatorname{ave}(y_i | x\\in R_1(a,v))=\\frac{1}{|R_1(a,v)|}\\sum_{\\boldsymbol {x}_i\\in{R_1(a,v)}}y_i$$\n$$c_2=\\operatorname{ave}(y_i | x\\in R_2(a,v))=\\frac{1}{|R_2(a,v)|}\\sum_{\\boldsymbol {x}_i\\in{R_2(a,v)}}y_i$$\n- 遍历所有属性，找到最优划分属性$a^*$，然后根据$a^*$的最优划分点$v^*$将特征空间划分为两个子空间，接着对每个子空间重复上述步骤，直至满足停止条件。这样就生成了一颗CART回归树，假设最终将特征空间被划分为了$M$个子空间$R_1,R_2,...,R_M$，那么CART回归树的模型公式可以表示为\n$$f(\\boldsymbol {x}) = \\sum_{m=1}^{M}c_m\\mathbb{I}(x\\in{R_m})$$\n同理，其中的$c_m$表示的也是集合$R_m$中的样本$\\boldsymbol {x}_i$对应的输出值$y_i$的均值。此公式直观上的理解就是，对于一个给定的样本$\\boldsymbol {x}_i$，首先判断其属于哪个子空间，然后将其所属的子空间对应的输出值作为该样本的预测值$y_i$。\n\n## 参考文献\n[1]李航编著.统计学习方法[M].清华大学出版社,2012.\n\n','2021-12-10 12:43:49','2021-12-19 13:22:15'),
	(5,1,'chapter5','## 5.2\n$$\\Delta w_i=\\eta(y-\\hat{y})x_i$$\n[解析]：此公式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介绍<sup>[1]</sup>：\n### 感知机模型\n已知感知机由两层神经元组成，故感知机模型的公式可表示为\n$$y=f(\\sum\\limits_{i=1}^{n}w_ix_i-\\theta)=f(\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta)$$\n其中，$\\boldsymbol{x} \\in \\mathbb{R}^n$为样本的特征向量，是感知机模型的输入；$\\boldsymbol{w},\\theta$是感知机模型的参数，$\\boldsymbol{w} \\in \\mathbb{R}^n$为权重，$\\theta$为阈值。上式中的$f$通常设为符号函数，那么感知机模型的公式可进一步表示为\n$$ y=\\operatorname{sign}(\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta)=\\left\\{\\begin{array}{rcl}\n1,& {\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x} -\\theta\\geq 0}\\\\\n0,& {\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x} -\\theta < 0}\\\\\n\\end{array} \\right.$$\n由于$n$维空间中的超平面方程为\n$$w_1x_1+w_2x_2+\\cdots+w_nx_n+b  =\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x} +b=0$$\n所以此时感知机模型公式中的$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta$可以看作是$n$维空间中的一个超平面，通过它将$n$维空间划分为$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta\\geq 0$和$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta<0$两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间的样本对应的模型输出值为0，以此来实现分类功能。\n### 感知机学习策略\n给定一个线性可分的数据集$T$（参见附录①），感知机的学习目标是求得能对数据集$T$中的正负样本完全正确划分的分离超平面：\n$$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}-\\theta=0$$\n假设此时误分类样本集合为$M\\subseteq T$，对任意一个误分类样本$(\\boldsymbol{x},y)\\in M$来说，当$\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}-\\theta \\geq 0$时，模型输出值为$\\hat{y}=1$，样本真实标记为$y=0$；反之，当$\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}-\\theta<0$时，模型输出值为$\\hat{y}=0$，样本真实标记为$y=1$。综合两种情形可知，以下公式恒成立\n$$(\\hat{y}-y)(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}-\\theta)\\geq0$$\n所以，给定数据集$T$，其损失函数可以定义为：\n$$L(\\boldsymbol{w},\\theta)=\\sum_{\\boldsymbol{x}\\in M}(\\hat{y}-y)(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}-\\theta)$$\n显然，此损失函数是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。因此，给定数据集$T$，损失函数$L(\\boldsymbol{w},\\theta)$是关于$\\boldsymbol{w},\\theta$的连续可导函数。\n### 感知机学习算法\n感知机模型的学习问题可以转化为求解损失函数的最优化问题，具体地，给定数据集\n$$T=\\{(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_N,y_N)\\}$$\n其中$\\boldsymbol{x}_i \\in \\mathbb{R}^n,y_i \\in \\{0,1\\}$，求参数$\\boldsymbol{w},\\theta$，使其为极小化损失函数的解：\n$$\\min\\limits_{\\boldsymbol{w},\\theta}L(\\boldsymbol{w},\\theta)=\\min\\limits_{\\boldsymbol{w},\\theta}\\sum_{\\boldsymbol{x_i}\\in M}(\\hat{y}_i-y_i)(\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}_i-\\theta)$$\n其中$M\\subseteq T$为误分类样本集合。若将阈值$\\theta$看作一个固定输入为$-1$的“哑节点”，即\n$$-\\theta=-1\\cdot w_{n+1}=x_{n+1}\\cdot w_{n+1}$$\n那么$\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x}_i-\\theta$可化简为\n$$\\begin{aligned}\n\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x_i}-\\theta&=\\sum \\limits_{j=1}^n w_jx_j+x_{n+1}\\cdot w_{n+1}\\\\\n&=\\sum \\limits_{j=1}^{n+1}w_jx_j\\\\\n&=\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x_i}\n \\end{aligned}$$\n其中$\\boldsymbol{x_i} \\in \\mathbb{R}^{n+1},\\boldsymbol{w} \\in \\mathbb{R}^{n+1}$。根据该式，可将要求解的极小化问题进一步简化为\n$$\\min\\limits_{\\boldsymbol{w}}L(\\boldsymbol{w})=\\min\\limits_{\\boldsymbol{w}}\\sum_{\\boldsymbol{x_i}\\in M}(\\hat{y}_i-y_i)\\boldsymbol{w}^\\mathrm{T}\\boldsymbol{x_i}$$\n假设误分类样本集合$M$固定，那么可以求得损失函数$L(\\boldsymbol{w})$的梯度为：\n$$\\nabla_{\\boldsymbol{w}}L(\\boldsymbol{w})=\\sum_{\\boldsymbol{x_i}\\in M}(\\hat{y}_i-y_i)\\boldsymbol{x_i}$$\n感知机的学习算法具体采用的是随机梯度下降法，也就是极小化过程中不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。所以权重$\\boldsymbol{w}$的更新公式为\n$$\\boldsymbol w \\leftarrow \\boldsymbol w+\\Delta \\boldsymbol w$$\n$$\\Delta \\boldsymbol w=-\\eta(\\hat{y}_i-y_i)\\boldsymbol x_i=\\eta(y_i-\\hat{y}_i)\\boldsymbol x_i$$\n相应地，$\\boldsymbol{w}$中的某个分量$w_i$的更新公式即为公式(5.2)。\n\n## 5.10\n$$\\begin{aligned}\ng_j&=-\\frac{\\partial {E_k}}{\\partial{\\hat{y}_j^k}} \\cdot \\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n\\\\&=-( \\hat{y}_j^k-y_j^k ) f ^{\\prime} (\\beta_j-\\theta_j)\n\\\\&=\\hat{y}_j^k(1-\\hat{y}_j^k)(y_j^k-\\hat{y}_j^k)\n\\end{aligned}$$\n[推导]：参见公式(5.12)\n\n## 5.12\n$$\\Delta \\theta_j = -\\eta g_j$$\n[推导]：因为\n$$\\Delta \\theta_j = -\\eta \\cfrac{\\partial E_k}{\\partial \\theta_j}$$\n又\n$$\n\\begin{aligned}	\n\\cfrac{\\partial E_k}{\\partial \\theta_j} &= \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot\\cfrac{\\partial \\hat{y}_j^k}{\\partial \\theta_j} \\\\\n&= \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot\\cfrac{\\partial [f(\\beta_j-\\theta_j)]}{\\partial \\theta_j} \\\\\n&=\\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot f^{\\prime}(\\beta_j-\\theta_j) \\times (-1) \\\\\n&=\\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot f\\left(\\beta_{j}-\\theta_{j}\\right)\\times\\left[1-f\\left(\\beta_{j}-\\theta_{j}\\right)\\right]  \\times (-1) \\\\\n&=\\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\hat{y}_j^k\\left(1-\\hat{y}_j^k\\right)  \\times (-1) \\\\\n&=\\cfrac{\\partial\\left[ \\cfrac{1}{2} \\sum\\limits_{j=1}^{l}\\left(\\hat{y}_{j}^{k}-y_{j}^{k}\\right)^{2}\\right]}{\\partial \\hat{y}_{j}^{k}} \\cdot \\hat{y}_j^k\\left(1-\\hat{y}_j^k\\right) \\times (-1)  \\\\\n&=\\cfrac{1}{2}\\times 2(\\hat{y}_j^k-y_j^k)\\times 1 \\cdot\\hat{y}_j^k\\left(1-\\hat{y}_j^k\\right)  \\times (-1) \\\\\n&=(y_j^k-\\hat{y}_j^k)\\hat{y}_j^k\\left(1-\\hat{y}_j^k\\right)  \\\\\n&= g_j\n\\end{aligned}\n$$\n所以\n$$\\Delta \\theta_j = -\\eta \\cfrac{\\partial E_k}{\\partial \\theta_j}=-\\eta g_j$$\n\n## 5.13\n$$\\Delta v_{ih} = \\eta e_h x_i$$\n[推导]：因为\n$$\\Delta v_{ih} = -\\eta \\cfrac{\\partial E_k}{\\partial v_{ih}}$$\n又\n$$\n\\begin{aligned}	\n\\cfrac{\\partial E_k}{\\partial v_{ih}} &= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\alpha_h} \\cdot \\cfrac{\\partial \\alpha_h}{\\partial v_{ih}} \\\\\n&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\alpha_h} \\cdot x_i \\\\ \n&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot f^{\\prime}(\\alpha_h-\\gamma_h) \\cdot x_i \\\\\n&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot w_{hj} \\cdot f^{\\prime}(\\alpha_h-\\gamma_h) \\cdot x_i \\\\\n&= \\sum_{j=1}^{l} (-g_j) \\cdot w_{hj} \\cdot f^{\\prime}(\\alpha_h-\\gamma_h) \\cdot x_i \\\\\n&= -f^{\\prime}(\\alpha_h-\\gamma_h) \\cdot \\sum_{j=1}^{l} g_j \\cdot w_{hj}  \\cdot x_i\\\\\n&= -b_h(1-b_h) \\cdot \\sum_{j=1}^{l} g_j \\cdot w_{hj}  \\cdot x_i \\\\\n&= -e_h \\cdot x_i\n\\end{aligned}\n$$\n所以\n$$\\Delta v_{ih} =-\\eta \\cfrac{\\partial E_k}{\\partial v_{ih}} =\\eta e_h x_i$$\n\n## 5.14\n$$\\Delta \\gamma_h= -\\eta e_h$$\n[推导]：因为\n$$\\Delta \\gamma_h = -\\eta \\cfrac{\\partial E_k}{\\partial \\gamma_h}$$\n又\n$$\n\\begin{aligned}	\n\\cfrac{\\partial E_k}{\\partial \\gamma_h} &= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot \\cfrac{\\partial b_h}{\\partial \\gamma_h} \\\\\n&= \\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot \\cfrac{\\partial \\beta_j}{\\partial b_h} \\cdot f^{\\prime}(\\alpha_h-\\gamma_h) \\cdot (-1) \\\\\n&= -\\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot w_{hj} \\cdot f^{\\prime}(\\alpha_h-\\gamma_h)\\\\\n&= -\\sum_{j=1}^{l} \\cfrac{\\partial E_k}{\\partial \\hat{y}_j^k} \\cdot \\cfrac{\\partial \\hat{y}_j^k}{\\partial \\beta_j} \\cdot w_{hj} \\cdot b_h(1-b_h)\\\\\n&= \\sum_{j=1}^{l}g_j\\cdot w_{hj} \\cdot b_h(1-b_h)\\\\\n&=e_h\n\\end{aligned}\n$$\n所以\n$$\\Delta \\gamma_h=-\\eta\\cfrac{\\partial E_k}{\\partial \\gamma_h} = -\\eta e_h$$\n\n## 5.15\n$$\\begin{aligned}\ne_h&=-\\frac{\\partial {E_k}}{\\partial{b_h}}\\cdot \\frac{\\partial{b_h}}{\\partial{\\alpha_h}}\n\\\\&=-\\sum_{j=1}^l \\frac{\\partial {E_k}}{\\partial{\\beta_j}}\\cdot \\frac{\\partial{\\beta_j}}{\\partial{b_h}}f^{\\prime}(\\alpha_h-\\gamma_h)\n\\\\&=\\sum_{j=1}^l w_{hj}g_j f^{\\prime}(\\alpha_h-\\gamma_h)\n\\\\&=b_h(1-b_h)\\sum_{j=1}^l w_{hj}g_j \n\\end{aligned}$$\n[推导]：参见公式(5.13)\n\n## 5.20\n$$E(\\boldsymbol{s})=-\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}w_{ij}s_is_j-\\sum_{i=1}^n\\theta_is_i$$\n[解析]：Boltzmann机本质上是一个引入了隐变量的无向图模型，无向图的能量可理解为\n$$E_{graph}=E_{edges}+E_{nodes}$$\n其中，$E_{graph}$表示图的能量，$E_{edges}$表示图中边的能量，$E_{nodes}$表示图中结点的能量；边能量由两连接结点的值及其权重的乘积确定：$E_{{edge}_{ij}}=-w_{ij}s_is_j$，结点能量由结点的值及其阈值的乘积确定：$E_{{node}_i}=-\\theta_is_i$；图中边的能量为图中所有边能量之和\n$$E_{edges}=\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}E_{{edge}_{ij}}=-\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}w_{ij}s_is_j$$\n图中结点的能量为图中所有结点能量之和\n$$E_{nodes}=\\sum_{i=1}^nE_{{node}_i}=-\\sum_{i=1}^n\\theta_is_i$$\n故状态向量$\\boldsymbol{s}$所对应的Boltzmann机能量为\n$$E_{graph}=E_{edges}+E_{nodes}=-\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}w_{ij}s_is_j-\\sum_{i=1}^n\\theta_is_i$$\n\n## 5.22\n$$P(\\boldsymbol{v}|\\boldsymbol{h})=\\prod_{i=1}^dP(v_i\\,  |  \\, \\boldsymbol{h})$$\n[解析]：受限Boltzmann机仅保留显层与隐层之间的连接，显层的状态向量为$\\boldsymbol{v}$，隐层的状态向量为$\\boldsymbol{h}$。\n$$\\boldsymbol{v}=\\left[\\begin{array}{c}v_1\\\\ v_2\\\\ \\vdots\\\\ v_d\\\\\\end{array} \\right]\\qquad\n\\boldsymbol{h}=\\left[\\begin{array}{c}h_1\\\\h_2\\\\ \\vdots\\\\ h_q\\end{array} \\right]$$\n对于显层状态向量$\\boldsymbol{v}$中的变量$v_i$，其仅与隐层状态向量$\\boldsymbol{h}$有关，所以给定隐层状态向量$\\boldsymbol{h}$，$v_1,v_2,...,v_d$相互独立。\n\n## 5.23\n$$P(\\boldsymbol{h}|\\boldsymbol{v})=\\prod_{j=1}^qP(h_i\\,  |  \\, \\boldsymbol{v})$$\n[解析]：由公式5.22的解析同理可得：给定显层状态向量$\\boldsymbol{v}$，$h_1,h_2,\\cdots,h_q$相互独立。\n\n## 5.24\n$$\\Delta w=\\eta(\\boldsymbol{v}\\boldsymbol{h}^\\mathrm{T}-\\boldsymbol{v}’\\boldsymbol{h}’^{\\mathrm{T}})$$\n[推导]：由公式(5.20)可推导出受限Boltzmann机（以下简称RBM）的能量函数为：\n$$\\begin{aligned}\nE(\\boldsymbol{v},\\boldsymbol{h})&=-\\sum_{i=1}^d\\sum_{j=1}^qw_{ij}v_ih_j-\\sum_{i=1}^d\\alpha_iv_i-\\sum_{j=1}^q\\beta_jh_j \\\\\n&=-\\boldsymbol{h}^{\\mathrm{T}}\\mathbf{W}\\boldsymbol{v}-\\boldsymbol{\\alpha}^{\\mathrm{T}}\\boldsymbol{v}-\\boldsymbol{\\beta}^{\\mathrm{T}}\\boldsymbol{h}\n\\end{aligned}$$\n其中\n$$\\mathbf{W}=\\begin{bmatrix}\n\\boldsymbol{w}_1\\\\\n\\boldsymbol{w}_2\\\\ \n\\vdots\\\\\n\\boldsymbol{w}_q\n\\end{bmatrix}\\in \\mathbb{R}^{q*d}$$\n再由公式(5.21)可知，RBM的联合概率分布为\n$$P(\\boldsymbol{v},\\boldsymbol{h})=\\frac{1}{Z}e^{-E(\\boldsymbol{v},\\boldsymbol{h})}$$\n其中$Z$为规范化因子\n$$Z=\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E(\\boldsymbol{v},\\boldsymbol{h})}$$\n给定含$m$个独立同分布数据的数据集$V=\\{\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_m\\}$，记$\\boldsymbol{\\theta}=\\{\\mathbf{W},\\boldsymbol{\\alpha},\\boldsymbol{\\beta}\\}$，学习RBM的策略是求出参数$\\boldsymbol{\\theta}$的值，使得如下对数似然函数最大化\n$$\\begin{aligned}\nL(\\boldsymbol{\\theta})&=\\ln\\left(\\prod_{k=1}^{m}P(\\boldsymbol{v}_k)\\right) \\\\\n&=\\sum_{k=1}^m\\ln P(\\boldsymbol{v}_k) \\\\\n&= \\sum_{k=1}^m L_k(\\boldsymbol{\\theta})\n\\end{aligned}$$\n具体采用的是梯度上升法来求解参数$\\boldsymbol{\\theta}$，因此，下面来考虑求对数似然函数$L(\\boldsymbol{\\theta})$的梯度。对于$V$中的任意一个样本$\\boldsymbol{v}_k$来说，其$L_k(\\boldsymbol{\\theta})$的具体形式为\n$$\\begin{aligned}\nL_k(\\boldsymbol{\\theta})&=\\ln P(\\boldsymbol{v}_k)\n\\\\&=\\ln\\left(\\sum_{\\boldsymbol{h}}P(\\boldsymbol{v}_k,\\boldsymbol{h})\\right)\n\\\\&=\\ln\\left(\\sum_{\\boldsymbol{h}}\\frac{1}{Z}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\right)\n\\\\&=\\ln\\left(\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\right)-\\ln Z\n\\\\&=\\ln\\left(\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\right)-\\ln\\left(\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}\\right)\n\\end{aligned}$$\n对$L_k(\\boldsymbol{\\theta})$进行求导\n$$\n\\begin{aligned}\n\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{\\boldsymbol{\\theta}}}&=\\frac{\\partial}{\\partial{\\boldsymbol{\\theta}}}\\left[\\ln\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\right]-\\frac{\\partial}{\\partial{\\boldsymbol{\\theta}}}\\left[\\ln\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}\\right]\n\\\\&=-\\frac{\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}}{\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}}+\\frac{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E(\\boldsymbol{v},\\boldsymbol{h})}\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}}{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E(\\boldsymbol{v},\\boldsymbol{h})}}\n\\\\&=-\\sum_{\\boldsymbol{h}}\\frac{e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}}{\\sum_{\\boldsymbol{h}}e^{-E(\\boldsymbol{v}_k,\\boldsymbol{h})}}+\\sum_{\\boldsymbol{v},\\boldsymbol{h}}\\frac{e^{-E(\\boldsymbol{v},\\boldsymbol{h})}\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}}{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E(\\boldsymbol{v},\\boldsymbol{h})}}\n\\end{aligned}\n$$\n由于\n$$\\frac{e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\sum_{\\boldsymbol{h}}e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}=\\frac{\\frac{e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{Z}}{\\frac{\\sum_{\\boldsymbol{h}}e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{Z}}=\\frac{\\frac{e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{Z}}{\\sum_{\\boldsymbol{h}}\\frac{e^{-E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{Z}}=\\frac{P(\\boldsymbol{v}_k,\\boldsymbol{h})}{\\sum_{\\boldsymbol{h}}P(\\boldsymbol{v}_k,\\boldsymbol{h})}=P(\\boldsymbol{h}|\\boldsymbol{v}_k)\n$$\n$$\\frac{e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}=\\frac{\\frac{e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}{Z}}{\\frac{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}{Z}}=\\frac{\\frac{e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}{Z}}{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}\\frac{e^{-E({\\boldsymbol{v},\\boldsymbol{h})}}}{Z}}=\\frac{P(\\boldsymbol{v},\\boldsymbol{h})}{\\sum_{\\boldsymbol{v},\\boldsymbol{h}}P(\\boldsymbol{v},\\boldsymbol{h})}=P(\\boldsymbol{v},\\boldsymbol{h})$$\n故\n$$\n\\begin{aligned}\n\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{\\boldsymbol{\\theta}}}&=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}+\\sum_{\\boldsymbol{v},\\boldsymbol{h}}P(\\boldsymbol{v},\\boldsymbol{h})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}\n\\\\&=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}+\\sum_{\\boldsymbol{v}}\\sum_{\\boldsymbol{h}}P(\\boldsymbol{v})P(\\boldsymbol{h}|\\boldsymbol{v})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}\n\\\\&=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}+\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}\n\\end{aligned}$$\n由于$\\boldsymbol{\\theta}=\\{\\mathbf{W},\\boldsymbol{\\alpha},\\boldsymbol{\\beta}\\}$包含三个参数，在这里我们仅以$\\mathbf{W}$中的任意一个分量$w_{ij}$为例进行详细推导。首先将上式中的$\\boldsymbol{\\theta}$替换为$w_{ij}$可得\n$$\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{w_{ij}}}=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{w_{ij}}}+\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{w_{ij}}}$$\n根据公式(5.23)可知\n$$\\begin{aligned}\n&\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{w_{ij}}}\\\\\n=&-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v})h_iv_j\\\\\n=&-\\sum_{\\boldsymbol{h}}\\prod_{l=1}^{q}P(h_l|\\boldsymbol{v})h_iv_j\\\\\n=&-\\sum_{\\boldsymbol{h}}P(h_i|\\boldsymbol{v})\\prod_{l=1,l\\neq i}^{q}P(h_l|\\boldsymbol{v})h_iv_j\\\\\n=&-\\sum_{\\boldsymbol{h}}P(h_i|\\boldsymbol{v})P(h_1,...,h_{i-1},h_{i+1},...,h_q|\\boldsymbol{v})h_iv_j\\\\\n=&-\\sum_{h_i}P(h_i|\\boldsymbol{v})h_iv_j\\sum_{h_1,...,h_{i-1},h_{i+1},...,h_q}P(h_1,...,h_{i-1},h_{i+1},...,h_q|\\boldsymbol{v})\\\\\n=&-\\sum_{h_i}P(h_i|\\boldsymbol{v})h_iv_j\\cdot 1\\\\\n=&-\\left[P(h_i=0|\\boldsymbol{v})\\cdot0\\cdot v_j+P(h_i=1|\\boldsymbol{v})\\cdot 1\\cdot v_j\\right]\\\\\n=&-P(h_i=1|\\boldsymbol{v})v_j\n\\end{aligned}$$\n同理可推得\n$$\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{w_{ij}}}=-P(h_i=1|\\boldsymbol{v}_k)v_j^k$$\n将以上两式代回$\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{w_{ij}}}$中可得\n$$\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{w_{ij}}}=P(h_i=1|\\boldsymbol{v}_k){v_{j}^k}-\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})P(h_i=1|\\boldsymbol{v})v_j$$\n观察此式可知，通过枚举所有可能的$\\boldsymbol{v}$来计算$\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})P(h_i=1|\\boldsymbol{v})v_j$的复杂度太高，因此可以考虑求其近似值来简化计算。具体地，RBM通常采用的是西瓜书上所说的“对比散度”（Contrastive Divergence，简称CD）算法。CD算法的核心思想<sup>[2]</sup>是：用步长为$s$（通常设为1）的CD算法\n$$CD_s(\\boldsymbol{\\theta},\\boldsymbol{v})=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}^{(0)})\\frac{\\partial{E({\\boldsymbol{v}^{(0)},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}+\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}^{(s)})\\frac{\\partial{E({\\boldsymbol{v}^{(s)},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}$$\n近似代替\n$$\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{\\boldsymbol{\\theta}}}=-\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v}_k)\\frac{\\partial{E({\\boldsymbol{v}_k,\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}+\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})\\sum_{\\boldsymbol{h}}P(\\boldsymbol{h}|\\boldsymbol{v})\\frac{\\partial{E({\\boldsymbol{v},\\boldsymbol{h})}}}{\\partial{\\boldsymbol{\\theta}}}$$\n由此可知对于$w_{ij}$来说，就是用\n$$CD_s(w_{ij},\\boldsymbol{v})=P(h_i=1|\\boldsymbol{v}^{(0)}){v_{j}^{(0)}}-P(h_i=1|\\boldsymbol{v}^{(s)})v_j^{(s)}$$\n近似代替\n$$\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{w_{ij}}}=P(h_i=1|\\boldsymbol{v}_k){v_{j}^k}-\\sum_{\\boldsymbol{v}}P(\\boldsymbol{v})P(h_i=1|\\boldsymbol{v})v_j$$\n令$\\Delta w_{ij}:=\\frac{\\partial{L_k(\\boldsymbol{\\theta})}}{\\partial{w_{ij}}}$，$RBM(\\boldsymbol\\theta)$表示参数为$\\boldsymbol{\\theta}$的RBM网络，则$CD_s(w_{ij},\\boldsymbol{v})$的具体算法可表示为：\n- 输入：$s,V=\\{\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\cdots,\\boldsymbol{v}_m\\},RBM(\\boldsymbol\\theta)$\n- 过程：\n    1. 初始化：$\\Delta w_{ij}=0$\n    2. $for\\quad \\boldsymbol{v}\\in V \\quad do$\n    3. $\\quad \\boldsymbol{v}^{(0)}:=\\boldsymbol{v}$\n    4. $\\quad for\\quad t=1,2,...,s-1\\quad do$\n    5. $\\qquad \\boldsymbol{h}^{(t)}=h\\_given\\_v(\\boldsymbol{v}^{(t)},RBM(\\boldsymbol\\theta))$\n    6. $\\qquad \\boldsymbol{v}^{(t+1)}=v\\_given\\_h(\\boldsymbol{h}^{(t)},RBM(\\boldsymbol\\theta))$\n    7. $\\quad end\\quad for$\n    8. $\\quad for\\quad i=1,2,...,q;j=1,2,...,d\\quad do$\n    9. $\\qquad \\Delta w_{ij}=\\Delta w_{ij}+\\left[P(h_i=1|\\boldsymbol{v}^{(0)}){v_{j}^{(0)}}-P(h_i=1|\\boldsymbol{v}^{(s)})v_j^{(s)}\\right]$\n    10. $\\quad end\\quad for$\n    11. $end\\quad for$\n- 输出：$\\Delta w_{ij}$\n\n其中函数$h\\_given\\_v(\\boldsymbol{v},RBM(\\boldsymbol\\theta))$表示在给定$\\boldsymbol{v}$的条件下，从$RBM(\\boldsymbol\\theta)$中采样生成$\\boldsymbol{h}$，同理，函数$v\\_given\\_h(\\boldsymbol{h},RBM(\\boldsymbol\\theta))$表示在给定$\\boldsymbol{h}$的条件下，从$RBM(\\boldsymbol\\theta)$中采样生成$\\boldsymbol{v}$。由于两个函数的算法可以互相类比推得，因此，下面仅给出函数$h\\_given\\_v(\\boldsymbol{v},RBM(\\boldsymbol\\theta))$的具体算法：\n- 输入：$\\boldsymbol{v},RBM(\\boldsymbol\\theta)$\n- 过程：\n    1. $for \\quad i=1,2,...,q \\quad do$\n    2. $\\quad \\text{随机生成} 0\\leq\\alpha_i\\leq 1$\n    3. $\\quad h_{j}=\\left\\{\\begin{array}{ll}1, & \\text { if } \\alpha_{i}<P(h_i=1|\\boldsymbol{v}) \\\\ 0, & \\text { otherwise }\\end{array}\\right.$\n    4. $end \\quad for$\n- 输出：$\\boldsymbol{h}=(h_1,h_2,...,h_q)^{\\mathrm{T}}$\n\n综上可知，公式(5.24)其实就是带有学习率为$\\eta$的$\\Delta w_{ij}$的一种形式化的表示。\n\n## 附录\n### ①数据集的线性可分<sup>[1]</sup>\n给定一个数据集\n$$T=\\{(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),...,(\\boldsymbol{x}_N,y_N)\\}$$\n其中，$\\boldsymbol{x}_i\\in \\mathbb{R}^n,y_i\\in\\{0,1\\},i=1,2,...,N$，如果存在某个超平面\n$$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x} +b=0 $$\n能将数据集$T$中的正样本和负样本完全正确地划分到超平面两侧，即对所有$y_i=1$的样本$\\boldsymbol{x}_i$，有$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x}_i +b\\geq0$，对所有$y_i=0$的样本$\\boldsymbol{x}_i$，有$\\boldsymbol{w}^{\\mathrm{T}}\\boldsymbol{x} _i+b<0$，则称数据集$T$线性可分，否则称数据集$T$线性不可分。\n## 参考文献\n[1]李航编著.统计学习方法[M].清华大学出版社,2012.<br>\n[2]https://blog.csdn.net/itplus/article/details/19408143','2021-12-10 12:43:49','2021-12-19 13:22:21'),
	(6,1,'chapter6','## 6.9\n$$\\boldsymbol{w} = \\sum_{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i$$\n[推导]：公式(6.8)可作如下展开\n$$\\begin{aligned}\nL(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) &= \\frac{1}{2}||\\boldsymbol{w}||^2+\\sum_{i=1}^m\\alpha_i(1-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)) \\\\\n& =  \\frac{1}{2}||\\boldsymbol{w}||^2+\\sum_{i=1}^m(\\alpha_i-\\alpha_iy_i \\boldsymbol{w}^T\\boldsymbol{x}_i-\\alpha_iy_ib)\\\\\n& =\\frac{1}{2}\\boldsymbol{w}^T\\boldsymbol{w}+\\sum_{i=1}^m\\alpha_i -\\sum_{i=1}^m\\alpha_iy_i\\boldsymbol{w}^T\\boldsymbol{x}_i-\\sum_{i=1}^m\\alpha_iy_ib\n\\end{aligned}​$$\n对$\\boldsymbol{w}$和$b$分别求偏导数​并令其等于0\n$$\\frac {\\partial L}{\\partial \\boldsymbol{w}}=\\frac{1}{2}\\times2\\times\\boldsymbol{w} + 0 - \\sum_{i=1}^{m}\\alpha_iy_i \\boldsymbol{x}_i-0= 0 \\Longrightarrow \\boldsymbol{w}=\\sum_{i=1}^{m}\\alpha_iy_i \\boldsymbol{x}_i$$\n\n$$\\frac {\\partial L}{\\partial b}=0+0-0-\\sum_{i=1}^{m}\\alpha_iy_i=0  \\Longrightarrow  \\sum_{i=1}^{m}\\alpha_iy_i=0$$\n值得一提的是，上述求解过程遵循的是西瓜书附录B中公式(B.7)左边的那段话“在推导对偶问题时，常通过将拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$对$\\boldsymbol{x}$求导并令导数为0，来获得对偶函数的表达形式”。那么这段话背后的缘由是啥呢？在这里我认为有两种说法可以进行解释：\n1. 对于强对偶性成立的优化问题，其主问题的最优解$\\boldsymbol{x}^*$一定满足附录①给出的KKT条件（证明参见参考文献[3]的§ 5.5），而KKT条件中的条件(1)就要求最优解$\\boldsymbol{x}^*$能使得拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$关于$\\boldsymbol{x}$的一阶导数等于0；\n2. 对于任意优化问题，若拉格朗日函数$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$是关于$\\boldsymbol{x}$的凸函数，那么此时对$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$关于$\\boldsymbol{x}$求导并令导数等于0解出来的点一定是最小值点。根据对偶函数的定义可知，将最小值点代回$L(\\boldsymbol{x},\\boldsymbol{\\lambda},\\boldsymbol{\\mu})$即可得到对偶函数。\n\n显然，对于SVM来说，从以上任意一种说法都能解释得通。\n\n## 6.10\n$$0=\\sum_{i=1}^m\\alpha_iy_i$$\n[解析]：参见公式(6.9)\n\n## 6.11\n$$\\begin{aligned}\n\\max_{\\boldsymbol{\\alpha}} & \\sum_{i=1}^m\\alpha_i - \\frac{1}{2}\\sum_{i = 1}^m\\sum_{j=1}^m\\alpha_i \\alpha_j y_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j \\\\\n\\text { s.t. } & \\sum_{i=1}^m \\alpha_i y_i =0 \\\\ \n& \\alpha_i \\geq 0 \\quad i=1,2,\\dots ,m\n\\end{aligned}$$  \n[推导]：将公式(6.9)和公式(6.10)代入公式(6.8)即可将$L(\\boldsymbol{w},b,\\boldsymbol{\\alpha})$中的$\\boldsymbol{w}$和$b$消去，再考虑公式(6.10)的约束，就得到了公式(6.6)的对偶问题\n$$\\begin{aligned}\n\\inf_{\\boldsymbol{w},b} L(\\boldsymbol{w},b,\\boldsymbol{\\alpha})  &=\\frac{1}{2}\\boldsymbol{w}^T\\boldsymbol{w}+\\sum_{i=1}^m\\alpha_i -\\sum_{i=1}^m\\alpha_iy_i\\boldsymbol{w}^T\\boldsymbol{x}_i-\\sum_{i=1}^m\\alpha_iy_ib \\\\\n&=\\frac {1}{2}\\boldsymbol{w}^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i-\\boldsymbol{w}^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_\ni -b\\sum _{i=1}^m\\alpha_iy_i \\\\\n& = -\\frac {1}{2}\\boldsymbol{w}^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_i -b\\sum _{i=1}^m\\alpha_iy_i\n\\end{aligned}$$\n由于$\\sum\\limits_{i=1}^{m}\\alpha_iy_i=0$，所以上式最后一项可化为0，于是得\n$$\\begin{aligned}\n\\inf_{\\boldsymbol{w},b} L(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) &= -\\frac {1}{2}\\boldsymbol{w}^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_i \\\\\n&=-\\frac {1}{2}(\\sum_{i=1}^{m}\\alpha_iy_i\\boldsymbol{x}_i)^T(\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i)+\\sum _{i=1}^m\\alpha_i \\\\\n&=-\\frac {1}{2}\\sum_{i=1}^{m}\\alpha_iy_i\\boldsymbol{x}_i^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_i \\\\\n&=\\sum _{i=1}^m\\alpha_i-\\frac {1}{2}\\sum_{i=1 }^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j\n\\end{aligned}$$\n所以\n$$\\max_{\\boldsymbol{\\alpha}}\\inf_{\\boldsymbol{w},b} L(\\boldsymbol{w},b,\\boldsymbol{\\alpha})=\\max_{\\boldsymbol{\\alpha}} \\sum_{i=1}^m\\alpha_i - \\frac{1}{2}\\sum_{i = 1}^m\\sum_{j=1}^m\\alpha_i \\alpha_j y_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j $$\n\n## 6.13\n$$\\left\\{\\begin{array}{l}\\alpha_{i} \\geqslant 0 \\\\ y_{i} f\\left(\\boldsymbol{x}_{i}\\right)-1 \\geqslant 0 \\\\ \\alpha_{i}\\left(y_{i} f\\left(\\boldsymbol{x}_{i}\\right)-1\\right)=0\\end{array}\\right.$$\n[解析]：参见公式(6.9)中给出的第1点理由\n\n## 6.35\n$$\\begin{aligned}\n\\min _{\\boldsymbol{w}, b, \\xi_{i}} & \\frac{1}{2}\\|\\boldsymbol{w}\\|^{2}+C \\sum_{i=1}^{m} \\xi_{i} \\\\\n \\text { s.t. } & y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1-\\xi_{i} \\\\ & \\xi_{i} \\geqslant 0, i=1,2, \\ldots, m \\end{aligned}$$\n[解析]：令\n$$\\max \\left(0,1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\right)=\\xi_{i}$$\n显然$\\xi_i\\geq 0$，而且当$1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)>0$时\n$$1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)=\\xi_i$$\n当$1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\leq 0$时\n$$\\xi_i = 0$$\n所以综上可得\n$$1-y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right)\\leq\\xi_i\\Rightarrow y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1-\\xi_{i}$$\n\n## 6.37\n$$\\boldsymbol{w}=\\sum_{i=1}^{m}\\alpha_{i}y_{i}\\boldsymbol{x}_{i}$$\n[解析]：参见公式(6.9)\n\n## 6.38\n$$0=\\sum_{i=1}^{m}\\alpha_{i}y_{i}$$\n[解析]：参见公式(6.10)\n\n## 6.39\n$$ C=\\alpha_i +\\mu_i $$\n[推导]：对公式(6.36)关于$\\xi_i$求偏导并令其等于0可得：\n$$\\frac{\\partial L}{\\partial \\xi_i}=0+C \\times 1 - \\alpha_i \\times 1-\\mu_i\n\\times 1 =0\\Longrightarrow C=\\alpha_i +\\mu_i$$\n\n## 6.40\n$$\\begin{aligned}\n\\max_{\\boldsymbol{\\alpha}}&\\sum _{i=1}^m\\alpha_i-\\frac {1}{2}\\sum_{i=1 }^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j \\\\\n s.t. &\\sum_{i=1}^m \\alpha_i y_i=0 \\\\ \n &  0 \\leq\\alpha_i \\leq C \\quad i=1,2,\\dots ,m\n \\end{aligned}$$\n将公式(6.37)-(6.39)代入公式(6.36)可以得到公式(6.35)的对偶问题：\n$$\\begin{aligned}\n &\\frac{1}{2}||\\boldsymbol{w}||^2+C\\sum_{i=1}^m \\xi_i+\\sum_{i=1}^m \\alpha_i(1-\\xi_i-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b))-\\sum_{i=1}^m\\mu_i \\xi_i  \\\\\n=&\\frac{1}{2}||\\boldsymbol{w}||^2+\\sum_{i=1}^m\\alpha_i(1-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b))+C\\sum_{i=1}^m \\xi_i-\\sum_{i=1}^m \\alpha_i \\xi_i-\\sum_{i=1}^m\\mu_i \\xi_i \\\\\n=&-\\frac {1}{2}\\sum_{i=1}^{m}\\alpha_iy_i\\boldsymbol{x}_i^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_i +\\sum_{i=1}^m C\\xi_i-\\sum_{i=1}^m \\alpha_i \\xi_i-\\sum_{i=1}^m\\mu_i \\xi_i \\\\\n=&-\\frac {1}{2}\\sum_{i=1}^{m}\\alpha_iy_i\\boldsymbol{x}_i^T\\sum _{i=1}^m\\alpha_iy_i\\boldsymbol{x}_i+\\sum _{i=1}^m\\alpha_i +\\sum_{i=1}^m (C-\\alpha_i-\\mu_i)\\xi_i \\\\\n=&\\sum _{i=1}^m\\alpha_i-\\frac {1}{2}\\sum_{i=1 }^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j\\\\\n=&\\min_{\\boldsymbol{w},b,\\boldsymbol{\\xi}}L(\\boldsymbol{w},b,\\boldsymbol{\\alpha},\\boldsymbol{\\xi},\\boldsymbol{\\mu}) \n\\end{aligned}$$\n所以\n$$\\begin{aligned}\n\\max_{\\boldsymbol{\\alpha},\\boldsymbol{\\mu}} \\min_{\\boldsymbol{w},b,\\boldsymbol{\\xi}}L(\\boldsymbol{w},b,\\boldsymbol{\\alpha},\\boldsymbol{\\xi},\\boldsymbol{\\mu})&=\\max_{\\boldsymbol{\\alpha},\\boldsymbol{\\mu}}\\sum _{i=1}^m\\alpha_i-\\frac {1}{2}\\sum_{i=1 }^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j \\\\\n&=\\max_{\\boldsymbol{\\alpha}}\\sum _{i=1}^m\\alpha_i-\\frac {1}{2}\\sum_{i=1 }^{m}\\sum_{j=1}^{m}\\alpha_i\\alpha_jy_iy_j\\boldsymbol{x}_i^T\\boldsymbol{x}_j \n\\end{aligned}$$\n又\n$$\\begin{aligned}\n\\alpha_i &\\geq 0 \\\\\n\\mu_i &\\geq 0 \\\\\nC &= \\alpha_i+\\mu_i\n\\end{aligned}$$\n消去$\\mu_i$可得等价约束条件为：\n$$0 \\leq\\alpha_i \\leq C \\quad i=1,2,\\dots ,m$$\n\n## 6.41\n$$\\left\\{\\begin{array}{l}\\alpha_{i} \\geqslant 0, \\quad \\mu_{i} \\geqslant 0 \\\\ y_{i} f\\left(\\boldsymbol{x}_{i}\\right)-1+\\xi_{i} \\geqslant 0 \\\\ \\alpha_{i}\\left(y_{i} f\\left(\\boldsymbol{x}_{i}\\right)-1+\\xi_{i}\\right)=0 \\\\ \\xi_{i} \\geqslant 0, \\mu_{i} \\xi_{i}=0\\end{array}\\right.$$\n[解析]：参见公式(6.13)\n\n## 6.52\n$$\n\\left\\{\\begin{array}{l}\n{\\alpha_{i}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}\\right)=0} \\\\ {\\hat{\\alpha}_{i}\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)-\\epsilon-\\hat{\\xi}_{i}\\right)=0} \\\\ {\\alpha_{i} \\hat{\\alpha}_{i}=0, \\xi_{i} \\hat{\\xi}_{i}=0} \\\\ \n{\\left(C-\\alpha_{i}\\right) \\xi_{i}=0,\\left(C-\\hat{\\alpha}_{i}\\right) \\hat{\\xi}_{i}=0}\n\\end{array}\\right.\n$$\n[推导]：将公式(6.45)的约束条件全部恒等变形为小于等于0的形式可得：\n$$\n\\left\\{\\begin{array}{l}\n{f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}-\\epsilon-\\xi_{i} \\leq 0 }  \\\\ \n{y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)-\\epsilon-\\hat{\\xi}_{i} \\leq 0 } \\\\ \n{-\\xi_{i} \\leq 0} \\\\\n{-\\hat{\\xi}_{i} \\leq 0}\n\\end{array}\\right.\n$$\n由于以上四个约束条件的拉格朗日乘子分别为$\\alpha_i,\\hat{\\alpha}_i,\\mu_i,\\hat{\\mu}_i$，所以由附录①可知，以上四个约束条件可相应转化为以下KKT条件：\n$$\n\\left\\{\\begin{array}{l}\n{\\alpha_i\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}-\\epsilon-\\xi_{i} \\right) = 0 }  \\\\ \n{\\hat{\\alpha}_i\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)-\\epsilon-\\hat{\\xi}_{i} \\right) = 0 } \\\\ \n{-\\mu_i\\xi_{i} = 0 \\Rightarrow \\mu_i\\xi_{i} = 0 }  \\\\\n{-\\hat{\\mu}_i \\hat{\\xi}_{i} = 0  \\Rightarrow \\hat{\\mu}_i \\hat{\\xi}_{i} = 0 }\n\\end{array}\\right.\n$$\n由公式(6.49)和公式(6.50)可知：\n$$\n\\begin{aligned}\n\\mu_i=C-\\alpha_i \\\\\n\\hat{\\mu}_i=C-\\hat{\\alpha}_i\n\\end{aligned}\n$$\n所以上述KKT条件可以进一步变形为：\n$$\n\\left\\{\\begin{array}{l}\n{\\alpha_i\\left(f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}-\\epsilon-\\xi_{i} \\right) = 0 }  \\\\ \n{\\hat{\\alpha}_i\\left(y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)-\\epsilon-\\hat{\\xi}_{i} \\right) = 0 } \\\\ \n{(C-\\alpha_i)\\xi_{i} = 0 }  \\\\\n{(C-\\hat{\\alpha}_i) \\hat{\\xi}_{i} = 0 }\n\\end{array}\\right.\n$$\n又因为样本$(\\boldsymbol{x}_i,y_i)$只可能处在间隔带的某一侧，那么约束条件$f\\left(\\boldsymbol{x}_{i}\\right)-y_{i}-\\epsilon-\\xi_{i}=0$和$y_{i}-f\\left(\\boldsymbol{x}_{i}\\right)-\\epsilon-\\hat{\\xi}_{i}=0$不可能同时成立，所以$\\alpha_i$和$\\hat{\\alpha}_i$中至少有一个为0，也即$\\alpha_i\\hat{\\alpha}_i=0$。在此基础上再进一步分析可知，如果$\\alpha_i=0$的话，那么根据约束$(C-\\alpha_i)\\xi_{i} = 0$可知此时$\\xi_i=0$，同理，如果$\\hat{\\alpha}_i=0$的话，那么根据约束$(C-\\hat{\\alpha}_i)\\hat{\\xi}_{i} = 0$可知此时$\\hat{\\xi}_i=0$，所以$\\xi_i$和$\\hat{\\xi}_i$中也是至少有一个为0，也即$\\xi_{i} \\hat{\\xi}_{i}=0$。将$\\alpha_i\\hat{\\alpha}_i=0,\\xi_{i} \\hat{\\xi}_{i}=0$整合进上述KKT条件中即可得到公式(6.52)。\n\n## 6.60\n$$\\max _{\\boldsymbol{w}} J(\\boldsymbol{w})=\\frac{\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}}{\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{w}^{\\phi} \\boldsymbol{w}}$$\n[解析]：类似于第3章的公式(3.35)。\n\n## 6.62\n$$\\mathbf{S}_{b}^{\\phi}=\\left(\\boldsymbol{\\mu}_{1}^{\\phi}-\\boldsymbol{\\mu}_{0}^{\\phi}\\right)\\left(\\boldsymbol{\\mu}_{1}^{\\phi}-\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}$$\n[解析]：类似于第3章的公式(3.34)。\n\n## 6.63\n$$\\mathbf{S}_{w}^{\\phi}=\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\phi(\\boldsymbol{x})-\\boldsymbol{\\mu}_{i}^{\\phi}\\right)\\left(\\phi(\\boldsymbol{x})-\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}}$$\n[解析]：类似于第3章的公式(3.33)。\n\n## 6.65\n$$\\boldsymbol{w}=\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)$$\n[推导]：由表示定理可知，此时二分类KLDA最终求得的投影直线方程总可以写成如下形式\n$$h(\\boldsymbol{x})=\\sum_{i=1}^{m} \\alpha_{i} \\kappa\\left(\\boldsymbol{x}, \\boldsymbol{x}_{i}\\right)$$\n又因为直线方程的固定形式为\n$$h(\\boldsymbol{x})=\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})$$\n所以\n$$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\sum_{i=1}^{m} \\alpha_{i} \\kappa\\left(\\boldsymbol{x}, \\boldsymbol{x}_{i}\\right)$$\n将$\\kappa\\left(\\boldsymbol{x}, \\boldsymbol{x}_{i}\\right)=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi(\\boldsymbol{x}_i)$代入可得\n$$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi(\\boldsymbol{x}_i)$$\n$$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\cdot\\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x}_i)$$\n由于$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})$的计算结果为标量，而标量的转置等于其本身，所以\n$$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\left(\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\right)^{\\mathrm{T}}=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\cdot\\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x}_i)$$\n$$\\boldsymbol{w}^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\boldsymbol{w}=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\cdot\\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x}_i)$$\n$$\\boldsymbol{w}=\\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x}_i)$$\n\n## 6.66\n$$\\hat{\\boldsymbol{\\mu}}_{0}=\\frac{1}{m_{0}} \\mathbf{K} \\mathbf{1}_{0}$$\n[解析]：为了详细地说明此公式的计算原理，下面首先先举例说明，然后再在例子的基础上延展出其一般形式。假设此时仅有4个样本，其中第1和第3个样本的标记为0，第2和第4个样本的标记为1，那么此时：\n$$m=4$$\n$$m_0=2,m_1=2$$\n$$X_0=\\{\\boldsymbol{x}_1,\\boldsymbol{x}_3\\},X_1=\\{\\boldsymbol{x}_2,\\boldsymbol{x}_4\\}$$\n$$\\mathbf{K}=\\left[ \\begin{array}{cccc}\n\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_1\\right) & \\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_2\\right) & \\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_3\\right) & \\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_1\\right) & \\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_2\\right) & \\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_3\\right) & \\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_1\\right) & \\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_2\\right) & \\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_3\\right) & \\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_1\\right) & \\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_2\\right) & \\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_3\\right) & \\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_4\\right)\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{4\\times 4}$$\n$$\\mathbf{1}_{0}=\\left[ \\begin{array}{c}\n1\\\\ \n0\\\\ \n1\\\\ \n0\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{4\\times 1}$$\n$$\\mathbf{1}_{1}=\\left[ \\begin{array}{c}\n0\\\\ \n1\\\\ \n0\\\\ \n1\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{4\\times 1}$$\n所以\n$$\\hat{\\boldsymbol{\\mu}}_{0}=\\frac{1}{m_{0}} \\mathbf{K} \\mathbf{1}_{0}=\\frac{1}{2}\\left[ \\begin{array}{c}\n\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_1\\right)+\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_3\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_1\\right)+\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_3\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_1\\right)+\\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_3\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_1\\right)+\\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_3\\right)\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{4\\times 1}$$\n$$\\hat{\\boldsymbol{\\mu}}_{1}=\\frac{1}{m_{1}} \\mathbf{K} \\mathbf{1}_{1}=\\frac{1}{2}\\left[ \\begin{array}{c}\n\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_2\\right)+\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_2\\right)+\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_2\\right)+\\kappa\\left(\\boldsymbol{x}_3, \\boldsymbol{x}_4\\right)\\\\ \n\\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_2\\right)+\\kappa\\left(\\boldsymbol{x}_4, \\boldsymbol{x}_4\\right)\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{4\\times 1}$$\n根据此结果易得$\\hat{\\boldsymbol{\\mu}}_{0},\\hat{\\boldsymbol{\\mu}}_{1}$的一般形式为\n$$\\hat{\\boldsymbol{\\mu}}_{0}=\\frac{1}{m_{0}} \\mathbf{K} \\mathbf{1}_{0}=\\frac{1}{m_{0}}\\left[ \\begin{array}{c}\n\\sum_{\\boldsymbol{x} \\in X_{0}}\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}\\right)\\\\ \n\\sum_{\\boldsymbol{x} \\in X_{0}}\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}\\right)\\\\ \n\\vdots\\\\ \n\\sum_{\\boldsymbol{x} \\in X_{0}}\\kappa\\left(\\boldsymbol{x}_m, \\boldsymbol{x}\\right)\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{m\\times 1}$$\n$$\\hat{\\boldsymbol{\\mu}}_{1}=\\frac{1}{m_{1}} \\mathbf{K} \\mathbf{1}_{1}=\\frac{1}{m_{1}}\\left[ \\begin{array}{c}\n\\sum_{\\boldsymbol{x} \\in X_{1}}\\kappa\\left(\\boldsymbol{x}_1, \\boldsymbol{x}\\right)\\\\ \n\\sum_{\\boldsymbol{x} \\in X_{1}}\\kappa\\left(\\boldsymbol{x}_2, \\boldsymbol{x}\\right)\\\\ \n\\vdots\\\\ \n\\sum_{\\boldsymbol{x} \\in X_{1}}\\kappa\\left(\\boldsymbol{x}_m, \\boldsymbol{x}\\right)\\\\ \n\\end{array} \\right]\\in \\mathbb{R}^{m\\times 1}$$\n\n## 6.67\n$$\\hat{\\boldsymbol{\\mu}}_{1}=\\frac{1}{m_{1}} \\mathbf{K} \\mathbf{1}_{1}$$\n[解析]：参见公式(6.66)的解析。\n\n## 6.70\n$$\\max _{\\boldsymbol{\\alpha}} J(\\boldsymbol{\\alpha})=\\frac{\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{M} \\boldsymbol{\\alpha}}{\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{N} \\boldsymbol{\\alpha}}$$\n[推导]：此公式是将公式(6.65)代入公式(6.60)后推得而来的，下面给出详细的推导过程。首先将公式(6.65)代入公式(6.60)的分子可得：\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}&=\\left(\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\right)^{\\mathrm{T}}\\cdot\\mathbf{S}_{b}^{\\phi}\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n&=\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\cdot\\mathbf{S}_{b}^{\\phi}\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n\\end{aligned}$$\n其中\n$$\\begin{aligned}\n\\mathbf{S}_{b}^{\\phi} &=\\left(\\boldsymbol{\\mu}_{1}^{\\phi}-\\boldsymbol{\\mu}_{0}^{\\phi}\\right)\\left(\\boldsymbol{\\mu}_{1}^{\\phi}-\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n&=\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right)\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right)^{\\mathrm{T}} \\\\\n&=\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right)\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\right) \\\\\n\\end{aligned}$$\n将其代入上式可得\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}=&\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\cdot\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right)\\cdot\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\right)\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n=&\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}}\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}} \\phi(\\boldsymbol{x})-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\right)\\\\\n&\\cdot\\left(\\frac{1}{m_{1}} \\sum_{\\boldsymbol{x} \\in X_{1}} \\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi\\left(\\boldsymbol{x}_{i}\\right)-\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\sum_{i=1}^{m} \\alpha_{i} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi\\left(\\boldsymbol{x}_{i}\\right)\\right) \\\\\n\\end{aligned}$$\n由于$\\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)=\\phi(\\boldsymbol{x}_i)^{\\mathrm{T}}\\phi(\\boldsymbol{x})$为标量，所以其转置等于本身，也即$\\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)=\\phi(\\boldsymbol{x}_i)^{\\mathrm{T}}\\phi(\\boldsymbol{x})=\\left(\\phi(\\boldsymbol{x}_i)^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\right)^{\\mathrm{T}}=\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi(\\boldsymbol{x}_i)=\\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)^{\\mathrm{T}}$，将其代入上式可得\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}=&\\left(\\frac{1}{m_{1}} \\sum_{i=1}^{m}\\sum_{\\boldsymbol{x} \\in X_{1}}\\alpha_{i} \\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)-\\frac{1}{m_{0}} \\sum_{i=1}^{m} \\sum_{\\boldsymbol{x} \\in X_{0}}  \\alpha_{i} \\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)\\right)\\\\\n&\\cdot\\left(\\frac{1}{m_{1}} \\sum_{i=1}^{m}\\sum_{\\boldsymbol{x} \\in X_{1}} \\alpha_{i} \\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)-\\frac{1}{m_{0}}\\sum_{i=1}^{m}  \\sum_{\\boldsymbol{x} \\in X_{0}} \\alpha_{i} \\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)\\right)\n\\end{aligned}$$\n令$\\boldsymbol{\\alpha}=(\\alpha_1;\\alpha_2;...;\\alpha_m)^{\\mathrm{T}}\\in \\mathbb{R}^{m\\times 1}$，同时结合公式(6.66)的解析中得到的$\\hat{\\boldsymbol{\\mu}}_{0},\\hat{\\boldsymbol{\\mu}}_{1}$的一般形式，上式可以化简为\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}&=\\left(\\boldsymbol{\\alpha}^{\\mathrm{T}}\\hat{\\boldsymbol{\\mu}}_{1}-\\boldsymbol{\\alpha}^{\\mathrm{T}}\\hat{\\boldsymbol{\\mu}}_{0}\\right)\\cdot\\left(\\hat{\\boldsymbol{\\mu}}_{1}^{\\mathrm{T}}\\boldsymbol{\\alpha}-\\hat{\\boldsymbol{\\mu}}_{0}^{\\mathrm{T}}\\boldsymbol{\\alpha}\\right)\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}}\\cdot\\left(\\hat{\\boldsymbol{\\mu}}_{1}-\\hat{\\boldsymbol{\\mu}}_{0}\\right)\\cdot\\left(\\hat{\\boldsymbol{\\mu}}_{1}^{\\mathrm{T}}-\\hat{\\boldsymbol{\\mu}}_{0}^{\\mathrm{T}}\\right)\\cdot\\boldsymbol{\\alpha}\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}}\\cdot\\left(\\hat{\\boldsymbol{\\mu}}_{1}-\\hat{\\boldsymbol{\\mu}}_{0}\\right)\\cdot\\left(\\hat{\\boldsymbol{\\mu}}_{1}-\\hat{\\boldsymbol{\\mu}}_{0}\\right)^{\\mathrm{T}}\\cdot\\boldsymbol{\\alpha}\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{M} \\boldsymbol{\\alpha}\\\\\n\\end{aligned}$$\n以上便是公式(6.70)分子部分的推导，下面继续推导公式(6.70)的分母部分。将公式(6.65)代入公式(6.60)的分母可得：\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{w}^{\\phi} \\boldsymbol{w}&=\\left(\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)\\right)^{\\mathrm{T}}\\cdot\\mathbf{S}_{w}^{\\phi}\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n&=\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\cdot\\mathbf{S}_{w}^{\\phi}\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n\\end{aligned}$$\n其中\n$$\\begin{aligned}\n\\mathbf{S}_{w}^{\\phi}&=\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\phi(\\boldsymbol{x})-\\boldsymbol{\\mu}_{i}^{\\phi}\\right)\\left(\\phi(\\boldsymbol{x})-\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n&=\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\phi(\\boldsymbol{x})-\\boldsymbol{\\mu}_{i}^{\\phi}\\right)\\left(\\phi(\\boldsymbol{x})^{\\mathrm{T}}-\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}-\\phi(\\boldsymbol{x})\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}}-\\boldsymbol{\\mu}_{i}^{\\phi}\\phi(\\boldsymbol{x})^{\\mathrm{T}}+\\boldsymbol{\\mu}_{i}^{\\phi}\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}-\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\phi(\\boldsymbol{x})\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}}-\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\boldsymbol{\\mu}_{i}^{\\phi}\\phi(\\boldsymbol{x})^{\\mathrm{T}}+\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}}\\boldsymbol{\\mu}_{i}^{\\phi}\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n\\end{aligned}$$\n由于\n$$\\begin{aligned}\n\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}} \\phi(\\boldsymbol{x})\\left(\\boldsymbol{\\mu}_{i}^{\\phi}\\right)^{\\mathrm{T}} &=\\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}+\\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n &=m_{0} \\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}+m_{1} \\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n\\sum_{i=0}^{1} \\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{\\mu}_{i}^{\\phi} \\phi(\\boldsymbol{x})^{\\mathrm{T}} &=\\sum_{i=0}^{1} \\boldsymbol{\\mu}_{i}^{\\phi} \\sum_{\\boldsymbol{x} \\in X_{i}} \\phi(\\boldsymbol{x})^{\\mathrm{T}} \\\\ &=\\boldsymbol{\\mu}_{0}^{\\phi} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}+\\boldsymbol{\\mu}_{1}^{\\phi} \\sum_{\\boldsymbol{x} \\in X_{1}} \\phi(\\boldsymbol{x})^{\\mathrm{T}} \\\\ &=m_{0} \\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}+m_{1} \\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\n\\end{aligned}$$\n所以\n$$\\begin{aligned}\n\\mathbf{S}_{w}^{\\phi}&=\\sum_{\\boldsymbol{x} \\in  D}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}-2\\left[m_0\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}+m_1\\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\\right]+m_0 \\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}+m_1 \\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}} \\\\\n&=\\sum_{\\boldsymbol{x} \\in  D}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}-m_0\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}-m_1\\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\\\\\n\\end{aligned}$$\n再将此式代回$\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}$可得\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{w}^{\\phi} \\boldsymbol{w}=&\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\cdot\\mathbf{S}_{w}^{\\phi}\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n=&\\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\cdot\\left(\\sum_{\\boldsymbol{x} \\in  D}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}-m_0\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}-m_1\\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\\right)\\cdot \\sum_{i=1}^{m} \\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\\\\n=&\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\sum_{\\boldsymbol{x} \\in  D}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right)-\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}m_0\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right)\\\\\n&-\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}m_1\\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right) \\\\\n\\end{aligned}$$\n其中，第1项可化简为\n$$\\begin{aligned}\n\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\sum_{\\boldsymbol{x} \\in  D}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\phi(\\boldsymbol{x})^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right)&=\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\sum_{\\boldsymbol{x} \\in  D}\\alpha_{i} \\alpha_{j}\\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)\\kappa\\left(\\boldsymbol{x}_j, \\boldsymbol{x}\\right)\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{K} \\mathbf{K}^{\\mathrm{T}} \\boldsymbol{\\alpha}\n\\end{aligned}$$\n第2项可化简为\n$$\\begin{aligned}\n\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}m_0\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right)&=m_0\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i}\\alpha_{j}\\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\boldsymbol{\\mu}_{0}^{\\phi}\\left(\\boldsymbol{\\mu}_{0}^{\\phi}\\right)^{\\mathrm{T}} \\phi\\left(\\boldsymbol{x}_{j}\\right)\\\\\n&=m_0\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i}\\alpha_{j}\\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right]\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})\\right]^{\\mathrm{T}} \\phi\\left(\\boldsymbol{x}_{j}\\right)\\\\\n&=m_0\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i}\\alpha_{j}\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}\\phi(\\boldsymbol{x})\\right]\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\phi(\\boldsymbol{x})^{\\mathrm{T}}\\phi\\left(\\boldsymbol{x}_{j}\\right)\\right] \\\\\n&=m_0\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i}\\alpha_{j}\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\kappa\\left(\\boldsymbol{x}_i, \\boldsymbol{x}\\right)\\right]\\left[\\frac{1}{m_{0}} \\sum_{\\boldsymbol{x} \\in X_{0}} \\kappa\\left(\\boldsymbol{x}_j, \\boldsymbol{x}\\right)\\right] \\\\\n&=m_0\\boldsymbol{\\alpha}^{\\mathrm{T}} \\hat{\\boldsymbol{\\mu}}_{0} \\hat{\\boldsymbol{\\mu}}_{0}^{\\mathrm{T}} \\boldsymbol{\\alpha}\n\\end{aligned}$$\n同理可得，第3项可化简为\n$$\\sum_{i=1}^{m}\\sum_{j=1}^{m}\\alpha_{i} \\phi\\left(\\boldsymbol{x}_{i}\\right)^{\\mathrm{T}}m_1\\boldsymbol{\\mu}_{1}^{\\phi}\\left(\\boldsymbol{\\mu}_{1}^{\\phi}\\right)^{\\mathrm{T}}\\alpha_{j} \\phi\\left(\\boldsymbol{x}_{j}\\right)=m_1\\boldsymbol{\\alpha}^{\\mathrm{T}} \\hat{\\boldsymbol{\\mu}}_{1} \\hat{\\boldsymbol{\\mu}}_{1}^{\\mathrm{T}} \\boldsymbol{\\alpha}$$\n将上述三项的化简结果代回再将此式代回$\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}$可得\n$$\\begin{aligned}\n\\boldsymbol{w}^{\\mathrm{T}} \\mathbf{S}_{b}^{\\phi} \\boldsymbol{w}&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{K} \\mathbf{K}^{\\mathrm{T}} \\boldsymbol{\\alpha}-m_0\\boldsymbol{\\alpha}^{\\mathrm{T}} \\hat{\\boldsymbol{\\mu}}_{0} \\hat{\\boldsymbol{\\mu}}_{0}^{\\mathrm{T}} \\boldsymbol{\\alpha}-m_1\\boldsymbol{\\alpha}^{\\mathrm{T}} \\hat{\\boldsymbol{\\mu}}_{1} \\hat{\\boldsymbol{\\mu}}_{1}^{\\mathrm{T}} \\boldsymbol{\\alpha}\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\cdot\\left(\\mathbf{K} \\mathbf{K}^{\\mathrm{T}} -m_0\\hat{\\boldsymbol{\\mu}}_{0} \\hat{\\boldsymbol{\\mu}}_{0}^{\\mathrm{T}} -m_1\\hat{\\boldsymbol{\\mu}}_{1} \\hat{\\boldsymbol{\\mu}}_{1}^{\\mathrm{T}} \\right)\\cdot\\boldsymbol{\\alpha}\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\cdot\\left(\\mathbf{K} \\mathbf{K}^{\\mathrm{T}}-\\sum_{i=0}^{1} m_{i} \\hat{\\boldsymbol{\\mu}}_{i} \\hat{\\boldsymbol{\\mu}}_{i}^{\\mathrm{T}} \\right)\\cdot\\boldsymbol{\\alpha}\\\\\n&=\\boldsymbol{\\alpha}^{\\mathrm{T}} \\mathbf{N}\\boldsymbol{\\alpha}\\\\\n\\end{aligned}$$\n\n## 附录\n### ①KKT条件<sup>[1]</sup>\n对于一般地约束优化问题\n$$\\begin{array}{ll}\n{\\min } & {f(\\boldsymbol x)} \\\\ \n{\\text {s.t.}} & {g_{i}(\\boldsymbol x) \\leq 0 \\quad(i=1, \\ldots, m)} \\\\ \n{} & {h_{j}(\\boldsymbol x)=0 \\quad(j=1, \\ldots, n)}\n\\end{array}$$\n其中，自变量$\\boldsymbol x\\in \\mathbb{R}^n$。设$f(\\boldsymbol x),g_i(\\boldsymbol x),h_j(\\boldsymbol x)$具有连续的一阶偏导数，$\\boldsymbol x^*$是优化问题的局部可行解。若该优化问题满足任意一个约束限制条件（constraint qualifications or regularity conditions）<sup>[2]</sup>，则一定存在$\\boldsymbol \\mu^*=(\\mu_1^*,\\mu_2^*,...,\\mu_m^*)^T,\\boldsymbol \\lambda^*=(\\lambda_1^*,\\lambda_2^*,...,\\lambda_n^*)^T,$使得\n$$\\left\\{\n\\begin{aligned}\n& \\nabla_{\\boldsymbol x} L(\\boldsymbol x^* ,\\boldsymbol \\mu^* ,\\boldsymbol \\lambda^* )=\\nabla f(\\boldsymbol  x^* )+\\sum_{i=1}^{m}\\mu_i^* \\nabla g_i(\\boldsymbol x^* )+\\sum_{j=1}^{n}\\lambda_j^* \\nabla h_j(\\boldsymbol x^*)=0 &(1) \\\\\n& h_j(\\boldsymbol x^*)=0 &(2) \\\\\n& g_i(\\boldsymbol x^*) \\leq 0 &(3) \\\\\n& \\mu_i^* \\geq 0 &(4)\\\\\n& \\mu_i^* g_i(\\boldsymbol x^*)=0 &(5)\n\\end{aligned}\n\\right.\n$$\n其中$L(\\boldsymbol x,\\boldsymbol \\mu,\\boldsymbol \\lambda)$为拉格朗日函数\n$$L(\\boldsymbol x,\\boldsymbol \\mu,\\boldsymbol \\lambda)=f(\\boldsymbol x)+\\sum_{i=1}^{m}\\mu_i g_i(\\boldsymbol x)+\\sum_{j=1}^{n}\\lambda_j h_j(\\boldsymbol x)$$\n以上5条即为KKT条件，严格数学证明参见参考文献[1]的§ 4.2.1。\n\n## 参考文献\n[1] 王燕军. 《最优化基础理论与方法》 <br>\n[2] https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_(or_constraint_qualifications) <br>\n[3] 王书宁 译.《凸优化》','2021-12-10 12:43:49','2021-12-19 13:22:27'),
	(7,1,'chapter7','## 7.5\n$$R(c|\\boldsymbol x)=1−P(c|\\boldsymbol x)$$\n[推导]：由公式(7.1)和公式(7.4)可得：\n$$R(c_i|\\boldsymbol x)=1*P(c_1|\\boldsymbol x)+...+1*P(c_{i-1}|\\boldsymbol x)+0*P(c_i|\\boldsymbol x)+1*P(c_{i+1}|\\boldsymbol x)+...+1*P(c_N|\\boldsymbol x)$$\n又$\\sum_{j=1}^{N}P(c_j|\\boldsymbol x)=1$，则：\n$$R(c_i|\\boldsymbol x)=1-P(c_i|\\boldsymbol x)$$\n此即为公式(7.5）\n\n## 7.6\n$$h^{*}(\\boldsymbol{x})=\\underset{c \\in \\mathcal{Y}}{\\arg \\max } P(c | \\boldsymbol{x})$$\n[推导]：将公式(7.5)代入公式(7.3)即可推得此式。\n\n## 7.12\n$$\\hat{\\boldsymbol{\\mu}}_{c}=\\frac{1}{\\left|D_{c}\\right|} \\sum_{\\boldsymbol{x} \\in D_{c}} \\boldsymbol{x}$$\n[推导]：参见公式(7.13)\n\n## 7.13\n$$\\hat{\\boldsymbol{\\sigma}}_{c}^{2}=\\frac{1}{\\left|D_{c}\\right|} \\sum_{\\boldsymbol{x} \\in D_{c}}\\left(\\boldsymbol{x}-\\hat{\\boldsymbol{\\mu}}_{c}\\right)\\left(\\boldsymbol{x}-\\hat{\\boldsymbol{\\mu}}_{c}\\right)^{\\mathrm{T}}$$\n[推导]：根据公式(7.11)和公式(7.10)可知参数求解公式为\n$$\\begin{aligned}\n\\hat{\\boldsymbol{\\theta}}_{c}&=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\max } LL\\left(\\boldsymbol{\\theta}_{c}\\right) \\\\\n&=\\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\min } -LL\\left(\\boldsymbol{\\theta}_{c}\\right) \\\\\n&= \\underset{\\boldsymbol{\\theta}_{c}}{\\arg \\min }-\\sum_{\\boldsymbol{x} \\in D_{c}} \\log P\\left(\\boldsymbol{x} | \\boldsymbol{\\theta}_{c}\\right)\n\\end{aligned}$$\n由西瓜书上下文可知，此时假设概率密度函数$p(\\boldsymbol{x} | c) \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}_{c}, \\boldsymbol{\\sigma}_{c}^{2}\\right)$，其等价于假设\n$$P\\left(\\boldsymbol{x} | \\boldsymbol{\\theta}_{c}\\right)=P\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{c}, \\boldsymbol{\\sigma}_{c}^{2}\\right)=\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\boldsymbol{\\Sigma}_c|}} \\exp \\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)\\right)$$\n其中，$d$表示$\\boldsymbol{x}$的维数，$\\boldsymbol{\\Sigma}_c=\\boldsymbol{\\sigma}_{c}^{2}$为对称正定协方差矩阵，$|\\boldsymbol{\\Sigma}_c|$表示$\\boldsymbol{\\Sigma}_c$的行列式。将其代入参数求解公式可得\n$$\\begin{aligned}\n(\\hat{\\boldsymbol{\\mu}}_{c}, \\hat{\\boldsymbol{\\Sigma}}_{c})&= \\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }-\\sum_{\\boldsymbol{x} \\in D_{c}} \\log\\left[\\frac{1}{\\sqrt{(2 \\pi)^{d}|\\boldsymbol{\\Sigma}_c|}} \\exp \\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)\\right)\\right] \\\\\n&= \\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }-\\sum_{\\boldsymbol{x} \\in D_{c}} \\left[-\\frac{d}{2}\\log(2 \\pi)-\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_c|-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)\\right] \\\\\n&= \\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }\\sum_{\\boldsymbol{x} \\in D_{c}} \\left[\\frac{d}{2}\\log(2 \\pi)+\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)\\right] \\\\\n&= \\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }\\sum_{\\boldsymbol{x} \\in D_{c}} \\left[\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu}_c)\\right] \\\\\n\\end{aligned}$$\n假设此时数据集$D_c$中的样本个数为$n$，也即$|D_c|=n$，则上式可以改写为\n$$\\begin{aligned}\n(\\hat{\\boldsymbol{\\mu}}_{c}, \\hat{\\boldsymbol{\\Sigma}}_{c})&=\\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }\\sum_{i=1}^{n} \\left[\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\frac{1}{2}(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}_c)\\right]\\\\\n&=\\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }\\frac{n}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\sum_{i=1}^{n}\\frac{1}{2}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)\\\\\n\\end{aligned}$$\n为了便于分别求解$\\hat{\\boldsymbol{\\mu}}_{c}$和$\\hat{\\boldsymbol{\\Sigma}}_{c}$，在这里我们根据公式$\\boldsymbol{x}^{\\mathrm{T}}\\mathbf{A}\\boldsymbol{x}=\\operatorname{tr}(\\mathbf{A}\\boldsymbol{x}\\boldsymbol{x}^{\\mathrm{T}}),\\bar{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_i$将上式中的最后一项作如下恒等变形\n$$\\begin{aligned}\n&\\sum_{i=1}^{n}\\frac{1}{2}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)(\\boldsymbol{x}_i-\\boldsymbol{\\mu}_c)^{\\mathrm{T}}\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}\\left(\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\mathrm{T}}-\\boldsymbol{x}_i\\boldsymbol{\\mu}_c^{\\mathrm{T}}-\\boldsymbol{\\mu}_c\\boldsymbol{x}_i^{\\mathrm{T}}+\\boldsymbol{\\mu}_c\\boldsymbol{\\mu}_c^{\\mathrm{T}}\\right)\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\left(\\sum_{i=1}^{n}\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\mathrm{T}}-n\\bar{\\boldsymbol{x}}\\boldsymbol{\\mu}_c^{\\mathrm{T}}-n\\boldsymbol{\\mu}_c\\bar{\\boldsymbol{x}}^{\\mathrm{T}}+n\\boldsymbol{\\mu}_c\\boldsymbol{\\mu}_c^{\\mathrm{T}}\\right)\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\left(\\sum_{i=1}^{n}\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\mathrm{T}}-2n\\bar{\\boldsymbol{x}}\\boldsymbol{\\mu}_c^{\\mathrm{T}}+n\\boldsymbol{\\mu}_c\\boldsymbol{\\mu}_c^{\\mathrm{T}}+2n\\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^{\\mathrm{T}}-2n\\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^{\\mathrm{T}}\\right)\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\left(\\left(\\sum_{i=1}^{n}\\boldsymbol{x}_i\\boldsymbol{x}_i^{\\mathrm{T}}-2n\\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^{\\mathrm{T}}+n\\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^{\\mathrm{T}}\\right)+\\left(n\\boldsymbol{\\mu}_c\\boldsymbol{\\mu}_c^{\\mathrm{T}}-2n\\bar{\\boldsymbol{x}}\\boldsymbol{\\mu}_c^{\\mathrm{T}}+n\\bar{\\boldsymbol{x}}\\bar{\\boldsymbol{x}}^{\\mathrm{T}}\\right)\\right)\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\left(\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}+\\sum_{i=1}^{n}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right)\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]+\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]+\\frac{1}{2}\\operatorname{tr}\\left[n\\cdot\\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]+\\frac{n}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]\\\\\n=&\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_c^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]+\\frac{n}{2}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})\n\\end{aligned}$$\n所以\n$$(\\hat{\\boldsymbol{\\mu}}_{c}, \\hat{\\boldsymbol{\\Sigma}}_{c})=\\underset{(\\boldsymbol{\\mu}_{c},\\boldsymbol{\\Sigma}_c)}{\\arg \\min }\\frac{n}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_{c}^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]+\\frac{n}{2}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})^{\\mathrm{T}} \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}})$$\n观察上式可知，由于此时$\\boldsymbol{\\Sigma}_c^{-1}$和$\\boldsymbol{\\Sigma}_c$一样均为正定矩阵，所以当$\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}}\\neq\\boldsymbol{0}$时，上式最后一项为正定二次型。根据正定二次型的性质可知，上式最后一项取值的大小此时仅与$\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}}$相关，而且当且仅当$\\boldsymbol{\\mu}_c-\\bar{\\boldsymbol{x}}=\\boldsymbol{0}$时，上式最后一项取到最小值0，此时可以解得\n$$\\hat{\\boldsymbol{\\mu}}_{c}=\\bar{\\boldsymbol{x}}=\\frac{1}{n}\\sum_{i=1}^{n}\\boldsymbol{x}_i$$\n将求解出来的$\\hat{\\boldsymbol{\\mu}}_{c}$代回参数求解公式可得新的参数求解公式为\n$$\\hat{\\boldsymbol{\\Sigma}}_{c}=\\underset{\\boldsymbol{\\Sigma}_c}{\\arg \\min }\\frac{n}{2}\\log|\\boldsymbol{\\Sigma}_c|+\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}_{c}^{-1}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}\\right]$$\n此时的参数求解公式是仅与$\\boldsymbol{\\Sigma}_c$相关的函数。为了求解$\\hat{\\boldsymbol{\\Sigma}}_{c}$，在这里我们不加证明地给出一个引理（具体证明参见参考文献[8]）：设$\\mathbf{B}$为$p$阶正定矩阵，$n>0$为实数，在对所有$p$阶正定矩阵$\\boldsymbol{\\Sigma}$有\n$$\\frac{n}{2}\\log|\\boldsymbol{\\Sigma}|+\\frac{1}{2}\\operatorname{tr}\\left[\\boldsymbol{\\Sigma}^{-1}\\mathbf{B}\\right]\\geq\\frac{n}{2}\\log|\\mathbf{B}|+\\frac{pn}{2}(1-\\log n)$$\n当且仅当$\\boldsymbol{\\Sigma}=\\frac{1}{n}\\mathbf{B}$时等号成立。所以根据此引理可知，当且仅当$\\boldsymbol{\\Sigma}_c=\\frac{1}{n}\\sum_{i=1}^{n}(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})(\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^{\\mathrm{T}}$\n时，上述参数求解公式中$\\arg \\min$后面的式子取到最小值，那么此时的$\\boldsymbol{\\Sigma}_c$即为我们想要求解的$\\hat{\\boldsymbol{\\Sigma}}_{c}$。\n\n## 7.19\n$$\\hat{P}(c)=\\frac{\\left|D_{c}\\right|+1}{|D|+N}$$\n[推导]：从贝叶斯估计（参见附录①）的角度来说，拉普拉斯修正就等价于先验概率为Dirichlet分布（参见附录③）的后验期望值估计。为了接下来的叙述方便，我们重新定义一下相关数学符号。设包含$m$个独立同分布样本的训练集为$D$，$D$中可能的类别数为$k$，其类别的具体取值范围为$\\{c_1,c_2,...,c_k\\}$。若令随机变量$C$表示样本所属的类别，且$C$取到每个值的概率分别为$P(C=c_1)=\\theta_1,P(C=c_2)=\\theta_2,...,P(C=c_k)=\\theta_k$，那么显然$C$服从参数为$\\boldsymbol{\\theta}=(\\theta_1,\\theta_2,...,\\theta_k)\\in\\mathbb{R}^{k}$的Categorical分布（参见附录②），其概率质量函数为\n$$P(C=c_i)=P(c_i)=\\theta_i$$\n其中$P(c_i)=\\theta_i$就是公式(7.9)所要求解的$\\hat{P}(c)$，下面我们用贝叶斯估计中的后验期望值估计来估计$\\theta_i$。根据贝叶斯估计的原理可知，在进行参数估计之前，需要先主观预设一个先验概率$P(\\boldsymbol{\\theta})$，通常为了方便计算<sup>[7]</sup>后验概率$P(\\boldsymbol{\\theta}|D)$，我们会用似然函数$P(D|\\boldsymbol{\\theta})$的共轭先验<sup>[6]</sup>作为我们的先验概率。显然，此时的似然函数$P(D|\\boldsymbol{\\theta})$是一个基于Categorical分布的似然函数，而Categorical分布的共轭先验为Dirichlet分布，所以此时只需要预设先验概率$P(\\boldsymbol{\\theta})$为Dirichlet分布，然后使用后验期望值估计就能估计出$\\theta_i$。具体地，记$D$中样本类别取值为$c_i$的样本个数为$y_i$，则似然函数$P(D|\\boldsymbol{\\theta})$可展开为\n$$P(D|\\boldsymbol{\\theta})=\\theta_1^{y_1}\\ldots\\theta_k^{y_k}=\\prod_{i=1}^{k}\\theta_i^{y_i}$$\n那么后验概率$P(D|\\boldsymbol{\\theta})$为\n$$\\begin{aligned}\nP(\\boldsymbol{\\theta}|D)&=\\frac{P(D|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(D)}\\\\\n&=\\frac{P(D|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{\\sum_{\\boldsymbol{\\theta}}P(D|\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}\\\\\n&=\\frac{\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot P(\\boldsymbol{\\theta})}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot P(\\boldsymbol{\\theta})\\right]}\n\\end{aligned}$$\n假设此时先验概率$P(\\boldsymbol{\\theta})$是参数为$\\boldsymbol{\\alpha}=(\\alpha_1,\\alpha_2,...,\\alpha_k)\\in \\mathbb{R}^{k}$的Dirichlet分布，则$P(\\boldsymbol{\\theta})$可写为\n$$P(\\boldsymbol{\\boldsymbol{\\theta}};\\boldsymbol{\\alpha})=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}$$\n将其代入$P(D|\\boldsymbol{\\theta})$可得\n$$\\begin{aligned}\nP(\\boldsymbol{\\theta}|D)&=\\frac{\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot P(\\boldsymbol{\\theta})}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot P(\\boldsymbol{\\theta})\\right]} \\\\\n&=\\frac{\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}\\right]} \\\\\n&=\\frac{\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}\\right]\\cdot \\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}} \\\\\n&=\\frac{\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{y_i}\\cdot \\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}-1}\\right]} \\\\\n&=\\frac{\\prod_{i=1}^{k}\\theta_i^{\\alpha_{i}+y_i-1}}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{\\alpha_{i}+y_i-1}\\right]}\n\\end{aligned}$$\n此时若设$\\boldsymbol{\\alpha}+\\boldsymbol{y}=(\\alpha_1+y_1,\\alpha_2+y_2,...,\\alpha_k+y_k)\\in \\mathbb{R}^{k}$，则根据Dirichlet分布的定义可知\n$$\\begin{aligned}\nP(\\boldsymbol{\\theta};\\boldsymbol{\\alpha}+\\boldsymbol{y})&=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha_{i}+y_i-1} \\\\\n\\sum_{\\boldsymbol{\\theta}}P(\\boldsymbol{\\theta};\\boldsymbol{\\alpha}+\\boldsymbol{y})&=\\sum_{\\boldsymbol{\\theta}}\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha_{i}+y_i-1}\\\\\n1&=\\sum_{\\boldsymbol{\\theta}}\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha_{i}+y_i-1} \\\\\n1&=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)}\\sum_{\\boldsymbol{\\theta}}\\left[\\prod _{i=1}^{k}\\theta_{i}^{\\alpha_{i}+y_i-1}\\right] \\\\\n\\frac{1}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod _{i=1}^{k}\\theta_{i}^{\\alpha_{i}+y_i-1}\\right]}&=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)} \\\\\n\\end{aligned}$$\n将此结论代入$P(D|\\boldsymbol{\\theta})$可得\n$$\\begin{aligned}\nP(\\boldsymbol{\\theta}|D)&=\\frac{\\prod_{i=1}^{k}\\theta_i^{\\alpha_{i}+y_i-1}}{\\sum_{\\boldsymbol{\\theta}}\\left[\\prod_{i=1}^{k}\\theta_i^{\\alpha_{i}+y_i-1}\\right]} \\\\\n&=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}(\\alpha_{i}+y_i)\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha_{i}+y_i)}\\prod _{i=1}^{k}\\theta_{i}^{\\alpha _{i}+y_i-1} \\\\\n&=P(\\boldsymbol{\\theta};\\boldsymbol{\\alpha}+\\boldsymbol{y})\n\\end{aligned}$$\n综上可知，对于服从Categorical分布的$\\boldsymbol{\\theta}$来说，假设其先验概率$P(\\boldsymbol{\\theta})$是参数为$\\boldsymbol{\\alpha}$的Dirichlet分布时，得到的后验概率$P(\\boldsymbol{\\theta}|D)$是参数为$\\boldsymbol{\\alpha}+\\boldsymbol{y}$的Dirichlet分布，通常我们称这种先验概率分布和后验概率分布形式相同的这对分布为共轭分布<sup>[6]</sup>。在推得后验概率$P(\\boldsymbol{\\theta}|D)$的具体形式以后，根据后验期望值估计可得$\\theta_i$的估计值为\n$$\\begin{aligned}\n\\theta_i&=E_{P(\\boldsymbol{\\theta}|D)}[\\theta_i]\\\\\n&=E_{P(\\boldsymbol{\\theta};\\boldsymbol{\\alpha}+\\boldsymbol{y})}[\\theta_i]\\\\\n&=\\frac{\\alpha_i+y_i}{\\sum_{j=1}^k(\\alpha_j+y_j)}\\\\\n&=\\frac{\\alpha_i+y_i}{\\sum_{j=1}^k\\alpha_j+\\sum_{j=1}^ky_j}\\\\\n&=\\frac{\\alpha_i+y_i}{\\sum_{j=1}^k\\alpha_j+m}\\\\\n\\end{aligned}$$\n显然，公式(7.9)是当$\\boldsymbol{\\alpha}=(1,1,...,1)$时推得的具体结果，此时等价于我们主观预设的先验概率$P(\\boldsymbol{\\theta})$服从均匀分布，此即为拉普拉斯修正。同理，当我们调整$\\boldsymbol{\\alpha}$的取值后，即可推得其他数据平滑的公式。\n\n## 7.20\n$$\\hat{P}\\left(x_{i} | c\\right)=\\frac{\\left|D_{c, x_{i}}\\right|+1}{\\left|D_{c}\\right|+N_{i}}$$\n[推导]：参见公式(7.19)\n\n## 7.24\n$$\\hat{P}\\left(c, x_{i}\\right)=\\frac{\\left|D_{c, x_{i}}\\right|+1}{|D|+N_{i}}$$\n[推导]：参见公式(7.19)\n\n## 7.25\n$$\\hat{P}\\left(x_{j} | c, x_{i}\\right)=\\frac{\\left|D_{c, x_{i}, x_{j}}\\right|+1}{\\left|D_{c, x_{i}}\\right|+N_{j}}$$\n[推导]：参见公式(7.20)\n\n## 7.27\n$$\\begin{aligned} \nP\\left(x_{1}, x_{2}\\right) &=\\sum_{x_{4}} P\\left(x_{1}, x_{2}, x_{4}\\right) \\\\ \n&=\\sum_{x_{4}} P\\left(x_{4} | x_{1}, x_{2}\\right) P\\left(x_{1}\\right) P\\left(x_{2}\\right) \\\\ \n&=P\\left(x_{1}\\right) P\\left(x_{2}\\right) \n\\end{aligned}$$\n[解析]：在这里补充一下同父结构和顺序结构的推导。同父结构：在给定父节点$x_1$的条件下$x_3,x_4$独立\n$$\\begin{aligned} \nP(x_3,x_4|x_1)&=\\frac{P(x_1,x_3,x_4)}{P(x_1)} \\\\\n&=\\frac{P(x_1)P(x_3|x_1)P(x_4|x_1)}{P(x_1)} \\\\\n&=P(x_3|x_1)P(x_4|x_1) \\\\\n\\end{aligned}$$\n顺序结构：在给定节点$x$的条件下$y,z$独立\n$$\\begin{aligned} \nP(y,z|x)&=\\frac{P(x,y,z)}{P(x)} \\\\\n&=\\frac{P(z)P(x|z)P(y|x)}{P(x)} \\\\\n&=\\frac{P(z,x)P(y|x)}{P(x)} \\\\\n&=P(z|x)P(y|x) \\\\\n\\end{aligned}$$\n\n## 7.34\n$$LL(\\mathbf{\\Theta}|\\mathbf{X},\\mathbf{Z})=\\ln P(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\Theta})$$\n[解析]：EM算法这一节建议以李航老师的《统计学习方法》为主，西瓜书为辅进行学习。\n\n## 附录\n### ①贝叶斯估计<sup>[1]</sup>\n贝叶斯学派视角下的一类点估计法称为贝叶斯估计，常用的贝叶斯估计有最大后验估计（Maximum A Posteriori Estimation，简称MAP）、后验中位数估计和后验期望值估计这3种参数估计方法，下面给出这3种方法的具体定义。设总体的概率质量函数（若总体的分布为连续型时则改为概率密度函数，此处以离散型为例）为$P(x|\\theta)$，从该总体中抽取出的$n$个独立同分布的样本构成的样本集为$D=\\{x_1,x_2,...,x_n\\}$，则根据贝叶斯公式可得在给定样本集$D$的条件下，$\\theta$的条件概率为\n$$P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{P(D)}=\\frac{P(D|\\theta)P(\\theta)}{\\sum_{\\theta}P(D|\\theta)P(\\theta)}$$\n其中$P(D|\\theta)$为似然函数，由于样本集$D$中的样本是独立同分布的，所以似然函数可以进一步展开\n$$P(\\theta|D)=\\frac{P(D|\\theta)P(\\theta)}{\\sum_{\\theta}P(D|\\theta)P(\\theta)}=\\frac{\\prod_{i=1}^{n}P(x_i|\\theta) P(\\theta)}{\\sum_{\\theta}\\prod_{i=1}^{n}P(x_i|\\theta)P(\\theta)}$$\n根据贝叶斯学派的观点，此条件概率代表了我们在已知样本集$D$后对$\\theta$产生的新的认识，它综合了我们对$\\theta$主观预设的先验概率$P(\\theta)$和样本集$D$带来的信息，通常称其为$\\theta$的后验概率。贝叶斯学派认为，在得到$P(\\theta|D)$以后，对参数$\\theta$的任何统计推断，都只能基于$P(\\theta|D)$。至于具体如何去使用它，可以结合某种准则一起去进行，统计学家也有一定的自由度。对于点估计来说，求使得$P(\\theta|D)$达到最大值的$\\hat{\\theta}_{MAP}$作为$\\theta$的估计称为最大后验估计；求$P(\\theta|D)$的中位数$\\hat{\\theta}_{Median}$作为$\\theta$的估计称为后验中位数估计；求$P(\\theta|D)$的期望值（均值）$\\hat{\\theta}_{Mean}$作为$\\theta$的估计称为后验期望值估计。\n\n### ②Categorical分布<sup>[2]</sup>\nCategorical分布又称为广义伯努利分布，是将伯努利分布中的随机变量可取值个数由两个泛化为多个得到的分布。具体地，设离散型随机变量$X$共有$k$种可能的取值$\\{x_1,x_2,...,x_k\\}$，且$X$取到每个值的概率分别为$P(X=x_1)=\\theta_1,P(X=x_2)=\\theta_2,...,P(X=x_k)=\\theta_k$，则称随机变量$X$服从参数为$\\theta_1,\\theta_2,...,\\theta_k$的Categorical分布，其概率质量函数为\n$$P(X=x_i)=P(x_i)=\\theta_i$$\n\n### ③Dirichlet分布<sup>[3]</sup>\n类似于Categorical分布是伯努利分布的泛化形式，Dirichlet分布是Beta分布<sup>[4]</sup>的泛化形式。对于一个$k$维随机变量$\\boldsymbol{x}=(x_1,x_2,...,x_k)\\in \\mathbb{R}^{k}$，其中$x_i(i=1,2,...,k)$满足$0\\leqslant x_i \\leqslant 1,\\sum_{i=1}^{k}x_i=1$，若$\\boldsymbol{x}$服从参数为$\\boldsymbol{\\alpha}=(\\alpha_1,\\alpha_2,...,\\alpha_k)\\in \\mathbb{R}^{k}$的Dirichlet分布，则其概率密度函数为\n$$p(\\boldsymbol{x};\\boldsymbol{\\alpha})=\\frac{\\Gamma \\left(\\sum _{i=1}^{k}\\alpha _{i}\\right)}{\\prod _{i=1}^{k}\\Gamma (\\alpha _{i})}\\prod _{i=1}^{k}x_{i}^{\\alpha _{i}-1}$$\n其中$\\Gamma (z)=\\int _{0}^{\\infty }x^{z-1}e^{-x}dx$为Gamma函数<sup>[5]</sup>，当$\\boldsymbol{\\alpha}=(1,1,...,1)$时，Dirichlet分布等价于均匀分布。\n\n## 参考文献\n[1]陈希孺编著.概率论与数理统计[M].中国科学技术大学出版社,2009. <br>\n[2]https://en.wikipedia.org/wiki/Categorical_distribution <br>\n[3]https://en.wikipedia.org/wiki/Dirichlet_distribution <br>\n[4]https://en.wikipedia.org/wiki/Beta_distribution <br>\n[5]https://en.wikipedia.org/wiki/Gamma_function <br>\n[6]https://en.wikipedia.org/wiki/Conjugate_prior <br>\n[7]https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83 <br>\n[8]http://staff.ustc.edu.cn/~zwp/teach/MVA/Lec5_slides.pdf <br>','2021-12-10 12:43:49','2021-12-19 13:22:33'),
	(8,1,'chapter8','## 8.1\n\n$$\nP\\left(h_{i}(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})\\right)=\\epsilon\n$$\n\n[解析]：$h_{i}(\\boldsymbol{x})$是编号为$i$的基分类器给$x$的预测标记，$f(\\boldsymbol{x})$是$x$的真实标记，它们之间不一致的概率记为$\\epsilon$。\n\n## 8.2\n\n$$\nH(\\boldsymbol{x})=\\operatorname{sign}\\left(\\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\\right)\n$$\n\n[解析]：$h_i(\\boldsymbol{x})$当把$\\boldsymbol{x}$分成1时，$h_i(\\boldsymbol{x})=1$，否则$h_i(\\boldsymbol{x})=-1$。各个基分类器$h_i$的分类结果求和之后数字的正、负或0，代表投票法产生的结果，即“少数服从多数”，符号函数$\\operatorname{sign}$，将正数变成1，负数变成-1，0仍然是0，所以$H(\\boldsymbol{x})$是由投票法产生的分类结果。\n\n## 8.3\n\n$$\n\\begin{aligned} P(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor} \\left( \\begin{array}{c}{T} \\\\ {k}\\end{array}\\right)(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\ & \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right) \\end{aligned}\n$$\n\n[推导]：由基分类器相互独立，假设随机变量$X$为$T$个基分类器分类正确的次数，因此随机变量$\\mathrm{X}$服从二项分布：$\\mathrm{X} \\sim \\mathcal{B}(\\mathrm{T}, 1-\\mathrm{\\epsilon})$，设$x_i$为每一个分类器分类正确的次数，则$x_i\\sim \\mathcal{B}(1, 1-\\mathrm{\\epsilon})（i=1，2，3，...，\\mathrm{T}）$，那么有\n$$\n\\begin{aligned}\n\\mathrm{X}&=\\sum_{i=1}^{\\mathrm{T}} x_i\\\\\n\\mathbb{E}(X)&=\\sum_{i=1}^{\\mathrm{T}}\\mathbb{E}(x_i)=(1-\\epsilon)T\n\\end{aligned}\n$$\n则：\n$$\n\\begin{aligned} P(H(x) \\neq f(x))=& P(X \\leq\\lfloor T / 2\\rfloor) \\\\ & \\leqslant P(X \\leq T / 2)\n\\\\ & =P\\left[X-(1-\\epsilon) T \\leqslant \\frac{T}{2}-(1-\\epsilon) T\\right] \n\\\\ & =P\\left[X-\n(1-\\epsilon) T \\leqslant -\\frac{T}{2}\\left(1-2\\epsilon\\right)]\\right]\n\\\\ &=P\\left[\\sum_{i=1}^{\\mathrm{T}} x_i-\n\\sum_{i=1}^{\\mathrm{T}}\\mathbb{E}(x_i) \\leqslant -\\frac{T}{2}\\left(1-2\\epsilon\\right)]\\right]\n\\\\ &=P\\left[\\frac{1}{\\mathrm{T}}\\sum_{i=1}^{\\mathrm{T}} x_i-\\frac{1}{\\mathrm{T}}\n\\sum_{i=1}^{\\mathrm{T}}\\mathbb{E}(x_i) \\leqslant -\\frac{1}{2}\\left(1-2\\epsilon\\right)]\\right]\n \\end{aligned}\n$$\n根据Hoeffding不等式知\n$$\nP\\left(\\frac{1}{m} \\sum_{i=1}^{m} x_{i}-\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right) \\leqslant -\\delta\\right) \\leqslant  \\exp \\left(-2 m \\delta^{2}\\right)\n$$\n令$\\delta=\\frac {(1-2\\epsilon)}{2},m=T$得\n$$\n\\begin{aligned} P(H(\\boldsymbol{x}) \\neq f(\\boldsymbol{x})) &=\\sum_{k=0}^{\\lfloor T / 2\\rfloor} \\left( \\begin{array}{c}{T} \\\\ {k}\\end{array}\\right)(1-\\epsilon)^{k} \\epsilon^{T-k} \\\\ & \\leqslant \\exp \\left(-\\frac{1}{2} T(1-2 \\epsilon)^{2}\\right) \\end{aligned}\n$$\n\n## 8.4\n\n$$\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})\n$$\n\n\n\n[解析]：这个式子是集成学习的加性模型，加性模型不采用梯度下降的思想，而是$H(\\boldsymbol{x})=\\sum_{t=1}^{T-1} \\alpha_{t} h_{t}(\\boldsymbol{x})+\\alpha_{T}h_{T}(\\boldsymbol{x})$每次更新求解一个理论上最优的$h_T$（见式8.18）和$\\alpha_T$（见式8.11）\n\n## 8.5\n\n$$\n\\ell_{\\mathrm{exp}}(H | \\mathcal{D})=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]\n$$\n\n[解析]：由式(8.4)知\n$$\nH(\\boldsymbol{x})=\\sum_{t=1}^{T} \\alpha_{t} h_{t}(\\boldsymbol{x})\n$$\n又由式(8.11)可知\n$$\n\\alpha_{t}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}\\right)\n$$\n由$\\ln$函数的单调性可知，该分类器的权重只与分类器的错误率负相关(即错误率越大，权重越低)，下面解释指数损失函数的意义：\n\n1. 先考虑指数损失函数$e^{-f(x) H(x)}$的含义：$f$为真实函数，对于样本$x$来说，$f(\\boldsymbol{x}) \\in\\{+1,-1\\}$只能取$+1$和$-1$，而$H(\\boldsymbol{x})$是一个实数；\n   当$H(\\boldsymbol{x})$的符号与$f(x)$一致时，$f(\\boldsymbol{x}) H(\\boldsymbol{x})>0$，因此$e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}=e^{-|H(\\boldsymbol{x})|}<1$，且$|H(\\boldsymbol{x})|$越大指数损失函数$e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}$越小（这很合理：此时$|H(\\boldsymbol{x})|$越大意味着分类器本身对预测结果的信心越大，损失应该越小；若$|H(\\boldsymbol{x})|$在零附近，虽然预测正确，但表示分类器本身对预测结果信心很小，损失应该较大）；\n   当$H(\\boldsymbol{x})$的符号与$f(\\boldsymbol{x})$不一致时，$f(\\boldsymbol{x}) H(\\boldsymbol{x})<0$，因此$e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}=e^{|H(\\boldsymbol{x})|}>1$，且$| H(\\boldsymbol{x}) |$越大指数损失函数越大（这很合理：此时$| H(\\boldsymbol{x}) |$越大意味着分类器本身对预测结果的信心越大，但预测结果是错的，因此损失应该越大；若$| H(\\boldsymbol{x}) |$在零附近，虽然预测错误，但表示分类器本身对预测结果信心很小，虽然错了，损失应该较小）；\n   \n2. 符号$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}[\\cdot]$的含义：$\\mathcal{D}$为概率分布，可简单理解为在数据集$D$中进行一次随机抽样，每个样本被取到的概率；$\\mathbb{E}[\\cdot]$为经典的期望，则综合起来$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}[\\cdot]$表示在概率分布$\\mathcal{D}$上的期望，可简单理解为对数据集$D$以概率$\\mathcal{D}$进行加权后的期望。即\n$$\n  \\begin{aligned}\n  \\ell_{\\exp }(H | \\mathcal{D}) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right] \\\\\n  &=\\sum_{\\boldsymbol{x} \\in D} \\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\n  \\end{aligned}\n$$\n\n\n## 8.6\n\n$$\n\\frac{\\partial \\ell_{\\exp }(H | \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 | \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 | \\boldsymbol{x})\n$$\n\n[解析]：由公式(8.5)中对于符号$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}[\\cdot]$的解释可知\n$$\n\\begin{aligned}\n\\ell_{\\exp }(H | \\mathcal{D}) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right] \\\\\n&=\\sum_{\\boldsymbol{x} \\in D} \\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})} \\\\\n&=\\sum_{i=1}^{|D|} \\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right)\\left(e^{-H\\left(\\boldsymbol{x}_{i}\\right)} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1\\right)+e^{H\\left(\\boldsymbol{x}_{i}\\right)} \\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=-1\\right)\\right)\\\\\n&=\\sum_{i=1}^{|D|} \\left(e^{-H\\left(\\boldsymbol{x}_{i}\\right)} \\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right)\\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1\\right)+e^{H\\left(\\boldsymbol{x}_{i}\\right)} \\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right)\\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=-1\\right)\\right)\\\\\n&=\\sum_{i=1}^{|D|} \\left(e^{-H\\left(\\boldsymbol{x}_{i}\\right)} P\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1 \\mid \\boldsymbol{x}_{i}\\right)+e^{H\\left(\\boldsymbol{x}_{i}\\right)} P\\left(f\\left(\\boldsymbol{x}_{i}\\right)=-1 \\mid \\boldsymbol{x}_{i}\\right)\\right)\n\\end{aligned}\n$$\n\n其中$\\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right)\\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1\\right)=P\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1 \\mid \\boldsymbol{x}_{i}\\right)$可以这样理解：\n\n$\\mathcal{D}(x_i)$表示在数据集$D$中进行一次随机抽样，样本$x_i$被取到的概率，$\\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right)\\mathbb{I}\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1\\right)$表示在数据集$D$中进行一次随机抽样，使得$f(x_i)=1$的样本$x_i$被抽到的概率，即为$P\\left(f\\left(\\boldsymbol{x}_{i}\\right)=1 \\mid \\boldsymbol{x}_{i}\\right)$。\n\n当对$H(x_i)$求导时，求和号中只有含$x_i$项不为0，由求导公式\n$$\n\\frac{\\partial e^{-H(\\boldsymbol{x})}}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})}\\qquad \\frac{\\partial e^{H(\\boldsymbol{x})}}{\\partial H(\\boldsymbol{x})}=e^{H(\\boldsymbol{x})}\n$$\n有\n$$\n\\frac{\\partial \\ell_{\\exp }(H | \\mathcal{D})}{\\partial H(\\boldsymbol{x})}=-e^{-H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=1 | \\boldsymbol{x})+e^{H(\\boldsymbol{x})} P(f(\\boldsymbol{x})=-1 | \\boldsymbol{x})\n$$\n\n## 8.7\n\n$$\nH(\\boldsymbol{x})=\\frac{1}{2} \\ln \\frac{P(f(x)=1 | \\boldsymbol{x})}{P(f(x)=-1 | \\boldsymbol{x})}\n$$\n\n[解析]：令式(8.6)等于0，移项并分离$H(\\boldsymbol{x})$，即可得到式(8.7)。\n\n## 8.8\n\n$$\n\\begin{aligned}\n\\operatorname{sign}(H(\\boldsymbol{x}))&=\\operatorname{sign}\\left(\\frac{1}{2} \\ln \\frac{P(f(x)=1 | \\boldsymbol{x})}{P(f(x)=-1 | \\boldsymbol{x})}\\right)\n\\\\ & =\\left\\{\\begin{array}{ll}{1,} & {P(f(x)=1 | \\boldsymbol{x})>P(f(x)=-1 | \\boldsymbol{x})} \\\\ {-1,} & {P(f(x)=1 | \\boldsymbol{x})<P(f(x)=-1 | \\boldsymbol{x})}\\end{array}\\right.\n\\\\ & =\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y | \\boldsymbol{x})\n\\end{aligned}\n$$\n\n[解析]：第一行到第二行显然成立，第二行到第三行是利用了$\\arg\\max$函数的定义。$\\underset{y \\in\\{-1,1\\}}{\\arg \\max } P(f(x)=y | \\boldsymbol{x})$表示使得函数$P(f(x)=y | \\boldsymbol{x})$取得最大值的$y$的值，展开刚好是第二行的式子。\n\n## 8.9\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(\\alpha_{t} h_{t} | \\mathcal{D}_{t}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}\\right] \\\\\n&=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left[e^{-\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} \\mathbb{I}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right)\\right] \\\\\n&=e^{-\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x})=h_{t}(\\boldsymbol{x})\\right)+e^{\\alpha_{t}} P_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}\\left(f(\\boldsymbol{x}) \\neq h_{t}(\\boldsymbol{x})\\right) \\\\\n&=e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n\\end{aligned}\n$$\n\n[解析]：$\\epsilon_t$与式(8.1)一致，表示$h_t(\\boldsymbol{x})$分类错误的概率。\n\n## 8.10\n\n$$\n\\frac{\\partial \\ell_{\\exp }\\left(\\alpha_{t} h_{t} | \\mathcal{D}_{t}\\right)}{\\partial \\alpha_{t}}=-e^{-\\alpha_{t}}\\left(1-\\epsilon_{t}\\right)+e^{\\alpha_{t}} \\epsilon_{t}\n$$\n\n[解析]：指数损失函数对$\\alpha_t$求偏导，为了得到使得损失函数取最小值时$\\alpha_t$的值。\n\n## 8.11\n\n$$\n\\alpha_{t}=\\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}\\right)\n$$\n\n[解析]：令公式(8.10)等于0移项即得到的该式。此时$\\alpha_t$的取值使得该基分类器经$\\alpha_t$加权后的损失函数最小。\n\n## 8.12\n\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} | \\mathcal{D}\\right) &=\\mathbb{E}_{x \\sim \\mathcal{D}}\\left[e^{-f(x)\\left(H_{t-1}(x)+h_{t}(x)\\right)}\\right] \\\\\n&=\\mathbb{E}_{x \\sim \\mathcal{D}}\\left[e^{-f(x) H_{t-1}(x)} e^{-f(x) h_{t}(x)}\\right]\n\\end{aligned}\n$$\n\n[解析]：将$H_{t}(\\boldsymbol{x})=H_{t-1}(\\boldsymbol{x})+h_{t}(\\boldsymbol{x})$代入公式(8.5)即可，因为理想的$h_t$可以纠正$H_{t-1}$的全部错误，所以这里指定其权重系数为1。如果权重系数$\\alpha_t$是个常数的话，对后续结果也没有影响。\n\n## 8.13\n\n$$\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} | \\mathcal{D}\\right)=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n$$\n\n[推导]：由$e^x$的二阶泰勒展开为$1+x+\\frac{x^2}{2}+o(x^2)$得:\n$$\n\\begin{aligned}\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} | \\mathcal{D}\\right) &=\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})}\\right]\n\\\\ & \\simeq \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{f^{2}(\\boldsymbol{x}) h_{t}^{2}(\\boldsymbol{x})}{2}\\right)\\right]\n\\end{aligned}\n$$\n因为$f(\\boldsymbol{x})$与$h_t(\\boldsymbol{x})$取值都为1或-1，所以$f^2(\\boldsymbol{x})=h_t^2(\\boldsymbol{x})=1$，所以得:\n$$\n\\ell_{\\exp }\\left(H_{t-1}+h_{t} | \\mathcal{D}\\right)= \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h_{t}(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\n$$\n\n## 8.14\n\n$$\n\\begin{aligned}\nh_{t}(\\boldsymbol{x})&=\\underset{h}{\\arg \\min } \\ell_{\\exp }\\left(H_{t-1}+h | \\mathcal{D}\\right)\\\\\n&=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\left(1-f(\\boldsymbol{x}) h(\\boldsymbol{x})+\\frac{1}{2}\\right)\\right]\\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\\\\\n&=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right]\n\\end{aligned}\n$$\n\n[解析]：理想的$h_t(\\boldsymbol{x})$是使得$H_{t}(\\boldsymbol{x})$的指数损失函数取得最小值时的$h_t(\\boldsymbol{x})$，该式将此转化成某个期望的最大值。第二个式子到第三个式子是因为$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{3}{2} e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]$与$h(\\boldsymbol{x})$无关，是一个常数。第三个式子到最后一个式子是因为$\\frac{1}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{3}{2} e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}$与$h(\\boldsymbol{x})$无关因此可以引入进来。\n\n## 8.16\n\n$$\n\\begin{aligned} h_{t}(\\boldsymbol{x}) &=\\underset{h}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\ &=\\underset{\\boldsymbol{h}}{\\arg \\max } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})] \\end{aligned}\n$$\n\n[推导]：首先解释下符号$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}$的含义，注意在本章中有两个符号$D$和$\\mathcal{D}$，其中$D$表示数据集，而$\\mathcal{D}$表示数据集$D$的样本分布，可以理解为在数据集$D$上进行一次随机采样，样本$x$被抽到的概率是$\\mathcal{D}(x)$，那么符号$\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}$表示的是在概率分布$\\mathcal{D}$上的期望，可以简单地理解为对数据及$D$以概率$\\mathcal{D}$加权之后的期望，因此有：\n$$\n\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H(\\boldsymbol{x})}\\right]=\\sum_{i=1}^{|D|} \\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right) e^{-f\\left(\\boldsymbol{x}_{i}\\right) H\\left(\\boldsymbol{x}_{i}\\right)}\n$$\n由式(8.15)可知\n$$\n\\mathcal{D}_{t}\\left(\\boldsymbol{x}_{i}\\right)=\\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right) \\frac{e^{-f\\left(\\boldsymbol{x}_{i}\\right) H_{t-1}\\left(\\boldsymbol{x}_{i}\\right)}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}\n$$\n\n所以式(8.16)可以表示为\n$$\n\\begin{aligned} & \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[\\frac{e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]} f(\\boldsymbol{x}) h(\\boldsymbol{x})\\right] \\\\=& \\sum_{i=1}^{|D|} \\mathcal{D}\\left(\\boldsymbol{x}_{i}\\right) \\frac{e^{-f\\left(\\boldsymbol{x}_{i}\\right) H_{t-1}\\left(\\boldsymbol{x}_{i}\\right)}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x}) }]  \\right.}f(x_i)h(x_i) \\\\=& \\sum_{i=1}^{|D|} \\mathcal{D}_{t}\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{i}\\right) h\\left(\\boldsymbol{x}_{i}\\right) \\\\=& \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})] \\end{aligned}\n$$\n\n## 8.17\n\n$$\nf(\\boldsymbol{x}) h(\\boldsymbol{x})=1-2 \\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))\n$$\n\n[解析]：当$f(\\boldsymbol{x})=h(\\boldsymbol{x})$时，$\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))=0$，$f(\\boldsymbol{x}) h(\\boldsymbol{x})=1$，当$f(\\boldsymbol{x})\\neq h(\\boldsymbol{x})$时，$\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))=1$，$f(\\boldsymbol{x}) h(\\boldsymbol{x})=-1$。\n\n## 8.18\n\n$$\nh_{t}(\\boldsymbol{x})=\\underset{h}{\\arg \\min } \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\n$$\n\n[解析]：由公式(8.16) 和公式(8.17)有：\n$$\n\\begin{aligned}\nh_{t}(\\boldsymbol{x}) &=\\arg \\max _{h} \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[f(\\boldsymbol{x}) h(\\boldsymbol{x})] \\\\\n&=\\arg \\max _{h}\\left(1-2 \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\\right) \\\\\n&=\\underset{h}{\\arg \\max }\\left(-2 \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\\right) \\\\\n&=\\arg \\min \\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}_{t}}[\\mathbb{I}(f(\\boldsymbol{x}) \\neq h(\\boldsymbol{x}))]\n\\end{aligned}\n$$\n\n## 8.19\n\n$$\n\\begin{aligned}\n\\mathcal{D}_{t+1}(\\boldsymbol{x}) &=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\frac{\\mathcal{D}(\\boldsymbol{x}) e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})} e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})}}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]} \\\\\n&=\\mathcal{D}_{t}(\\boldsymbol{x}) \\cdot e^{-f(\\boldsymbol{x}) \\alpha_{t} h_{t}(\\boldsymbol{x})} \\frac{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t-1}(\\boldsymbol{x})}\\right]}{\\mathbb{E}_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left[e^{-f(\\boldsymbol{x}) H_{t}(\\boldsymbol{x})}\\right]}\n\\end{aligned}\n$$\n\n[解析]：boosting算法是根据调整后的样本再去训练下一个基分类器，这就是“重赋权法”的样本分布的调整公式。\n\n\n## 8.20\n\n$$\nH^{\\mathrm{oob}}(\\boldsymbol{x})=\\underset{y \\in \\mathcal{Y}}{\\arg \\max } \\sum_{t=1}^{\\mathrm{T}} \\mathbb{I}\\left(h_{t}(\\boldsymbol{x})=y\\right) \\cdot \\mathbb{I}\\left(\\boldsymbol{x} \\notin D_{t}\\right)\n$$\n\n[解析]：$\\mathbb{I}\\left(h_{t}(\\boldsymbol{x})=y\\right)$表示对$\\mathrm{T}$个基学习器，每一个都判断结果是否与$y$一致，$y$的取值一般是$-1$和$1$，如果基学习器结果与$y$一致，则$\\mathbb{I}\\left(h_{t}(\\boldsymbol{x})=y\\right)=1$，如果样本不在训练集内，则$\\mathbb{I}\\left(\\boldsymbol{x} \\notin D_{t}\\right)=1$，综合起来看就是，对包外的数据，用“投票法”选择包外估计的结果，即1或-1。\n\n## 8.21\n\n$$\n\\epsilon^{\\mathrm{oob}}=\\frac{1}{|D|} \\sum_{(\\boldsymbol{x}, y) \\in D} \\mathbb{I}\\left(H^{\\mathrm{oob}}(\\boldsymbol{x}) \\neq y\\right)\n$$\n\n[解析]：由8.20知，$H^{\\mathrm{oob}}(\\boldsymbol{x})$是对包外的估计，该式表示估计错误的个数除以总的个数，得到泛化误差的包外估计。\n\n## 8.22\n\n$$\nH(\\boldsymbol{x})=\\frac{1}{T} \\sum_{i=1}^{T} h_{i}(\\boldsymbol{x})\n$$\n\n[解析]：对基分类器的结果进行简单的平均。\n\n## 8.23\n\n$$\nH(\\boldsymbol{x})=\\sum_{i=1}^{T} w_{i} h_{i}(\\boldsymbol{x})\n$$\n\n[解析]：对基分类器的结果进行加权平均。\n\n## 8.24\n\n$$\nH(\\boldsymbol{x})=\\left\\{\\begin{array}{ll}\n{c_{j},} & {\\text { if } \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})>0.5 \\sum_{k=1}^{N} \\sum_{i=1}^{T} h_{i}^{k}(\\boldsymbol{x})} \\\\\n{\\text { reject, }} & {\\text { otherwise. }}\n\\end{array}\\right.\n$$\n\n[解析]：当某一个类别$j$的基分类器的结果之和，大于所有结果之和的$\\frac {1}{2}$，则选择该类别$j$为最终结果。\n\n## 8.25\n\n$$\nH(\\boldsymbol{x})=c_{\\underset{j}{ \\arg \\max} \\sum_{i=1}^{T} h_{i}^{j}(\\boldsymbol{x})}\n$$\n\n[解析]：相比于其他类别，该类别$j$的基分类器的结果之和最大，则选择类别$j$为最终结果。\n\n## 8.26\n\n$$\nH(\\boldsymbol{x})=c_{\\underset{j}{ \\arg \\max} \\sum_{i=1}^{T} w_i h_{i}^{j}(\\boldsymbol{x})}\n$$\n\n[解析]：相比于其他类别，该类别$j$的基分类器的结果之和最大，则选择类别$j$为最终结果，与式(8.25)不同的是，该式在基分类器前面乘上一个权重系数，该系数大于等于0，且T个权重之和为1。\n\n## 8.27\n\n$$\nA\\left(h_{i} | \\boldsymbol{x}\\right)=\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n$$\n\n[解析]：该式表示个体学习器结果与预测结果的差值的平方，即为个体学习器的“分歧”。\n\n## 8.28\n\n$$\n\\begin{aligned}\n\\bar{A}(h | \\boldsymbol{x}) &=\\sum_{i=1}^{T} w_{i} A\\left(h_{i} | \\boldsymbol{x}\\right) \\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\n\\end{aligned}\n$$\n\n[解析]：该式表示对各个个体学习器的“分歧”加权平均的结果，即集成的“分歧”。\n\n## 8.29\n\n$$\nE\\left(h_{i} | \\boldsymbol{x}\\right)=\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2}\n$$\n\n[解析]：该式表示个体学习器与真实值之间差值的平方，即个体学习器的平方误差。\n\n## 8.30\n\n$$\nE(H | \\boldsymbol{x})=(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\n$$\n\n[解析]：该式表示集成与真实值之间差值的平方，即集成的平方误差。\n\n## 8.31\n\n$$\n\\bar{A}(h | \\boldsymbol{x}) =\\sum_{i=1}^{T} w_{i} E\\left(h_{i} | \\boldsymbol{x}\\right)-E(H | \\boldsymbol{x})\n$$\n\n[推导]：由(8.28)知\n$$\n\\begin{aligned}\n\\bar{A}(h | \\boldsymbol{x})&=\\sum_{i=1}^{T} w_{i}\\left(h_{i}(\\boldsymbol{x})-H(\\boldsymbol{x})\\right)^{2}\\\\\n&=\\sum_{i=1}^{T} w_{i}(h_i(\\boldsymbol{x})^2-2h_i(\\boldsymbol{x})H(\\boldsymbol{x})+H(\\boldsymbol{x})^2)\\\\\n&=\\sum_{i=1}^{T} w_{i}h_i(\\boldsymbol{x})^2-H(\\boldsymbol{x})^2\n\\end{aligned}\n$$\n\n又因为\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{T} w_{i} E\\left(h_{i} | \\boldsymbol{x}\\right)-E(H | \\boldsymbol{x})\\\\\n&=\\sum_{i=1}^{T} w_{i}\\left(f(\\boldsymbol{x})-h_{i}(\\boldsymbol{x})\\right)^{2}-(f(\\boldsymbol{x})-H(\\boldsymbol{x}))^{2}\\\\\n&=\\sum_{i=1}^{T} w_{i}h_i(\\boldsymbol{x})^2-H(\\boldsymbol{x})^{2}\n\\end{aligned}\n$$\n所以\n$$\n\\bar{A}(h | \\boldsymbol{x}) =\\sum_{i=1}^{T} w_{i} E\\left(h_{i} | \\boldsymbol{x}\\right)-E(H | \\boldsymbol{x})\n$$\n\n## 8.32\n\n$$\n\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}=\\sum_{i=1}^{T} w_{i} \\int E\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}-\\int E(H | \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n$$\n\n[解析]：$\\int A\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}$表示个体学习器在全样本上的“分歧”，$\\sum_{i=1}^{T} w_{i} \\int A\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}$表示集成在全样本上的“分歧”，然后根据式(8.31)拆成误差的形式。\n\n## 8.33\n\n$$\nE_{i}=\\int E\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}\n$$\n\n[解析]：表示个体学习器在全样本上的泛化误差。\n\n## 8.34\n\n$$\nA_{i}=\\int A\\left(h_{i} | \\boldsymbol{x}\\right) p(\\boldsymbol{x}) d \\boldsymbol{x}\n$$\n\n[解析]：表示个体学习器在全样本上的分歧。\n\n## 8.35\n\n$$\nE=\\int E(H | \\boldsymbol{x}) p(\\boldsymbol{x}) d \\boldsymbol{x}\n$$\n\n[解析]：表示集成在全样本上的泛化误差。\n\n## 8.36\n\n$$\nE=\\bar{E}-\\bar{A}\n$$\n\n[解析]：$\\bar{E}$表示个体学习器泛化误差的加权均值，$\\bar{A}$表示个体学习器分歧项的加权均值，该式称为“误差-分歧分解”。','2021-12-10 12:43:49','2021-12-19 13:22:38'),
	(9,1,'chapter9','## 9.5 \n\n$$\n\\mathrm{JC}=\\frac{a}{a+b+c}\n$$\n\n[解析]：给定两个集合$A$和$B$，则Jaccard系数定义为如下公式\n\n$$\n\\mathrm{JC}=\\frac{|A\\bigcap B|}{|A\\bigcup B|}=\\frac{|A\\bigcap B|}{|A|+|B|-|A\\bigcap B|}\n$$\nJaccard系数可以用来描述两个集合的相似程度。\n\n推论：假设全集$U$共有$n$个元素，且$A\\subseteq U$，$B\\subseteq U$，则每一个元素的位置共有四种情况：\n\n1、元素同时在集合$A$和$B$中，这样的元素个数记为$M_{11}$；\n\n2、元素出现在集合$A$中，但没有出现在集合$B$中，这样的元素个数记为$M_{10}$；\n\n3、元素没有出现在集合$A$中，但出现在集合$B$中，这样的元素个数记为$M_{01}$；\n\n4、元素既没有出现在集合$A$中，也没有出现在集合$B$中，这样的元素个数记为$M_{00}$。\n\n根据Jaccard系数的定义，此时的Jaccard系数为如下公式\n$$\n\\mathrm{JC}=\\frac{M_{11}}{M_{11}+M_{10}+M_{01}}\n$$\n由于聚类属于无监督学习，事先并不知道聚类后样本所属类别的类别标记所代表的意义，即便参考模型的类别标记意义是已知的，我们也无法知道聚类后的类别标记与参考模型的类别标记是如何对应的，况且聚类后的类别总数与参考模型的类别总数还可能不一样，因此只用单个样本无法衡量聚类性能的好坏。\n\n由于外部指标的基本思想就是以参考模型的类别划分为参照，因此如果某一个样本对中的两个样本在聚类结果中同属于一个类，在参考模型中也同属于一个类，或者这两个样本在聚类结果中不同属于一个类，在参考模型中也不同属于一个类，那么对于这两个样本来说这是一个好的聚类结果。\n\n总的来说所有样本对中的两个样本共存在四种情况：	\n1、样本对中的两个样本在聚类结果中属于同一个类，在参考模型中也属于同一个类；	\n2、样本对中的两个样本在聚类结果中属于同一个类，在参考模型中不属于同一个类；	\n3、样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中属于同一个类；	\n4、样本对中的两个样本在聚类结果中不属于同一个类，在参考模型中也不属于同一个类。\n\n综上所述，即所有样本对存在着书中公式(9.1)-(9.4)的四种情况，现在假设集合$A$中存放着两个样本都同属于聚类结果的同一个类的样本对，即$A=SS\\bigcup SD$，集合$B$中存放着两个样本都同属于参考模型的同一个类的样本对，即$B=SS\\bigcup DS$，那么根据Jaccard系数的定义有：\n$$\n\\mathrm{JC}=\\frac{|A\\bigcap B|}{|A\\bigcup B|}=\\frac{|SS|}{|SS\\bigcup SD\\bigcup DS|}=\\frac{a}{a+b+c}\n$$\n也可直接将书中公式(9.1)-(9.4)的四种情况类比推论，即$M_{11}=a$，$M_{10}=b$，$M_{01}=c$，所以\n$$\n\\mathrm{JC}=\\frac{M_{11}}{M_{11}+M_{10}+M_{01}}=\\frac{a}{a+b+c}\n$$\n\n## 9.6\n$$\n\\mathrm{FMI}=\\sqrt{\\frac{a}{a+b}\\cdot \\frac{a}{a+c}}\n$$\n\n[解析]：其中$\\frac{a}{a+b}$和$\\frac{a}{a+c}$为Wallace提出的两个非对称指标，$a$代表两个样本在聚类结果和参考模型中均属于同一类的样本对的个数，$a+b$代表两个样本在聚类结果中属于同一类的样本对的个数，$a+c$代表两个样本在参考模型中属于同一类的样本对的个数，这两个非对称指标均可理解为样本对中的两个样本在聚类结果和参考模型中均属于同一类的概率。由于指标的非对称性，这两个概率值往往不一样，因此Fowlkes和Mallows提出利用几何平均数将这两个非对称指标转化为一个对称指标，即Fowlkes and Mallows Index, FMI。\n\n## 9.7\n$$\n\\mathrm{RI}=\\frac{2(a+d)}{m(m-1)}\n$$\n[解析]：Rand Index定义如下：\n$$\n\\mathrm{RI}=\\frac{a+d}{a+b+c+d}=\\frac{a+d}{m(m-1)/2}=\\frac{2(a+d)}{m(m-1)}\n$$\n其中$a$表示聚类结果为同一类别且参考模型给出的划分也属于统一类别的样本对的个数。$d$表示聚类结果不属于同一类别且参考模型也不划分到同一类别的样本对的个数。分母$m(m-1)/2$是所有样本的总对数，因此$\\mathrm{RI}$可以简单理解为聚类结果与参考模型的一致性。\n\n## 9.8\n\n$$\n\\operatorname{avg}(C)=\\frac{2}{|C|(|C|-1)} \\sum_{1 \\leqslant i<j \\leqslant|C|} \\operatorname{dist}\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)\n$$\n\n[解析]：簇内距离的定义式：求和号左边是$(x_i, x_j)$组合个数的倒数，求和号右边是这些组合的距离和，所以两者相乘定义为平均距离。\n\n## 9.33\n$$\n\\sum_{j=1}^{m} \\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\boldsymbol{\\Sigma}_{l}\\right)}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)=0\n$$\n[推导]：根据公式(9.28)可知：\n$$\np\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|\\boldsymbol{\\Sigma}_{i}\\right|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{T} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\right)\n$$\n又根据公式(9.32)，由\n$$\n\\frac{\\partial L L(D)}{\\partial \\boldsymbol{\\mu}_{i}}=\\frac{\\partial L L(D)}{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot \\frac{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\partial \\boldsymbol{\\mu}_{i}}=0\n$$\n其中：\n$$\n\\begin{aligned}\n\\frac{\\partial L L(D)}{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\mathbf{\\Sigma}_{i}\\right)} &=\\frac{\\partial \\sum_{j=1}^{m} \\ln \\left(\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\boldsymbol{\\Sigma}_{l}\\right)\\right)}{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\\\\n&=\\sum_{j=1}^{m} \\frac{\\partial \\ln \\left(\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\boldsymbol{\\Sigma}_{l}\\right)\\right)}{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\\\\n&=\\sum_{j=1}^{m} \\frac{\\alpha_{i}}{\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\boldsymbol{\\Sigma}_{l}\\right)}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\frac{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} &=\\frac{\\partial \\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|\\Sigma_{i}\\right|^{\\frac{1}{2}}} \\exp\\left({-\\frac{1}{2}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|\\boldsymbol{\\Sigma}_{i}\\right|^{\\frac{1}{2}}} \\cdot \\frac{\\partial \\exp\\left({-\\frac{1}{2}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}\\right)}{\\partial \\boldsymbol{\\mu}_{i}}\\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|\\boldsymbol{\\Sigma}_{i}\\right|^{\\frac{1}{2}}}\\cdot \\exp\\left({-\\frac{1}{2}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}\\right) \\cdot-\\frac{1}{2} \\frac{\\partial\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}{\\partial \\boldsymbol{\\mu}_{i}}\\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|\\boldsymbol{\\Sigma}_{i}\\right|^{\\frac{1}{2}}}\\cdot \\exp\\left({-\\frac{1}{2}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}\\right) \\cdot\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\\\\n&=p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\n\\end{aligned}\n$$\n\n其中，由矩阵求导的法则$\\frac{\\partial \\mathbf{a}^{T} \\mathbf{X} \\mathbf{a}}{\\partial \\mathbf{a}}=2\\mathbf{X} \\mathbf{a}$可得：\n$$\n\\begin{aligned}\n-\\frac{1}{2} \\frac{\\partial\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} &=-\\frac{1}{2} \\cdot 2 \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{x}_{j}\\right) \\\\\n&=\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\n\\end{aligned}\n$$\n\n\n因此有：\n$$\n\\frac{\\partial L L(D)}{\\partial \\boldsymbol{\\mu}_{i}}=\\sum_{j=1}^{m} \\frac{\\alpha_{i}}{\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\mathbf{\\Sigma}_{l}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)=0\n$$\n\n\n## 9.34\n\n$$\n\\boldsymbol\\mu_{i}=\\frac{\\sum_{j=1}^{m} \\gamma_{j i} \\boldsymbol{x}_{j}}{\\sum_{j=1}^{m} \\gamma_{j i}}\n$$\n\n\n\n[推导]：由式9.30\n$$\n\\gamma_{j i}=p_{\\mathcal{M}}\\left(z_{j}=i | \\boldsymbol{x}_{j}\\right)=\\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{l=1}^{k} \\alpha_{l} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{l}, \\mathbf{\\Sigma}_{l}\\right)}\n$$\n代入9.33\n$$\n\\sum_{j=1}^{m} \\gamma_{j i}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)=0\n$$\n因此有\n$$\n\\boldsymbol\\mu_{i}=\\frac{\\sum_{j=1}^{m} \\gamma_{j i} \\boldsymbol{x}_{j}}{\\sum_{j=1}^{m} \\gamma_{j i}}\n$$\n\n## 9.35\n$$\n\\mathbf\\Sigma_{i}=\\cfrac{\\sum_{j=1}^m\\gamma_{ji}(\\boldsymbol x_{j}-\\boldsymbol \\mu_{i})(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})^T}{\\sum_{j=1}^m\\gamma_{ji}}\n$$\n[推导]：根据公式(9.28)可知：\n$$\np(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})=\\cfrac{1}{(2\\pi)^\\frac{n}{2}\\left| \\mathbf\\Sigma_{i}\\right |^\\frac{1}{2}}\\exp\\left({-\\frac{1}{2}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})^T\\mathbf\\Sigma_{i}^{-1}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})}\\right)\n$$\n又根据公式(9.32)，由\n$$\n\\cfrac{\\partial LL(D)}{\\partial \\mathbf\\Sigma_{i}}=0\n$$\n可得\n$$\\begin{aligned}\n\\cfrac {\\partial LL(D)}{\\partial\\mathbf\\Sigma_{i}}&=\\cfrac {\\partial}{\\partial \\mathbf\\Sigma_{i}}\\left[\\sum_{j=1}^m\\ln\\Bigg(\\sum_{i=1}^k \\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\Bigg)\\right] \\\\\n&=\\sum_{j=1}^m\\frac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left[\\ln\\Bigg(\\sum_{i=1}^k \\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\Bigg)\\right] \\\\\n&=\\sum_{j=1}^m\\cfrac{\\alpha_{i}\\cdot \\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left(p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\right)}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})} \\\\\n\\end{aligned}$$\n其中\n$$\\begin{aligned}\n\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left(p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\right)&=\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left[\\cfrac{1}{(2\\pi)^\\frac{n}{2}\\left| \\mathbf\\Sigma_{i}\\right |^\\frac{1}{2}}\\exp\\left({-\\frac{1}{2}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})^T\\mathbf\\Sigma_{i}^{-1}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})}\\right)\\right] \\\\\n&=\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left\\{\\exp\\left[\\ln\\left(\\cfrac{1}{(2\\pi)^\\frac{n}{2}\\left| \\mathbf\\Sigma_{i}\\right |^\\frac{1}{2}}\\exp\\left({-\\frac{1}{2}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})^T\\mathbf\\Sigma_{i}^{-1}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})}\\right)\\right)\\right]\\right\\} \\\\\n&=p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\cdot\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left[\\ln\\left(\\cfrac{1}{(2\\pi)^\\frac{n}{2}\\left| \\mathbf\\Sigma_{i}\\right |^\\frac{1}{2}}\\exp\\left({-\\frac{1}{2}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})^T\\mathbf\\Sigma_{i}^{-1}(\\boldsymbol x_{j}-\\boldsymbol\\mu_{i})}\\right)\\right)\\right]\\\\\n&=p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\cdot\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left[\\ln\\cfrac{1}{(2\\pi)^{\\frac{n}{2}}}-\\cfrac{1}{2}\\ln{|\\mathbf{\\Sigma}_i|}-\\frac{1}{2}(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)\\right]\\\\\n&=p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\cdot\\left[-\\cfrac{1}{2}\\cfrac{\\partial\\left(\\ln{|\\mathbf{\\Sigma}_i|}\\right) }{\\partial \\mathbf{\\Sigma}_i}-\\cfrac{1}{2}\\cfrac{\\partial \\left[(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)\\right]}{\\partial \\mathbf{\\Sigma}_i}\\right]\\\\\n\\end{aligned}$$\n由矩阵微分公式$\\cfrac{\\partial |\\mathbf{X}|}{\\partial \\mathbf{X}}=|\\mathbf{X}|\\cdot(\\mathbf{X}^{-1})^{T},\\cfrac{\\partial \\boldsymbol{a}^{T} \\mathbf{X}^{-1} \\boldsymbol{b}}{\\partial \\mathbf{X}}=-\\mathbf{X}^{-T} \\boldsymbol{a b}^{T} \\mathbf{X}^{-T}$可得\n$$\n\\begin{aligned}\n\\cfrac{\\partial}{\\partial\\mathbf\\Sigma_{i}}\\left(p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\right)&=p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})\\cdot\\left[-\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}+\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}\\right]\\\\\n\\end{aligned}\n$$\n将此式代回$\\cfrac {\\partial LL(D)}{\\partial\\mathbf\\Sigma_{i}}$中可得\n$$\n\\cfrac {\\partial LL(D)}{\\partial\\mathbf\\Sigma_{i}}=\\sum_{j=1}^m\\cfrac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}\\cdot\\left[-\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}+\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}\\right]\n$$\n\n\n又由公式(9.30)可知$\\cfrac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=\\gamma_{ji}$，所以上式可进一步化简为\n$$\n\\cfrac {\\partial LL(D)}{\\partial\\mathbf\\Sigma_{i}}=\\sum_{j=1}^m\\gamma_{ji}\\cdot\\left[-\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}+\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}\\right]\n$$\n令上式等于0可得\n$$\n\\cfrac {\\partial LL(D)}{\\partial\\mathbf\\Sigma_{i}}=\\sum_{j=1}^m\\gamma_{ji}\\cdot\\left[-\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}+\\cfrac{1}{2}\\mathbf{\\Sigma}_i^{-1}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}\\right]=0\n$$\n移项推导有：\n$$\n\\begin{aligned}\n\\sum_{j=1}^m\\gamma_{ji}\\cdot\\left[-\\boldsymbol{I}+(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}\\right]&=0\\\\\n\\sum_{j=1}^m\\gamma_{ji}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T\\mathbf{\\Sigma}_i^{-1}&=\\sum_{j=1}^m\\gamma_{ji}\\boldsymbol{I}\\\\\n\\sum_{j=1}^m\\gamma_{ji}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T&=\\sum_{j=1}^m\\gamma_{ji}\\mathbf{\\Sigma}_i\\\\\n\\mathbf{\\Sigma}_i^{-1}\\cdot\\sum_{j=1}^m\\gamma_{ji}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T&=\\sum_{j=1}^m\\gamma_{ji}\\\\\n\\mathbf{\\Sigma}_i&=\\cfrac{\\sum_{j=1}^m\\gamma_{ji}(\\boldsymbol x_j-\\boldsymbol\\mu_i)(\\boldsymbol x_j-\\boldsymbol\\mu_i)^T}{\\sum_{j=1}^m\\gamma_{ji}}\n\\end{aligned}\n$$\n此即为公式(9.35)\n\n## 9.38\n$$\n\\alpha_{i}=\\frac{1}{m}\\sum_{j=1}^m\\gamma_{ji}\n$$\n[推导]：对公式(9.37)两边同时乘以$\\alpha_{i}$可得\n$$\n\\begin{aligned}\n\\sum_{j=1}^m\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}+\\lambda\\alpha_{i}=0\\\\\n\\sum_{j=1}^m\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=-\\lambda\\alpha_{i}\n\\end{aligned}\n$$\n两边对所有混合成分求和可得\n$$\n\\begin{aligned}\\sum_{i=1}^k\\sum_{j=1}^m\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}&=-\\lambda\\sum_{i=1}^k\\alpha_{i}\\\\\n\\sum_{j=1}^m\\sum_{i=1}^k\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}&=-\\lambda\\sum_{i=1}^k\\alpha_{i}\n\\end{aligned}\n$$\n因为\n$$\n\\sum_{i=1}^k\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=\\frac{\\sum_{i=1}^k\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=1\n$$\n且 $\\sum_{i=1}^k\\alpha_{i}=1$，所以有$$m=-\\lambda$$\n\n因此\n$$\n\\sum_{j=1}^m\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=-\\lambda\\alpha_{i}=m\\alpha_{i}\n$$\n因此\n$$\n\\alpha_{i}=\\cfrac{1}{m}\\sum_{j=1}^m\\frac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}\n$$\n又由公式(9.30)可知$\\cfrac{\\alpha_{i}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{i},\\mathbf\\Sigma_{i})}{\\sum_{l=1}^k\\alpha_{l}\\cdot p(\\boldsymbol x_{j}|\\boldsymbol\\mu_{l},\\mathbf\\Sigma_{l})}=\\gamma_{ji}$，所以上式可进一步化简为\n$$\n\\alpha_{i}=\\cfrac{1}{m}\\sum_{j=1}^m\\gamma_{ji}\n$$\n\n此即为公式(9.38)\n\n\n','2021-12-10 12:43:49','2021-12-19 13:22:43'),
	(10,1,'chapter10','## 10.1\n\n$$\nP(e r r)=1-\\sum_{c \\in \\mathcal{Y}} P(c | \\boldsymbol{x}) P(c | \\boldsymbol{z})\n$$\n\n[解析]：$P(c | \\boldsymbol{x}) P(c | \\boldsymbol{z})$表示$x$和$z$同属类$c$的概率，对所有可能的类别$c\\in\\mathcal{Y}$求和，则得到$x$和$z$同属相同类别的概率，因此$1-\\sum_{c \\in \\mathcal{Y}} P(c | \\boldsymbol{x}) P(c | \\boldsymbol{z})$表示$x$和$z$分属不同类别的概率。\n\n\n\n## 10.2\n\n$$\n\\begin{aligned}\nP(e r r) &=1-\\sum_{c \\in \\mathcal{Y}} P(c | \\boldsymbol{x}) P(c | \\boldsymbol{z}) \\\\\n& \\simeq 1-\\sum_{c \\in \\mathcal{Y}} P^{2}(c | \\boldsymbol{x}) \\\\\n& \\leqslant 1-P^{2}\\left(c^{*} | \\boldsymbol{x}\\right) \\\\\n&=\\left(1+P\\left(c^{*} | \\boldsymbol{x}\\right)\\right)\\left(1-P\\left(c^{*} | \\boldsymbol{x}\\right)\\right) \\\\\n& \\leqslant 2 \\times\\left(1-P\\left(c^{*} | \\boldsymbol{x}\\right)\\right)\n\\end{aligned}\n$$\n\n[解析]：第二个式子是来源于前提假设\"假设样本独立同分布，且对任意$x$和任意小正数$\\delta$，在$x$附近$\\delta$距离范围内总能找到一个训练样本\"，假设所有$\\delta$中最小的$\\delta$组成和$\\boldsymbol{x}$同一维度的向量$\\boldsymbol{\\delta}$则$P(c | \\boldsymbol{z}) = P(c | \\boldsymbol{x\\pm\\delta})\\simeq P(c|\\boldsymbol{x})$。第三个式子是应为$c^{*}\\in\\mathcal{Y}$，因此$P^{2}\\left(c^{*} | \\boldsymbol{x}\\right)$是$\\sum_{c \\in \\mathcal{Y}} P^{2}(c | \\boldsymbol{x})$的一个分量，所以$\\sum_{c \\in \\mathcal{Y}} P^{2}(c | \\boldsymbol{x}) \\geqslant P^{2}\\left(c^{*} | \\boldsymbol{x}\\right)$。第四个式子是平方差公式展开，最后一个式子因为$1 + P\\left(c^{*} | \\boldsymbol{x}\\right)\\leqslant 2$。\n\n\n\n## 10.3\n\n$$\n\\begin{aligned}\n\\operatorname{dist}_{i j}^{2} &=\\left\\|\\boldsymbol{z}_{i}\\right\\|^{2}+\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}-2 \\boldsymbol{z}_{i}^{\\mathrm{T}} \\boldsymbol{z}_{j} \\\\\n&=b_{i i}+b_{j j}-2 b_{i j}\n\\end{aligned}\n$$\n\n[推导]：\n$$\n\\begin{aligned}\n\\operatorname{dist}_{i j}^{2} &=\\left\\|\\boldsymbol{z}_{i}-\\boldsymbol{z}_{j}\\right\\|^{2}=\\left(\\boldsymbol{z}_{i}-\\boldsymbol{z}_{j}\\right)^{\\top}\\left(\\boldsymbol{z}_{i}-\\boldsymbol{z}_{j}\\right) \\\\\n&=\\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{i}-\\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j}-\\boldsymbol{z}_{j}^{\\top} \\boldsymbol{z}_{i}+\\boldsymbol{z}_{j}^{\\top} \\boldsymbol{z}_{j} \\\\\n&=\\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{i}+\\boldsymbol{z}_{j}^{\\top} \\boldsymbol{z}_{j}-2 \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j} \\\\\n&=\\left\\|\\boldsymbol{z}_{i}\\right\\|^{2}+\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}-2 \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j} \\\\\n&=b_{i i}+b_{j j}-2 b_{i j}\n\\end{aligned}\n$$\n\n\n## 10.4\n\n$$\n\\sum^m_{i=1}dist^2_{ij}=tr(\\boldsymbol B)+mb_{jj}\n$$\n\n[解析]：首先根据式10.3有\n$$\n\\sum^m_{i=1}dist^2_{ij}= \\sum^m_{i=1}b_{ii}+\\sum^m_{i=1}b_{jj}-2\\sum^m_{i=1}b_{ij}\n$$\n对于第一项，根据矩阵迹的定义，$\\sum^m_{i=1}b_{ii} = tr(\\boldsymbol B)$，对于第二项，由于求和号内元素和$i$无关，因此$\\sum^m_{i=1}b_{jj}=mb_{jj}$，对于第三项有，\n$$\n\\sum_{i=1}^{m} b_{i j}=\\sum_{i=1}^{m} \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j}=\n\\sum_{i=1}^{m} \\boldsymbol{z}_{j}^{\\top} \\boldsymbol{z}_{i}=\n\\boldsymbol{z}_{j}^{\\top} \\sum_{i=1}^{m} \\boldsymbol{z}_{i}=\n\\boldsymbol{z}_{j}^{\\top} \\cdot \\mathbf{0}=0\n$$\n其中$\\sum_{i=1}^{m} \\boldsymbol{z}_{i}=\\mathbf{0}$是利用了书上的前提条件，即将降维后的样本被中心化。\n\n## 10.5\n\n$$\n\\sum_{j=1}^{m} \\operatorname{dist}_{i j}^{2}=\\operatorname{tr}(\\mathbf{B})+m b_{i i}\n$$\n\n[解析]：参考10.4\n\n\n\n## 10.6\n\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\operatorname{dist}_{i j}^{2}=2 m \\operatorname{tr}(\\mathbf{B})\n$$\n\n[推导]：\n$$\n\\begin{aligned}\n\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\operatorname{dist}_{i j}^{2} &=\\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left(\\left\\|z_{i}\\right\\|^{2}+\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}-2 \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j}\\right) \\\\\n&=\\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left\\|\\boldsymbol{z}_{i}\\right\\|^{2}+\\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}-2 \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j} \\\\\n\\end{aligned}\n$$\n其中\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left\\|\\boldsymbol{z}_{i}\\right\\|^{2}=m \\sum_{i=1}^{m}\\left\\|\\boldsymbol{z}_{i}\\right\\|^{2}=m \\operatorname{tr}(\\mathbf{B})\n$$\n\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m}\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}=m \\sum_{j=1}^{m}\\left\\|\\boldsymbol{z}_{j}\\right\\|^{2}=m \\operatorname{tr}(\\mathbf{B})\n$$\n\n$$\n\\sum_{i=1}^{m} \\sum_{j=1}^{m} \\boldsymbol{z}_{i}^{\\top} \\boldsymbol{z}_{j}=0\n$$\n\n最后一个式子是来自于书中的假设，假设降维后的样本$\\mathbf{Z}$被中心化。\n\n## 10.10\n\n$$\nb_{ij}=-\\frac{1}{2}(dist^2_{ij}-dist^2_{i\\cdot}-dist^2_{\\cdot j}+dist^2_{\\cdot\\cdot})\n$$\n\n\n\n[推导]：由公式（10.3）可得\n$$\nb_{ij}=-\\frac{1}{2}(dist^2_{ij}-b_{ii}-b_{jj})\n$$\n\n\n由公式（10.6）和（10.9）可得\n$$\n\\begin{aligned}\ntr(\\boldsymbol B)&=\\frac{1}{2m}\\sum^m_{i=1}\\sum^m_{j=1}dist^2_{ij}\\\\\n&=\\frac{m}{2}dist^2_{\\cdot}\n\\end{aligned}\n$$\n由公式（10.4）和（10.8）可得\n$$\n\\begin{aligned}\nb_{jj}&=\\frac{1}{m}\\sum^m_{i=1}dist^2_{ij}-\\frac{1}{m}tr(\\boldsymbol B)\\\\\n&=dist^2_{\\cdot j}-\\frac{1}{2}dist^2_{\\cdot}\n\\end{aligned}\n$$\n由公式（10.5）和（10.7）可得\n$$\n\\begin{aligned}\nb_{ii}&=\\frac{1}{m}\\sum^m_{j=1}dist^2_{ij}-\\frac{1}{m}tr(\\boldsymbol B)\\\\\n&=dist^2_{i\\cdot}-\\frac{1}{2}dist^2_{\\cdot}\n\\end{aligned}\n$$\n综合可得\n$$\n\\begin{aligned}\nb_{ij}&=-\\frac{1}{2}(dist^2_{ij}-b_{ii}-b_{jj})\\\\\n&=-\\frac{1}{2}(dist^2_{ij}-dist^2_{i\\cdot}+\\frac{1}{2}dist^2_{\\cdot\\cdot}-dist^2_{\\cdot j}+\\frac{1}{2}dist^2_{\\cdot\\cdot})\\\\\n&=-\\frac{1}{2}(dist^2_{ij}-dist^2_{i\\cdot}-dist^2_{\\cdot j}+dist^2_{\\cdot\\cdot})\n\\end{aligned}\n$$\n\n## 10.11\n\n$$\n\\mathbf{Z}=\\mathbf{\\Lambda}_{*}^{1 / 2} \\mathbf{V}_{*}^{\\mathrm{T}} \\in \\mathbb{R}^{d^{*} \\times m}\n$$\n\n[解析]：由题设知，$d^*$为$\\mathbf{V}$的非零特征值，因此$\\mathbf{B}=\\mathbf{V} \\mathbf{\\Lambda} \\mathbf{V}^{\\top}$可以写成$\\mathbf{B}=\\mathbf{V}_{*} \\mathbf{\\Lambda}_{*} \\mathbf{V}_{*}^{\\top}$，其中$\\mathbf{\\Lambda}_{*} \\in \\mathbb{R}^{d \\times d}$为$d$个非零特征值构成的特征值对角矩阵，而$\\mathbf{V}_{*} \\in \\mathbb{R}^{m \\times d}$ 为  $\\mathbf{\\Lambda}_{*} \\in \\mathbb{R}^{d \\times d}$对应的特征值向量矩阵，因此有\n$$\n\\mathbf{B}=\\left(\\mathbf{V}_{*} \\mathbf{\\Lambda}_{*}^{1 / 2}\\right)\\left(\\boldsymbol{\\Lambda}_{*}^{1 / 2} \\mathbf{V}_{*}^{\\top}\\right)\n$$\n故而$\\mathbf{Z}=\\mathbf{\\Lambda}_{*}^{1 / 2} \\mathbf{V}_{*}^{\\top} \\in \\mathbb{R}^{d \\times m}$\n\n## 10.14\n\n$$\n\\begin{aligned}\n\\sum^m_{i=1}\\left\\| \\sum^{d\'}_{j=1}z_{ij}\\boldsymbol{w}_j-\\boldsymbol x_i \\right\\|^2_2&=\\sum^m_{i=1}\\boldsymbol z^{\\mathrm{T}}_i\\boldsymbol z_i-2\\sum^m_{i=1}\\boldsymbol z^{\\mathrm{T}}_i\\mathbf{W}^{\\mathrm{T}}\\boldsymbol x_i +\\text { const }\\\\\n&\\propto -\\operatorname{tr}(\\mathbf{W}^{\\mathrm{T}}(\\sum^m_{i=1}\\boldsymbol x_i\\boldsymbol x^{\\mathrm{T}}_i)\\mathbf{W})\n\\end{aligned}\n$$\n\n[推导]：已知$\\mathbf{W}^{\\mathrm{T}} \\mathbf{W}=\\mathbf{I},\\boldsymbol z_i=\\mathbf{W}^{\\mathrm{T}} \\boldsymbol x_i$，则\n$$\n\\begin{aligned}\n\\sum^m_{i=1}\\left\\| \\sum^{d\'}_{j=1}z_{ij}\\boldsymbol{w}_j-\\boldsymbol x_i \\right\\|^2_2&=\\sum^m_{i=1}\\left\\|\\mathbf{W}\\boldsymbol z_i-\\boldsymbol x_i \\right\\|^2_2\\\\\n&= \\sum^m_{i=1} \\left(\\mathbf{W}\\boldsymbol z_i-\\boldsymbol x_i\\right)^{\\mathrm{T}}\\left(\\mathbf{W}\\boldsymbol z_i-\\boldsymbol x_i\\right)\\\\\n&= \\sum^m_{i=1} \\left(\\boldsymbol z_i^{\\mathrm{T}}\\mathbf{W}^{\\mathrm{T}}\\mathbf{W}\\boldsymbol z_i- \\boldsymbol z_i^{\\mathrm{T}}\\mathbf{W}^{\\mathrm{T}}\\boldsymbol x_i-\\boldsymbol x_i^{\\mathrm{T}}\\mathbf{W}\\boldsymbol z_i+\\boldsymbol x_i^{\\mathrm{T}}\\boldsymbol x_i \\right)\\\\\n&= \\sum^m_{i=1} \\left(\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i- 2\\boldsymbol z_i^{\\mathrm{T}}\\mathbf{W}^{\\mathrm{T}}\\boldsymbol x_i+\\boldsymbol x_i^{\\mathrm{T}}\\boldsymbol x_i \\right)\\\\\n&=\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i-2\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\mathbf{W}^{\\mathrm{T}}\\boldsymbol x_i+\\sum^m_{i=1}\\boldsymbol x^{\\mathrm{T}}_i\\boldsymbol x_i\\\\\n&=\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i-2\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\mathbf{W}^{\\mathrm{T}}\\boldsymbol x_i+\\text { const }\\\\\n&=\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i-2\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i+\\text { const }\\\\\n&=-\\sum^m_{i=1}\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_i+\\text { const }\\\\\n&=-\\sum^m_{i=1}\\operatorname{tr}\\left(\\boldsymbol z_i\\boldsymbol z_i^{\\mathrm{T}}\\right)+\\text { const }\\\\\n&=-\\operatorname{tr}\\left(\\sum^m_{i=1}\\boldsymbol z_i\\boldsymbol z_i^{\\mathrm{T}}\\right)+\\text { const }\\\\\n&=-\\operatorname{tr}\\left(\\sum^m_{i=1}\\mathbf{W}^{\\mathrm{T}} \\boldsymbol x_i\\boldsymbol x_i^{\\mathrm{T}}\\mathbf{W}\\right)+\\text { const }\\\\\n&= -\\operatorname{tr}\\left(\\mathbf{W}^{\\mathrm{T}}\\left(\\sum^m_{i=1}\\boldsymbol x_i\\boldsymbol x^{\\mathrm{T}}_i\\right)\\mathbf{W}\\right)+\\text { const }\\\\\n&\\propto-\\operatorname{tr}\\left(\\mathbf{W}^{\\mathrm{T}}\\left(\\sum^m_{i=1}\\boldsymbol x_i\\boldsymbol x^{\\mathrm{T}}_i\\right)\\mathbf{W}\\right)\\\\\n\\end{aligned}\n$$\n\n\n## 10.17\n$$\n\\mathbf X\\mathbf X^{\\mathrm{T}} \\boldsymbol w_i=\\lambda _i\\boldsymbol w_i\n$$\n[推导]：由式（10.15）可知，主成分分析的优化目标为\n$$\n\\begin{aligned}\n&\\min\\limits_{\\mathbf W} \\quad-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)\\\\\n&s.t. \\quad\\mathbf W^{\\mathrm{T}} \\mathbf W=\\mathbf I\n\\end{aligned}\n$$\n其中，$\\mathbf{X}=\\left(\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\ldots, \\boldsymbol{x}_{m}\\right) \\in \\mathbb{R}^{d \\times m},\\mathbf{W}=\\left(\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{d^{\\prime}}\\right) \\in \\mathbb{R}^{d \\times d^{\\prime}}$，$\\mathbf{I} \\in \\mathbb{R}^{d^{\\prime} \\times d^{\\prime}}$为单位矩阵。对于带矩阵约束的优化问题，根据<a href=\"#ref1\">[1]</a>中讲述的方法可得此优化目标的拉格朗日函数为\n$$\n\\begin{aligned}\nL(\\mathbf W,\\Theta)&=-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)+\\langle \\Theta,\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I\\rangle \\\\\n&=-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)+\\text { tr }\\left(\\Theta^{\\mathrm{T}} (\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I)\\right) \n\\end{aligned}\n$$\n\n\n其中，$\\Theta  \\in \\mathbb{R}^{d^{\\prime} \\times d^{\\prime}}$为拉格朗日乘子矩阵，其维度恒等于约束条件的维度，且其中的每个元素均为未知的拉格朗日乘子，$\\langle \\Theta,\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I\\rangle = \\text { tr }\\left(\\Theta^{\\mathrm{T}} (\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I)\\right)$为矩阵的内积<sup><a href=\"#ref2\">[2]</a></sup>。若此时仅考虑约束$\\boldsymbol{w}_i^{\\mathrm{T}}\\boldsymbol{w}_i=1(i=1,2,...,d^{\\prime})$，则拉格朗日乘子矩阵$\\Theta$此时为对角矩阵，令新的拉格朗日乘子矩阵为$\\Lambda=diag(\\lambda_1,\\lambda_2,...,\\lambda_{d^{\\prime}})\\in \\mathbb{R}^{d^{\\prime} \\times d^{\\prime}}$，则新的拉格朗日函数为\n$$\nL(\\mathbf W,\\Lambda)=-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)+\\text { tr }\\left(\\Lambda^{\\mathrm{T}} (\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I)\\right)\n$$\n对拉格朗日函数关于$\\mathbf{W}$求导可得\n$$\n\\begin{aligned}\n\\cfrac{\\partial L(\\mathbf W,\\Lambda)}{\\partial \\mathbf W}&=\\cfrac{\\partial}{\\partial \\mathbf W}\\left[-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)+\\text { tr }\\left(\\Lambda^{\\mathrm{T}} (\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I)\\right)\\right] \\\\\n&=-\\cfrac{\\partial}{\\partial \\mathbf W}\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)+\\cfrac{\\partial}{\\partial \\mathbf W}\\text { tr }\\left(\\Lambda^{\\mathrm{T}} (\\mathbf W^{\\mathrm{T}} \\mathbf W-\\mathbf I)\\right) \\\\\n\\end{aligned}\n$$\n由矩阵微分公式$\\cfrac{\\partial}{\\partial \\mathbf{X}} \\text { tr }(\\mathbf{X}^{\\mathrm{T}}  \\mathbf{B} \\mathbf{X})=\\mathbf{B X}+\\mathbf{B}^{\\mathrm{T}}  \\mathbf{X},\\cfrac{\\partial}{\\partial \\mathbf{X}} \\text { tr }\\left(\\mathbf{B X}^{\\mathrm{T}}  \\mathbf{X}\\right)=\\mathbf{X B}^{\\mathrm{T}} +\\mathbf{X B}$可得\n$$\n\\begin{aligned}\n\\cfrac{\\partial L(\\mathbf W,\\Lambda)}{\\partial \\mathbf W}&=-2\\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W+\\mathbf{W}\\Lambda+\\mathbf{W}\\Lambda^{\\mathrm{T}}  \\\\\n&=-2\\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W+\\mathbf{W}(\\Lambda+\\Lambda^{\\mathrm{T}} ) \\\\\n&=-2\\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W+2\\mathbf{W}\\Lambda\n\\end{aligned}\n$$\n令$\\cfrac{\\partial L(\\mathbf W,\\Lambda)}{\\partial \\mathbf W}=\\mathbf 0$可得\n$$\n\\begin{aligned}\n-2\\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W+2\\mathbf{W}\\Lambda&=\\mathbf 0\\\\\n\\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W&=\\mathbf{W}\\Lambda\\\\\n\\end{aligned}\n$$\n将$\\mathbf W$和$\\Lambda$展开可得\n$$\n\\mathbf X\\mathbf X^{\\mathrm{T}} \\boldsymbol w_i=\\lambda _i\\boldsymbol w_i,\\quad i=1,2,...,d^{\\prime}\n$$\n显然，此式为矩阵特征值和特征向量的定义式，其中$\\lambda_i,\\boldsymbol w_i$分别表示矩阵$\\mathbf X\\mathbf X^{\\mathrm{T}}$的特征值和单位特征向量。由于以上是仅考虑约束$\\boldsymbol{w}_i^{\\mathrm{T}}\\boldsymbol{w}_i=1$所求得的结果，而$\\boldsymbol{w}_i$还需满足约束$\\boldsymbol{w}_{i}^{\\mathrm{T}}\\boldsymbol{w}_{j}=0(i\\neq j)$。观察$\\mathbf X\\mathbf X^{\\mathrm{T}}$的定义可知，$\\mathbf X\\mathbf X^{\\mathrm{T}}$是一个实对称矩阵，实对称矩阵的不同特征值所对应的特征向量之间相互正交，同一特征值的不同特征向量可以通过施密特正交化使其变得正交，所以通过上式求得的$\\boldsymbol w_i$可以同时满足约束$\\boldsymbol{w}_i^{\\mathrm{T}}\\boldsymbol{w}_i=1,\\boldsymbol{w}_{i}^{\\mathrm{T}}\\boldsymbol{w}_{j}=0(i\\neq j)$。根据拉格朗日乘子法的原理可知，此时求得的结果仅是最优解的必要条件，而且$\\mathbf X\\mathbf X^{\\mathrm{T}}$有$d$个相互正交的单位特征向量，所以还需要从这$d$个特征向量里找出$d^{\\prime}$个能使得目标函数达到最优值的特征向量作为最优解。将$\\mathbf X\\mathbf X^{\\mathrm{T}} \\boldsymbol w_i=\\lambda _i\\boldsymbol w_i$代入目标函数可得\n$$\n\\begin{aligned}\n\\min\\limits_{\\mathbf W}-\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W)&=\\max\\limits_{\\mathbf W}\\text { tr }(\\mathbf W^{\\mathrm{T}} \\mathbf X\\mathbf X^{\\mathrm{T}} \\mathbf W) \\\\\n&=\\max\\limits_{\\mathbf W}\\sum_{i=1}^{d^{\\prime}}\\boldsymbol w_i^{\\mathrm{T}}\\mathbf X\\mathbf X^{\\mathrm{T}} \\boldsymbol w_i \\\\\n&=\\max\\limits_{\\mathbf W}\\sum_{i=1}^{d^{\\prime}}\\boldsymbol w_i^{\\mathrm{T}}\\cdot\\lambda _i\\boldsymbol w_i \\\\\n&=\\max\\limits_{\\mathbf W}\\sum_{i=1}^{d^{\\prime}}\\lambda _i\\boldsymbol w_i^{\\mathrm{T}}\\boldsymbol w_i \\\\\n&=\\max\\limits_{\\mathbf W}\\sum_{i=1}^{d^{\\prime}}\\lambda _i \\\\\n\\end{aligned}\n$$\n\n\n显然，此时只需要令$\\lambda_1,\\lambda_2,...,\\lambda_{d^{\\prime}}$和$\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{d^{\\prime}}$分别为矩阵$\\mathbf X\\mathbf X^{\\mathrm{T}}$的前$d^{\\prime}$个最大的特征值和单位特征向量就能使得目标函数达到最优值。\n\n## 10.24\n$$\n\\mathbf{K}\\boldsymbol{\\alpha}^j=\\lambda_j\\boldsymbol{\\alpha}^j\n$$\n\n[推导]：已知$\\boldsymbol z_i=\\phi(\\boldsymbol x_i)$，类比$\\mathbf{X}=\\{\\boldsymbol x_1,\\boldsymbol x_2,...,\\boldsymbol x_m\\}$可以构造$\\mathbf{Z}=\\{\\boldsymbol z_1,\\boldsymbol z_2,...,\\boldsymbol z_m\\}$，所以公式(10.21)可变换为\n$$\n\\left(\\sum_{i=1}^{m} \\phi(\\boldsymbol{x}_{i}) \\phi(\\boldsymbol{x}_{i})^{\\mathrm{T}}\\right)\\boldsymbol w_j=\\left(\\sum_{i=1}^{m} \\boldsymbol z_i \\boldsymbol z_i^{\\mathrm{T}}\\right)\\boldsymbol w_j=\\mathbf{Z}\\mathbf{Z}^{\\mathrm{T}}\\boldsymbol w_j=\\lambda_j\\boldsymbol w_j\n$$\n又由公式(10.22)可知\n$$\n\\boldsymbol w_j=\\sum_{i=1}^{m} \\phi\\left(\\boldsymbol{x}_{i}\\right) \\alpha_{i}^j=\\sum_{i=1}^{m} \\boldsymbol z_i \\alpha_{i}^j=\\mathbf{Z}\\boldsymbol{\\alpha}^j\n$$\n其中，$\\boldsymbol{\\alpha}^j=(\\alpha_{1}^j;\\alpha_{2}^j;...;\\alpha_{m}^j)\\in \\mathbb{R}^{m \\times 1} $。所以公式(10.21)可以进一步变换为\n$$\n\\begin{aligned}\n\\mathbf{Z}\\mathbf{Z}^{\\mathrm{T}}\\mathbf{Z}\\boldsymbol{\\alpha}^j&=\\lambda_j\\mathbf{Z}\\boldsymbol{\\alpha}^j \\\\\n\\mathbf{Z}\\mathbf{Z}^{\\mathrm{T}}\\mathbf{Z}\\boldsymbol{\\alpha}^j&=\\mathbf{Z}\\lambda_j\\boldsymbol{\\alpha}^j\n\\end{aligned}\n$$\n由于此时的目标是要求出$\\boldsymbol w_j$，也就等价于要求出满足上式的$\\boldsymbol{\\alpha}^j$，显然，此时满足$\\mathbf{Z}^{\\mathrm{T}}\\mathbf{Z}\\boldsymbol{\\alpha}^j=\\lambda_j\\boldsymbol{\\alpha}^j $的$\\boldsymbol{\\alpha}^j$一定满足上式，所以问题转化为了求解满足下式的$\\boldsymbol{\\alpha}^j$：\n$$\n\\mathbf{Z}^{\\mathrm{T}}\\mathbf{Z}\\boldsymbol{\\alpha}^j=\\lambda_j\\boldsymbol{\\alpha}^j\n$$\n\n\n令$\\mathbf{Z}^{\\mathrm{T}}\\mathbf{Z}=\\mathbf{K}$，那么上式可化为\n$$\n\\mathbf{K}\\boldsymbol{\\alpha}^j=\\lambda_j\\boldsymbol{\\alpha}^j\n$$\n\n\n此式即为公式(10.24)，其中矩阵$\\mathbf{K}$的第i行第j列的元素$(\\mathbf{K})_{ij}=\\boldsymbol z_i^{\\mathrm{T}}\\boldsymbol z_j=\\phi(\\boldsymbol x_i)^{\\mathrm{T}}\\phi(\\boldsymbol x_j)=\\kappa\\left(\\boldsymbol{x}_{i}, \\boldsymbol{x}_{j}\\right)$\n\n## 10.28\n$$\nw_{i j}=\\frac{\\sum\\limits_{k \\in Q_{i}} C_{j k}^{-1}}{\\sum\\limits_{l, s \\in Q_{i}} C_{l s}^{-1}}\n$$\n\n\n\n[推导]：由书中上下文可知，式(10.28)是如下优化问题的解。\n$$\n\\begin{aligned} \n\\min _{\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{m}} & \\sum_{i=1}^{m}\\left\\|\\boldsymbol{x}_{i}-\\sum_{j \\in Q_{i}} w_{i j} \\boldsymbol{x}_{j}\\right\\|_{2}^{2} \\\\ \n\\text { s.t. } & \\sum_{j \\in Q_{i}} w_{i j}=1 \n\\end{aligned}\n$$\n\n\n若令$\\boldsymbol{x}_{i}\\in \\mathbb{R}^{d\\times 1},Q_i=\\{q_i^1,q_i^2,...,q_i^n\\}$，则上述优化问题的目标函数可以进行如下恒等变形\n$$\n\\begin{aligned} \n\\sum_{i=1}^{m}\\left\\|\\boldsymbol{x}_{i}-\\sum_{j \\in Q_{i}} w_{i j} \\boldsymbol{x}_{j}\\right\\|_{2}^{2}&=\\sum_{i=1}^{m}\\left\\|\\sum_{j \\in Q_{i}} w_{i j} \\boldsymbol{x}_{i}-\\sum_{j \\in Q_{i}} w_{i j} \\boldsymbol{x}_{j}\\right\\|_{2}^{2} \\\\ \n&=\\sum_{i=1}^{m}\\left\\|\\sum_{j \\in Q_{i}} w_{i j}(\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}) \\right\\|_{2}^{2} \\\\ \n&=\\sum_{i=1}^{m}\\left\\|\\mathbf{X}_i\\boldsymbol{w_i} \\right\\|_{2}^{2} \\\\\n&=\\sum_{i=1}^{m}\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i} \\\\ \n\\end{aligned}\n$$\n\n\n其中$\\boldsymbol{w_i}=(w_{iq_i^1},w_{iq_i^2},...,w_{iq_i^n})\\in \\mathbb{R}^{n\\times 1}$，$\\mathbf{X}_i=\\left( \\boldsymbol{x}_{i}-\\boldsymbol{x}_{q_i^1}, \\boldsymbol{x}_{i}-\\boldsymbol{x}_{q_i^2},...,\\boldsymbol{x}_{i}-\\boldsymbol{x}_{q_i^n}\\right)\\in \\mathbb{R}^{d\\times n}$。同理，约束条件也可以进行如下恒等变形\n$$\n\\sum_{j \\in Q_{i}} w_{i j}=\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}=1 \n$$\n\n\n其中$\\boldsymbol{I}=(1,1,...,1)\\in \\mathbb{R}^{n\\times 1}$为$n$行1列的元素值全为1的向量。因此，上述优化问题可以重写为\n$$\n\\begin{aligned} \n\\min _{\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{m}} & \\sum_{i=1}^{m}\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i} \\\\ \n\\text { s.t. } & \\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}=1\n\\end{aligned}\n$$\n\n\n显然，此问题为带约束的优化问题，因此可以考虑使用拉格朗日乘子法来进行求解。由拉格朗日乘子法可得此优化问题的拉格朗日函数为\n$$\nL(\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{m},\\lambda)=\\sum_{i=1}^{m}\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}+\\lambda\\left(\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}-1\\right)\n$$\n\n\n对拉格朗日函数关于$\\boldsymbol{w_i}$求偏导并令其等于0可得\n$$\n\\begin{aligned} \n\\cfrac{\\partial L(\\boldsymbol{w}_{1}, \\boldsymbol{w}_{2}, \\ldots, \\boldsymbol{w}_{m},\\lambda)}{\\partial \\boldsymbol{w_i}}&=\\cfrac{\\partial \\left[\\sum_{i=1}^{m}\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}+\\lambda\\left(\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}-1\\right)\\right]}{\\partial \\boldsymbol{w_i}}=0\\\\\n&=\\cfrac{\\partial \\left[\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}+\\lambda\\left(\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}-1\\right)\\right]}{\\partial \\boldsymbol{w_i}}=0\\\\\n\\end{aligned}\n$$\n\n\n又由矩阵微分公式$\\cfrac{\\partial \\boldsymbol{x}^{T} \\mathbf{B} \\boldsymbol{x}}{\\partial \\boldsymbol{x}}=\\left(\\mathbf{B}+\\mathbf{B}^{\\mathrm{T}}\\right) \\boldsymbol{x},\\cfrac{\\partial \\boldsymbol{x}^{T} \\boldsymbol{a}}{\\partial \\boldsymbol{x}}=\\boldsymbol{a}$可得\n$$\n\\begin{aligned}\n\\cfrac{\\partial \\left[\\boldsymbol{w_i}^{\\mathrm{T}}\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}+\\lambda\\left(\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}-1\\right)\\right]}{\\partial \\boldsymbol{w_i}}&=2\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}+\\lambda \\boldsymbol{I}=0\\\\\n\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i\\boldsymbol{w_i}&=-\\frac{1}{2}\\lambda \\boldsymbol{I}\n\\end{aligned}\n$$\n若$\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i$可逆，则\n$$\n\\boldsymbol{w_i}=-\\frac{1}{2}\\lambda(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}\n$$\n\n\n又因为$\\boldsymbol{w_i}^{\\mathrm{T}}\\boldsymbol{I}=\\boldsymbol{I}^{\\mathrm{T}}\\boldsymbol{w_i}=1$，则上式两边同时左乘$\\boldsymbol{I}^{\\mathrm{T}}$可得\n$$\n\\begin{aligned}\n\\boldsymbol{I}^{\\mathrm{T}}\\boldsymbol{w_i}&=-\\frac{1}{2}\\lambda\\boldsymbol{I}^{\\mathrm{T}}(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}=1\\\\\n-\\frac{1}{2}\\lambda&=\\cfrac{1}{\\boldsymbol{I}^{\\mathrm{T}}(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}}\n\\end{aligned}\n$$\n将其代回$\\boldsymbol{w_i}=-\\frac{1}{2}\\lambda(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}$即可解得\n$$\n\\boldsymbol{w_i}=\\cfrac{(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}}{\\boldsymbol{I}^{\\mathrm{T}}(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}\\boldsymbol{I}}\n$$\n\n\n若令矩阵$(\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i)^{-1}$第$j$行第$k$列的元素为$C_{jk}^{-1}$，则\n$$\nw_{ij}=w_{i q_i^j}=\\frac{\\sum\\limits_{k \\in Q_{i}} C_{j k}^{-1}}{\\sum\\limits_{l, s \\in Q_{i}} C_{l s}^{-1}}\n$$\n\n\n此即为公式(10.28)。显然，若$\\mathbf{X}_i^{\\mathrm{T}}\\mathbf{X}_i$可逆，此优化问题即为凸优化问题，且此时用拉格朗日乘子法求得的$\\boldsymbol{w_i}$为全局最优解。\n\n## 10.31\n$$\n\\begin{aligned}\n&\\min\\limits_{\\boldsymbol Z}tr(\\boldsymbol Z \\boldsymbol M \\boldsymbol Z^T)\\\\\n&s.t. \\boldsymbol Z^T\\boldsymbol Z=\\boldsymbol I. \n\\end{aligned}\n$$\n\n\n\n[推导]：\n$$\n\\begin{aligned}\n\\min\\limits_{\\boldsymbol Z}\\sum^m_{i=1}\\| \\boldsymbol z_i-\\sum_{j \\in Q_i}w_{ij}\\boldsymbol z_j \\|^2_2&=\\sum^m_{i=1}\\|\\boldsymbol Z\\boldsymbol I_i-\\boldsymbol Z\\boldsymbol W_i\\|^2_2\\\\\n&=\\sum^m_{i=1}\\|\\boldsymbol Z(\\boldsymbol I_i-\\boldsymbol W_i)\\|^2_2\\\\\n&=\\sum^m_{i=1}(\\boldsymbol Z(\\boldsymbol I_i-\\boldsymbol W_i))^T\\boldsymbol Z(\\boldsymbol I_i-\\boldsymbol W_i)\\\\\n&=\\sum^m_{i=1}(\\boldsymbol I_i-\\boldsymbol W_i)^T\\boldsymbol Z^T\\boldsymbol Z(\\boldsymbol I_i-\\boldsymbol W_i)\\\\\n&=tr((\\boldsymbol I-\\boldsymbol W)^T\\boldsymbol Z^T\\boldsymbol Z(\\boldsymbol I-\\boldsymbol W))\\\\\n&=tr(\\boldsymbol Z(\\boldsymbol I-\\boldsymbol W)(\\boldsymbol I-\\boldsymbol W)^T\\boldsymbol Z^T)\\\\\n&=tr(\\boldsymbol Z\\boldsymbol M\\boldsymbol Z^T)\n\\end{aligned}\n$$\n\n\n其中，$\\boldsymbol M=(\\boldsymbol I-\\boldsymbol W)(\\boldsymbol I-\\boldsymbol W)^T$。\n[解析]：约束条件$\\boldsymbol Z^T\\boldsymbol Z=\\boldsymbol I$是为了得到标准化（标准正交空间）的低维数据。\n\n## 参考文献\n<span id=\"ref1\">[1][How to set up Lagrangian optimization with matrix constrains](https://math.stackexchange.com/questions/1104376/how-to-set-up-lagrangian-optimization-with-matrix-constrains)</span><br>\n<span id=\"ref2\">[2][Frobenius inner product](https://en.wikipedia.org/wiki/Frobenius_inner_product)</span>','2021-12-10 12:43:49','2021-12-19 13:22:48'),
	(11,1,'chapter11','## 11.1\n\n$$\n\\operatorname{Gain}(A)=\\operatorname{Ent}(D)-\\sum_{v=1}^{V} \\frac{\\left|D^{v}\\right|}{|D|} \\operatorname{Ent}\\left(D^{v}\\right)\n$$\n\n[解析]：此为信息增益的定义式，对数据集$D$和属性子集$A$，假设根据$A$的取值将$D$分为了$V$个子集$\\{D^1,D^2,\\dots,D^V\\}$，那么信息增益的定义为划分之前数据集$D$的信息熵和划分之后每个子数据集$D^v$的信息熵的差。熵用来衡量一个系统的混乱程度，因此划分前和划分后熵的差越大，表示划分越有效，划分带来的”信息增益“越大。\n\n## 11.2\n\n$$\n\\operatorname{Ent}(D)=-\\sum_{i=1}^{| \\mathcal{Y |}} p_{k} \\log _{2} p_{k}\n$$\n\n[解析]：此为信息熵的定义式，其中$p_k, k=1, 2, \\dots \\vert\\mathcal{Y}\\vert$表示$D$中第$i$类样本所占的比例。可以看出，样本越纯，即$p_k\\rightarrow 0$或$p_k\\rightarrow 1$时，$\\mathrm{Ent}(D)$越小，其最小值为0。\n\n## 11.5\n\n$$\n\\min _{\\boldsymbol{w}} \\sum_{i=1}^{m}\\left(y_{i}-\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}\\right)^{2}\n$$\n\n[解析]：该式为线性回归的优化目标式，$y_i$表示样本$i$的真实值，而$w^\\top x_i$表示其预测值，这里使用预测值和真实值差的平方衡量预测值偏离真实值的大小。\n\n## 11.6\n\n$$\n\\min _{\\boldsymbol{w}} \\sum_{i=1}^{m}\\left(y_{i}-\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}\\right)^{2}+\\lambda\\|\\boldsymbol{w}\\|_{2}^{2}\n$$\n\n[解析]：该式为加入了$\\mathrm{L}_2$正规化项的优化目标，也叫\"岭回归\"，$\\lambda$用来调节误差项和正规化项的相对重要性，引入正规化项的目的是为了防止$w$的分量过太而导致过拟合的风险。\n\n## 11.7\n\n$$\n\\min _{\\boldsymbol{w}} \\sum_{i=1}^{m}\\left(y_{i}-\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}\\right)^{2}+\\lambda\\|\\boldsymbol{w}\\|_{1}\n$$\n\n[解析]：该式将11.6中的$\\mathrm{L}_2$正规化项替换成了$\\mathrm{L}_1$正规化项，也叫LASSO回归。关于$\\mathrm{L}_2$和$\\mathrm{L}_1$两个正规化项的区别，原书图11.2给出了很形象的解释。具体来说，结合$\\mathrm{L}_1$范数优化的模型参数分量更偏向于取0，因此更容易取得稀疏解。\n\n## 11.10\n\n$$\n\\begin{aligned}\n\\hat{f}(\\boldsymbol{x}) & \\simeq f\\left(\\boldsymbol{x}_{k}\\right)+\\left\\langle\\nabla f\\left(\\boldsymbol{x}_{k}\\right), \\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\rangle+\\frac{L}{2}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\|^{2} \\\\\n&=\\frac{L}{2}\\left\\|\\boldsymbol{x}-\\left(\\boldsymbol{x}_{k}-\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\\right)\\right\\|_{2}^{2}+\\mathrm{const}\n\\end{aligned}\n$$\n\n[解析]：首先注意优化目标式和11.7 LASSO回归的联系和区别，该式中的$x$对应到式11.7的$w$，即我们优化的目标。再解释下什么是[$L\\mathrm{-Lipschitz}$条件](https://zh.wikipedia.org/wiki/利普希茨連續)，根据维基百科的定义：它是一个比通常[连续](https://zh.wikipedia.org/wiki/連續函數)更强的光滑性条件。直觉上，利普希茨连续函数限制了函数改变的速度，符合利普希茨条件的函数的斜率，必小于一个称为利普希茨常数的实数（该常数依函数而定）。\n\n注意这里存在一个笔误，在wiki百科的定义中，式11.9应该写成\n$$\n\\left\\vert\\nabla f\\left(\\boldsymbol{x}^{\\prime}\\right)-\\nabla f(\\boldsymbol{x})\\right\\vert \\leqslant L\\left\\vert\\boldsymbol{x}^{\\prime}-\\boldsymbol{x}\\right\\vert \\quad\\left(\\forall \\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)\n$$\n移项得\n$$\n\\frac{\\left|\\nabla f\\left(\\boldsymbol{x}^{\\prime}\\right)-\\nabla f(\\boldsymbol{x})\\right|}{\\vert x^\\prime - x\\vert}\\leqslant L \\quad\\left(\\forall \\boldsymbol{x}, \\boldsymbol{x}^{\\prime}\\right)\n$$\n由于上式对所有的$x, x^\\prime$都成立，由[导数的定义](https://zh.wikipedia.org/wiki/导数)，上式可以看成是$f(x)$的二阶导数恒不大于$L$。即\n$$\n\\nabla^2f(x)\\leqslant L\n$$\n得到这个结论之后，我们来推导式11.10。\n\n由[泰勒公式](https://zh.wikipedia.org/wiki/泰勒公式)，$x_k$附近的$f(x)$通过二阶泰勒展开式可近似为\n$$\n\\begin{aligned}\n\\hat{f}(\\boldsymbol{x}) & \\simeq f\\left(\\boldsymbol{x}_{k}\\right)+\\left\\langle\\nabla f\\left(\\boldsymbol{x}_{k}\\right), \\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\rangle+\\frac{\\nabla^2f(x_k)}{2}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\|^{2} \\\\\n&\\leqslant\n f\\left(\\boldsymbol{x}_{k}\\right)+\\left\\langle\\nabla f\\left(\\boldsymbol{x}_{k}\\right), \\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\rangle+\\frac{L}{2}\\left\\|\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right\\|^{2} \\\\\n&= f\\left(\\boldsymbol{x}_{k}\\right)+\\nabla f\\left(\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{L}{2}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)\\\\\n&=f(x_k)+\\frac{L}{2}\\left(\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{2}{L}\\nabla f\\left(\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)\\right)\\\\\n&=f(x_k)+\\frac{L}{2}\\left(\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{2}{L}\\nabla f\\left(\\boldsymbol{x}_{k}\\right)^{\\top}\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{1}{L^2}\\nabla f(x_k)^\\top\\nabla f(x_k)\\right) -\\frac{1}{2L}\\nabla f(x_k)^\\top\\nabla f(x_k)\\\\\n&=f(x_k)+\\frac{L}{2}\\left(\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\\right)^{\\top}\\left(\\left(\\boldsymbol{x}-\\boldsymbol{x}_{k}\\right)+\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\\right)-\\frac{1}{2L}\\nabla f(x_k)^\\top\\nabla f(x_k)\\\\\n&=\\frac{L}{2}\\left\\|\\boldsymbol{x}-\\left(\\boldsymbol{x}_{k}-\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\\right)\\right\\|_{2}^{2}+\\mathrm{const}\n\\end{aligned}\n$$\n其中$\\mathrm{const}=f(x_k)-\\frac{1}{2 L} \\nabla f\\left(x_{k}\\right)^{\\top} \\nabla f\\left(x_{k}\\right)$\n\n\n\n## 11.11\n\n$$\n\\boldsymbol{x}_{k+1}=\\boldsymbol{x}_{k}-\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\n$$\n[解析]：这个很容易理解，因为2范数的最小值为0，当$\\boldsymbol{x}_{k+1}=\\boldsymbol{x}_{k}-\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)$时，$\\hat{f}(x_{k+1})\\leqslant\\hat{f}(x_k)$恒成立，同理$\\hat{f}(x_{k+2})\\leqslant\\hat{f}(x_{k+1}), \\cdots$，因此反复迭代能够使$\\hat{f}(x)$的值不断下降。\n\n## 11.12\n\n$$\n\\boldsymbol{x}_{k+1}=\\underset{\\boldsymbol{x}}{\\arg \\min } \\frac{L}{2}\\left\\|\\boldsymbol{x}-\\left(\\boldsymbol{x}_{k}-\\frac{1}{L} \\nabla f\\left(\\boldsymbol{x}_{k}\\right)\\right)\\right\\|_{2}^{2}+\\lambda\\|\\boldsymbol{x}\\|_{1}\n$$\n\n[解析]：式11.11是用来优化$\\hat{f}(x)$的，而对于式11.8，优化的函数为$f(x)+\\lambda\\left\\Vert x\\right\\Vert_1$，由泰勒展开公式，优化的目标可近似为$\\hat{f}(x)+\\lambda\\Vert x\\Vert_1$，根据式11.10可知，$x$的更新由式11.12决定。\n\n## 11.13\n\n$$\n\\boldsymbol{x}_{k+1}=\\underset{\\boldsymbol{x}}{\\arg \\min } \\frac{L}{2}\\|\\boldsymbol{x}-\\boldsymbol{z}\\|_{2}^{2}+\\lambda\\|\\boldsymbol{x}\\|\n$$\n\n[解析]：这里将式11.12的优化步骤拆分成了两步，首先令$z=x_{k}-\\frac{1}{L} \\nabla f\\left(x_{k}\\right)$以计算$z$，然后再求解式11.13，得到的结果是一致的。\n\n## 11.14\n\n$$\nx_{k+1}^{i}=\\left\\{\\begin{array}{ll}\n{z^{i}-\\lambda / L,} & {\\lambda / L<z^{i}} \\\\\n{0,} & {\\left|z^{i}\\right| \\leqslant \\lambda / L} \\\\\n{z^{i}+\\lambda / L,} & {z^{i}<-\\lambda / L}\n\\end{array}\\right.\n$$\n\n[解析]：令优化函数\n$$\n\\begin{aligned}\ng(\\boldsymbol{x}) &=\\frac{L}{2}\\|\\boldsymbol{x}-\\boldsymbol{z}\\|_{2}^{2}+\\lambda\\|\\boldsymbol{x}\\|_{1} \\\\\n&=\\frac{L}{2} \\sum_{i=1}^{d}\\left\\|x^{i}-z^{i}\\right\\|_{2}^{2}+\\lambda \\sum_{i=1}^{d}\\left\\|x^{i}\\right\\|_{1} \\\\\n&=\\sum_{i=1}^{d}\\left(\\frac{L}{2}\\left(x^{i}-z^{i}\\right)^{2}+\\lambda\\left|x^{i}\\right|\\right)\n\\end{aligned}\n$$\n\n这个式子表明优化$g(\\boldsymbol{x})$可以被拆解成优化$\\boldsymbol{x}$的各个分量的形式，对分量$x_i$，其优化函数\n$$\ng\\left(x^{i}\\right)=\\frac{L}{2}\\left(x^{i}-z^{i}\\right)^{2}+\\lambda\\left|x^{i}\\right|\n$$\n求导得\n$$\n\\frac{d g\\left(x^{i}\\right)}{d x^{i}}=L\\left(x^{i}-z^{i}\\right)+\\lambda s g n\\left(x^{i}\\right)\n$$\n其中\n$$\n\\operatorname{sign}\\left(x^{i}\\right)=\\left\\{\\begin{array}{ll}\n{1,} & {x^{i}>0} \\\\\n{-1,} & {x^{i}<0}\n\\end{array}\\right.\n$$\n称为[符号函数](https://en.wikipedia.org/wiki/Sign_function)，对于$x_i=0$的特殊情况，由于$\\vert x_i \\vert$在$x_i=0$点出不光滑，所以其不可导，需单独讨论。令$\\frac{d g\\left(x^{i}\\right)}{d x^{i}}=0$有\n$$\nx^{i}=z^{i}-\\frac{\\lambda}{L} \\operatorname{sign}\\left(x^{i}\\right)\n$$\n此式的解即为优化目标$g(x^i)$的极值点，因为等式两端均含有未知变量$x^i$，故分情况讨论。\n\n1. 当$z^i>\\frac{\\lambda}{L}$时：\n\n   a. 假设$x^i<0$，则$\\operatorname{sign}(x^i)=-1$，那么有$x^i=z^i+\\frac{\\lambda}{L}>0$与假设矛盾；\n\n   b. 假设$x^i>0$，则$\\operatorname{sign}(x^i)=1$，那么有$x^i=z^i-\\frac{\\lambda}{L}>0$和假设相符合，下面来检验$x^i=z^i-\\frac{\\lambda}{L}$是否是使函数$g(x^i)$的取得最小值。当$x^i>0$时，\n   $$\n   \\frac{d g\\left(x^{i}\\right)}{d x^{i}}=L\\left(x^{i}-z^{i}\\right)+\\lambda\n   $$\n   在定义域内连续可导，则$g(x^i)$的二阶导数\n   $$\n   \\frac{d^2 g\\left(x^{i}\\right)}{{d x^{i}}^2}=L\n   $$\n   由于$L$是Lipschitz常数恒大于0，因为$x^i=z^i-\\frac{\\lambda}{L}$是函数$g(x^i)$的最小值。\n\n2. 当$z_i<-\\frac{\\lambda}{L}$时：\n\n   a. 假设$x^i>0$，则$\\operatorname{sign}(x^i)=1$，那么有$x^i=z^i-\\frac{\\lambda}{L}<0$与假设矛盾；\n\n   b. 假设$x^i<0$，则$\\operatorname{sign}(x^i)=-1$，那么有$x^i=z^i+\\frac{\\lambda}{L}<0$与假设相符，由上述二阶导数恒大于0可知，$x^i=z^i+\\frac{\\lambda}{L}$是$g(x^i)$的最小值。\n\n3. 当$-\\frac{\\lambda}{L} \\leqslant z_i \\leqslant \\frac{\\lambda}{L}$时：\n\n   a. 假设$x^i>0$，则$\\operatorname{sign}(x^i)=1$，那么有$x^i=z^i-\\frac{\\lambda}{L}\\leqslant 0$与假设矛盾；\n\n   b. 假设$x^i<0$，则$\\operatorname{sign}(x^i)=-1$，那么有$x^i=z^i+\\frac{\\lambda}{L}\\geqslant 0$与假设矛盾。\n\n4. 最后讨论$x_i=0$的情况，此时$g(x^i)=\\frac{L}{2}\\left({z^i}\\right)^2$\n\n   a. 当$\\vert z^i\\vert>\\frac{\\lambda}{L}$时，由上述推导可知$g(x_i)$的最小值在$x^i=z^i-\\frac{\\lambda}{L}$处取得，因为\n   $$\n   \\begin{aligned}\n   g(x^i)\\vert_{x^i=0}-g(x^i)\\vert_{x_i=z^i-\\frac{\\lambda}{L}}\n   &=\\frac{L}{2}\\left({z^i}\\right)^2 - \\left(\\lambda z^i-\\frac{\\lambda^2}{2L}\\right)\\\\\n   &=\\frac{L}{2}\\left(z^i-\\frac{\\lambda}{L}\\right)^2\\\\\n   &>0\n   \\end{aligned}\n   $$\n   \n\n   因此当$\\vert z^i\\vert>\\frac{\\lambda}{L}$时，$x_i=0$不会是函数$g(x_i)$的最小值。\n\n   b. 当$-\\frac{\\lambda}{L} \\leqslant z_i \\leqslant \\frac{\\lambda}{L}$时，对于任何$\\Delta x\\neq 0$有\n   $$\n   \\begin{aligned}\n   g(\\Delta x) &=\\frac{L}{2}\\left(\\Delta x-z^{i}\\right)^{2}+\\lambda|\\Delta x| \\\\\n   &=\\frac{L}{2}\\left((\\Delta x)^{2}-2 \\Delta x \\cdot z^{i}+\\frac{2 \\lambda}{L}|\\Delta x|\\right)+\\frac{L}{2}\\left(z^{i}\\right)^{2} \\\\\n   &\\ge\\frac{L}{2}\\left((\\Delta x)^{2}-2 \\Delta x \\cdot z^{i}+\\frac{2 \\lambda}{L}\\Delta x\\right)+\\frac{L}{2}\\left(z^{i}\\right)^{2}\\\\\n   &\\ge\\frac{L}{2}\\left(\\Delta x\\right)^2+\\frac{L}{2}\\left(z^{i}\\right)^{2}\\\\\n   &>g(x^i)\\vert_{x^i=0}\n   \\end{aligned}\n   $$\n   因此$x^i=0$是$g(x^i)$的最小值点。\n\n5. 综上所述，11.14成立\n\n   \n\n## 11.15\n\n$$\n\\min _{\\mathbf{B}, \\boldsymbol{\\alpha}_{i}} \\sum_{i=1}^{m}\\left\\|\\boldsymbol{x}_{i}-\\mathbf{B} \\boldsymbol{\\alpha}_{i}\\right\\|_{2}^{2}+\\lambda \\sum_{i=1}^{m}\\left\\|\\boldsymbol{\\alpha}_{i}\\right\\|_{1}\n$$\n[解析]：这个式子表达的意思很容易理解，即希望样本$x_i$的稀疏表示$\\boldsymbol{\\alpha}_i$通过字典$\\mathbf{B}$重构后和样本$x_i$的原始表示尽量相似，如果满足这个条件，那么稀疏表示$\\boldsymbol{\\alpha}_i$是比较好的。后面的1范数项是为了使表示更加稀疏。\n\n## 11.16\n\n$$\n\\min _{\\boldsymbol{\\alpha}_{i}}\\left\\|\\boldsymbol{x}_{i}-\\mathbf{B} \\boldsymbol{\\alpha}_{i}\\right\\|_{2}^{2}+\\lambda\\left\\|\\boldsymbol{\\alpha}_{i}\\right\\|_{1}\n$$\n\n[解析]：为了优化11.15，我们采用变量交替优化的方式(有点类似EM算法)，首先固定变量$\\mathbf{B}$，则11.15求解的是$m$个样本相加的最小值，因为公式里没有样本之间的交互(即文中所述$\\alpha_{i}^{u} \\alpha_{i}^{v}(u \\neq v)$这样的形式)，因此可以对每个变量做分别的优化求出$\\boldsymbol{\\alpha}_i$，求解方法见11.13，11.14。\n\n## 11.17\n\n$$\n\\min _{\\mathbf{B}}\\|\\mathbf{X}-\\mathbf{B} \\mathbf{A}\\|_{F}^{2}\n$$\n\n[解析]：这是优化11.15的第二步，固定住$\\boldsymbol{\\alpha}_i, i=1, 2,\\dots,m$，此时式11.15的第二项为一个常数，优化11.15即优化$\\min _{\\mathbf{B}} \\sum_{i=1}^{m}\\left\\|\\boldsymbol{x}_{i}-\\mathbf{B} \\boldsymbol{\\alpha}_{i}\\right\\|_{2}^{2}$。其写成矩阵相乘的形式为$\\min _{\\mathbf{B}}\\|\\mathbf{X}-\\mathbf{B} \\mathbf{A}\\|_{2}^{2}$，将2范数扩展到$F$范数即得优化目标为$\\min _{\\mathbf{B}}\\|\\mathbf{X}-\\mathbf{B} \\mathbf{A}\\|_{F}^{2}$。\n\n## 11.18\n\n$$\n\\begin{aligned}\n\\min _{\\mathbf{B}}\\|\\mathbf{X}-\\mathbf{B} \\mathbf{A}\\|_{F}^{2} &=\\min _{\\boldsymbol{b}_{i}}\\left\\|\\mathbf{X}-\\sum_{j=1}^{k} \\boldsymbol{b}_{j} \\boldsymbol{\\alpha}^{j}\\right\\|_{F}^{2} \\\\\n&=\\min _{\\boldsymbol{b}_{i}}\\left\\|\\left(\\mathbf{X}-\\sum_{j \\neq i} \\boldsymbol{b}_{j} \\boldsymbol{\\alpha}^{j}\\right)-\\boldsymbol{b}_{i} \\boldsymbol{\\alpha}^{i}\\right\\| _{F}^{2} \\\\\n&=\\min _{\\boldsymbol{b}_{i}}\\left\\|\\mathbf{E}_{i}-\\boldsymbol{b}_{i} \\boldsymbol{\\alpha}^{i}\\right\\|_{F}^{2}\n\\end{aligned}\n$$\n[解析]：这个公式难点在于推导$\\mathbf{B}\\mathbf{A}=\\sum_{j=1}^k\\boldsymbol{b}_j\\boldsymbol{\\alpha}^j$。大致的思路是$\\boldsymbol{b}_{j} \\boldsymbol{\\alpha}^{j}$会生成和矩阵$\\mathbf{B}\\mathbf{A}$同样维度的矩阵，这个矩阵对应位置的元素是$\\mathbf{B}\\mathbf{A}$中对应位置元素的一个分量，这样的分量矩阵一共有$k$个，把所有分量矩阵加起来就得到了最终结果。推导过程如下：\n$$\n\\begin{aligned}\n\\boldsymbol B\\boldsymbol A\n& =\\begin{bmatrix}\nb_{1}^{1} &b_{2}^{1}  & \\cdot  & \\cdot  & \\cdot  & b_{k}^{1}\\\\ \nb_{1}^{2} &b_{2}^{2}  & \\cdot  & \\cdot  & \\cdot  & b_{k}^{2}\\\\ \n\\cdot  & \\cdot  & \\cdot  &  &  & \\cdot \\\\ \n\\cdot  &  \\cdot &  & \\cdot  &  &\\cdot  \\\\ \n \\cdot & \\cdot  &  &  & \\cdot  & \\cdot \\\\ \n b_{1}^{d}& b_{2}^{d}  & \\cdot  & \\cdot  &\\cdot   &  b_{k}^{d}\n\\end{bmatrix}_{d\\times k}\\cdot \n\\begin{bmatrix}\n\\alpha_{1}^{1} &\\alpha_{2}^{1}  & \\cdot  & \\cdot  & \\cdot  & \\alpha_{m}^{1}\\\\ \n\\alpha_{1}^{2} &\\alpha_{2}^{2}  & \\cdot  & \\cdot  & \\cdot  & \\alpha_{m}^{2}\\\\ \n\\cdot  & \\cdot  & \\cdot  &  &  & \\cdot \\\\ \n\\cdot  &  \\cdot &  & \\cdot  &  &\\cdot  \\\\ \n \\cdot & \\cdot  &  &  & \\cdot  & \\cdot \\\\ \n \\alpha_{1}^{k}& \\alpha_{2}^{k}  & \\cdot  & \\cdot  &\\cdot   &  \\alpha_{m}^{k}\n\\end{bmatrix}_{k\\times m} \\\\\n& =\\begin{bmatrix}\n\\sum_{j=1}^{k}b_{j}^{1}\\alpha _{1}^{j} &\\sum_{j=1}^{k}b_{j}^{1}\\alpha _{2}^{j} & \\cdot  & \\cdot  & \\cdot  & \\sum_{j=1}^{k}b_{j}^{1}\\alpha _{m}^{j}\\\\ \n\\sum_{j=1}^{k}b_{j}^{2}\\alpha _{1}^{j} &\\sum_{j=1}^{k}b_{j}^{2}\\alpha _{2}^{j}  & \\cdot  & \\cdot  & \\cdot  & \\sum_{j=1}^{k}b_{j}^{2}\\alpha _{m}^{j}\\\\ \n\\cdot  & \\cdot  & \\cdot  &  &  & \\cdot \\\\ \n\\cdot  &  \\cdot &  & \\cdot  &  &\\cdot  \\\\ \n \\cdot & \\cdot  &  &  & \\cdot  & \\cdot \\\\ \n\\sum_{j=1}^{k}b_{j}^{d}\\alpha _{1}^{j}& \\sum_{j=1}^{k}b_{j}^{d}\\alpha _{2}^{j}  & \\cdot  & \\cdot  &\\cdot   &  \\sum_{j=1}^{k}b_{j}^{d}\\alpha _{m}^{j}\n\\end{bmatrix}_{d\\times m} &\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n\\boldsymbol b_{\\boldsymbol j}\\boldsymbol \\alpha ^{\\boldsymbol j}\n& =\\begin{bmatrix}\nb_{j}^{1}\\\\ b_{j}^{2}\n\\\\ \\cdot \n\\\\ \\cdot \n\\\\ \\cdot \n\\\\ b_{j}^{d}\n\\end{bmatrix}\\cdot \n\\begin{bmatrix}\n \\alpha _{1}^{j}& \\alpha _{2}^{j} & \\cdot  & \\cdot  & \\cdot  & \\alpha _{m}^{j}\n\\end{bmatrix}\\\\\n& =\\begin{bmatrix}\nb_{j}^{1}\\alpha _{1}^{j} &b_{j}^{1}\\alpha _{2}^{j} & \\cdot  & \\cdot  & \\cdot  & b_{j}^{1}\\alpha _{m}^{j}\\\\ \nb_{j}^{2}\\alpha _{1}^{j} &b_{j}^{2}\\alpha _{2}^{j}  & \\cdot  & \\cdot  & \\cdot  & b_{j}^{2}\\alpha _{m}^{j}\\\\ \n\\cdot  & \\cdot  & \\cdot  &  &  & \\cdot \\\\ \n\\cdot  &  \\cdot &  & \\cdot  &  &\\cdot  \\\\ \n \\cdot & \\cdot  &  &  & \\cdot  & \\cdot \\\\ \nb_{j}^{d}\\alpha _{1}^{j}& b_{j}^{d}\\alpha _{2}^{j}  & \\cdot  & \\cdot  &\\cdot   &  b_{j}^{d}\\alpha _{m}^{j}\n\\end{bmatrix}_{d\\times m} &\n\\end{aligned}\n$$\n\n求和可得：\n$$\n\\begin{aligned}\n\\sum_{j=1}^{k}\\boldsymbol b_{\\boldsymbol j}\\boldsymbol \\alpha ^{\\boldsymbol j} \n& = \\sum_{j=1}^{k}\\left (\\begin{bmatrix}\nb_{j}^{1}\\\\ b_{j}^{2}\n\\\\ \\cdot \n\\\\ \\cdot \n\\\\ \\cdot \n\\\\ b_{j}^{d}\n\\end{bmatrix}`\\cdot \n\\begin{bmatrix}\n \\alpha _{1}^{j}& \\alpha _{2}^{j} & \\cdot  & \\cdot  & \\cdot  & \\alpha _{m}^{j}\n\\end{bmatrix} \\right )\\\\\n& =\\begin{bmatrix}\n\\sum_{j=1}^{k}b_{j}^{1}\\alpha _{1}^{j} &\\sum_{j=1}^{k}b_{j}^{1}\\alpha _{2}^{j} & \\cdot  & \\cdot  & \\cdot  & \\sum_{j=1}^{k}b_{j}^{1}\\alpha _{m}^{j}\\\\ \n\\sum_{j=1}^{k}b_{j}^{2}\\alpha _{1}^{j} &\\sum_{j=1}^{k}b_{j}^{2}\\alpha _{2}^{j}  & \\cdot  & \\cdot  & \\cdot  & \\sum_{j=1}^{k}b_{j}^{2}\\alpha _{m}^{j}\\\\ \n\\cdot  & \\cdot  & \\cdot  &  &  & \\cdot \\\\ \n\\cdot  &  \\cdot &  & \\cdot  &  &\\cdot  \\\\ \n \\cdot & \\cdot  &  &  & \\cdot  & \\cdot \\\\ \n\\sum_{j=1}^{k}b_{j}^{d}\\alpha _{1}^{j}& \\sum_{j=1}^{k}b_{j}^{d}\\alpha _{2}^{j}  & \\cdot  & \\cdot  &\\cdot   &  \\sum_{j=1}^{k}b_{j}^{d}\\alpha _{m}^{j}\n\\end{bmatrix}_{d\\times m} &\n\\end{aligned}\n$$\n得证。\n\n将矩阵$\\mathbf{B}$分解成矩阵列$\\boldsymbol{b}_j,j=1,2,\\dots,k$带来一个好处，即和11.16的原理相同，矩阵列与列之间无关，因此可以分别优化各个列，即将$\\min_\\mathbf{B}\\Vert\\dots\\mathbf{B}\\dots\\Vert^2_F$转化成了$\\min_{b_i}\\Vert\\cdots\\boldsymbol{b}_i\\cdots\\Vert^2_F$，得到第三行的等式之后，再利用文中介绍的KSVD算法求解即可。\n\n','2021-12-10 12:43:49','2021-12-19 13:22:53'),
	(12,1,'chapter12','## 12.1\n\n$$\nE(h ; \\mathcal{D})=P_{\\boldsymbol{x} \\sim \\mathcal{D}}(h(\\boldsymbol{x}) \\neq y)\n$$\n\n[解析]：该式为泛化误差的定义式，所谓泛化误差，是指当样本$x$从真实的样本分布$\\mathcal{D}$中采样后其预测值$h(\\boldsymbol{x})$不等于真实值$y$的概率。在现实世界中，我们很难获得样本分布$\\mathcal{D}$，我们拿到的数据集可以看做是从样本分布$\\mathcal{D}$中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集$D$[也叫观测集、样本集，注意与花体$\\mathcal{D}$的区别]。\n\n\n\n## 12.2\n\n$$\n\\widehat{E}(h ; D)=\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{I}\\left(h\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right)\n$$\n\n[解析]：该式为经验误差的定义式，所谓经验误差，是指观测集$D$中的样本$x_i, i=1,2,\\cdots,m$的预测值$h(\\boldsymbol{x}_i)$和真实值$y_i$的期望误差。\n\n\n\n## 12.3\n\n$$\nd\\left(h_{1}, h_{2}\\right)=P_{\\boldsymbol{x} \\sim \\mathcal{D}}\\left(h_{1}(\\boldsymbol{x}) \\neq h_{2}(\\boldsymbol{x})\\right)\n$$\n\n\n\n[解析]：假设我们有两个模型$h_1$和$h_2$，将它们同时作用于样本$\\boldsymbol{x}$上，那么他们的”不合“度定义为这两个模型预测值不相同的概率。\n\n\n\n## 12.4\n\n$$\nf(\\mathbb{E}(x)) \\leqslant \\mathbb{E}(f(x))\n$$\n\n[解析]：Jensen不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的抛物线，假如我们有两个点$x_1, x_2$，那么$f(\\mathbb{E}(x))$表示的是两个点的均值的纵坐标，而$\\mathbb{E}(f(x))$表示的是两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。\n\n\n\n## 12.5\n\n$$\nP\\left(\\frac{1}{m} \\sum_{i=1}^{m} x_{i}-\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right) \\geqslant \\epsilon\\right) \\leqslant \\exp \\left(-2 m \\epsilon^{2}\\right)\n$$\n\n[解析]：Hoeffding 不等式：对于独立随机变量$x_1, x_2, \\cdots, x_m$来说，他们观测值$x_i$的均值$\\frac{1}{m} \\sum_{i=1}^{m} x_{i}$总是和他们期望$\\mathbb{E}(x_i)$的均值$\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right)$相近，上式从概率的角度对这样一个结论进行了描述：即**它们之间差值不小于$\\epsilon$**这样的事件出现的概率不大于$\\exp \\left(-2 m \\epsilon^{2}\\right)$，可以看出当观测到的变量越多，观测值的均值越逼近期望的均值。\n\n\n\n## 12.6\n\n$$\nP\\left(\\left\\vert\\frac{1}{m} \\sum_{i=1}^{m} x_{i}-\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right)\\right\\vert \\geqslant \\epsilon\\right) \\leqslant 2 \\exp \\left(-2 m \\epsilon^{2}\\right)\n$$\n\n[解析]：略\n\n\n\n## 12.7\n\n$$\nP\\left(f\\left(x_{1}, \\ldots, x_{m}\\right)-\\mathbb{E}\\left(f\\left(x_{1}, \\ldots, x_{m}\\right)\\right) \\geqslant \\epsilon\\right) \\leqslant \\exp \\left(\\frac{-2 \\epsilon^{2}}{\\sum_{i} c_{i}^{2}}\\right)\n$$\n\n[解析]：McDiarmid不等式：首先解释下前提条件：$$\n\\sup _{x_{1}, \\ldots, x_{m}, x_{i}^{\\prime}}\\left|f\\left(x_{1}, \\ldots, x_{m}\\right)-f\\left(x_{1}, \\ldots, x_{i-1}, x_{i}^{\\prime}, x_{i+1}, \\ldots, x_{m}\\right)\\right| \\leqslant c_{i}\n$$ 表示当函数$f$某个输入$x_i$变到$x_i^\\prime$的时候，其变化的上确$\\sup$仍满足不大于$c_i$。所谓上确界sup可以理解成变化的极限最大值，可能取到也可能无穷逼近。当满足这个条件时，McDiarmid不等式指出：函数值$f(x_1,\\dots,x_m)$和其期望值$\\mathbb{E}\\left(f(x_1,\\dots,x_m)\\right)$也相近，从概率的角度描述是：它们之间差值不小于$\\epsilon$这样的事件出现的概率不大于$\n\\exp \\left(\\frac{-2 \\epsilon^{2}}{\\sum_{i} c_{i}^{2}}\\right)\n$，可以看出当每次变量改动带来函数值改动的上限越小，函数值和其期望越相近。\n\n## 12.8\n\n$$\nP\\left(\\left|f\\left(x_{1}, \\ldots, x_{m}\\right)-\\mathbb{E}\\left(f\\left(x_{1}, \\ldots, x_{m}\\right)\\right)\\right| \\geqslant \\epsilon\\right) \\leqslant 2 \\exp \\left(\\frac{-2 \\epsilon^{2}}{\\sum_{i} c_{i}^{2}}\\right)\n$$\n\n[解析]：略\n\n\n\n## 12.9\n\n$$\nP(E(h)\\le\\epsilon)\\ge 1-\\delta\n$$\n\n[解析]：PAC辨识的定义：$E(h)$表示算法$\\mathcal{L}$在用观测集$D$训练后输出的假设函数$h$，它的泛化误差(见公式12.1)。这个概率定义指出，如果$h$的泛化误差不大于$\\epsilon$的概率不小于$1-\\delta$，那么我们称学习算法$\\mathcal{L}$能从假设空间$\\mathcal{H}$中PAC辨识概念类$\\mathcal{C}$。\n\n\n\n**从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念$c$的有效近似。只要训练集$D$的规模能使学习算法$\\mathcal{L}$以概率$1-\\delta$找到目标假设的$\\epsilon$近似即可。下面就是用数学公式进行抽象**\n\n## 12.10\n\n$$\n\\begin{aligned} P(h(\\boldsymbol{x})=y) &=1-P(h(\\boldsymbol{x}) \\neq y) \\\\ &=1-E(h) \\\\ &<1-\\epsilon \\end{aligned}\n$$\n\n[解析]：$P(h(\\boldsymbol{x})=y) =1-P(h(\\boldsymbol{x}) \\neq y)$ 因为它们是对立事件，$P(h(x)\\neq y)=E(h)$是泛化误差的定义(见12.1)，由于我们假定了泛化误差$E(h)>\\epsilon$，因此有$1-E(h)<1-\\epsilon$。\n\n\n\n## 12.11\n\n$$\n\\begin{aligned} P\\left(\\left(h\\left(\\boldsymbol{x}_{1}\\right)=y_{1}\\right) \\wedge \\ldots \\wedge\\left(h\\left(\\boldsymbol{x}_{m}\\right)=y_{m}\\right)\\right) &=(1-P(h(\\boldsymbol{x}) \\neq y))^{m} \\\\ &<(1-\\epsilon)^{m} \\end{aligned}\n$$\n\n\n\n[解析]：先解释什么是$h$与$D$“表现一致”，12.2节开头阐述了这样的概念，如果$h$能将$D$中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即$\\left(h\\left(\\boldsymbol{x}_{1}\\right)=y_{1}\\right) \\wedge \\ldots \\wedge\\left(h\\left(\\boldsymbol{x}_{m}\\right)=y_{m}\\right)$为True。因为每个事件是独立的，所以上式可以写成$P\\left(\\left(h\\left(\\boldsymbol{x}_{1}\\right)=y_{1}\\right) \\wedge \\ldots \\wedge\\left(h\\left(\\boldsymbol{x}_{m}\\right)=y_{m}\\right)\\right)=\\prod_{i=1}^{m} P\\left(h\\left(\\boldsymbol{x}_{i}\\right)=y_{i}\\right)$。根据对立事件的定义有：$\\prod_{i=1}^{m} P\\left(h\\left(\\boldsymbol{x}_{i}\\right)=y_{i}\\right)=\\prod_{i=1}^{m}\\left(1-P\\left(h\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right)\\right)$，又根据公式(12.10)，有$$\\prod_{i=1}^{m}\\left(1-P\\left(h\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right)\\right)<\\prod_{i=1}^{m}(1-\\epsilon)=(1-\\epsilon)^{m}$$\n\n\n\n## 12.12\n\n$$\n\\begin{aligned} P(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0) &<|\\mathcal{H}|(1-\\epsilon)^{m} \\\\ &<|\\mathcal{H}| e^{-m \\epsilon} \\end{aligned}\n$$\n\n[解析]：首先解释为什么”我们事先并不知道学习算法$\\mathcal{L}$会输出$\\mathcal{H}$中的哪个假设“，因为一些学习算法对用一个观察集$D$的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设$h$参数的值。泛化误差大于$\\epsilon$且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为$P(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0)$，根据式12.11，每一个这样的假设$h$都满足$P(E(h)>\\epsilon \\wedge \\widehat{E}(h)=0)<\\left(1-\\epsilon \\right)^m$，假设一共有$\\vert\\mathcal{H}\\vert$这么多个这样的假设$h$，因为每个假设$h$满足$E(h)>\\epsilon$且$\\widehat{E}(h)=0$是互斥的，因此总的概率$P(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0)$就是这些互斥事件之和，即\n$$\n\\begin{aligned}P\\left(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0\\right) &=\\sum_i^{\\mathcal{\\vert H\\vert}}P\\left(E(h_i)>\\epsilon \\wedge \\widehat{E}(h_i)=0\\right)\\\\&<|\\mathcal{H}|(1-\\epsilon)^{m}\\end{aligned}\n$$\n小于号依据公式(12.11)。\n\n第二个小于号实际上是要证明$\\vert\\mathcal{H}\\vert(1-\\epsilon)^m < \\vert\\mathcal{H}\\vert e^{-m\\epsilon}$，即证明$(1-\\epsilon)^m < e^{-m\\epsilon}$，其中$\\epsilon\\in(0,1]$，$m$是正整数，推导如下：\n\n[推导]：当$\\epsilon=1$时，显然成立，当$\\epsilon\\in(0, 1)$时，因为左式和右式的值域均大于0，所以可以左右两边同时取对数，又因为对数函数是单调递增函数，所以即证明$m\\ln(1-\\epsilon) < -m\\epsilon$，即证明$\\ln(1-\\epsilon)<-\\epsilon$，这个式子很容易证明：令$f(\\epsilon)=\\ln(1-\\epsilon) + \\epsilon$，其中$\\epsilon\\in(0,1)$，$f^\\prime(\\epsilon)=1-\\frac{1}{1-\\epsilon}=0 \\Rightarrow \\epsilon=0$ 取极大值0，因此$ln(1-\\epsilon)<-\\epsilon$ 也即$\\vert\\mathcal{H}\\vert(1-\\epsilon)^m < \\vert\\mathcal{H}\\vert e^{-m\\epsilon}$成立。\n\n\n\n## 12.13\n\n$$\n|\\mathcal{H}| e^{-m \\epsilon} \\leqslant \\delta\n$$\n\n[解析]：回到我们要回答的问题：到底需要多少样例才能学得目标概念$c$的有效近似。只要训练集$D$的规模能使学习算法$\\mathcal{L}$以概率$1-\\delta$找到目标假设的$\\epsilon$近似即可。根据式12.12，学习算法$\\mathcal{L}$生成的假设大于目标假设的$\\epsilon$近似的概率为$P\\left(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0\\right)<\\vert\\mathcal{H}\\vert e^{-m\\epsilon}$，因此学习算法$\\mathcal{L}$生成的假设落在目标假设的$\\epsilon$近似的概率为$1-P\\left(h \\in \\mathcal{H}: E(h)>\\epsilon \\wedge \\widehat{E}(h)=0\\right)\\ge 1-\\vert\\mathcal{H}\\vert e^{-m\\epsilon}$，这个概率我们希望至少是$1-\\delta$，因此$1-\\delta\\leqslant 1-\\vert\\mathcal{H}\\vert e^{-m\\epsilon}\\Rightarrow\\vert\\mathcal{H}\\vert e^{-m\\epsilon}\\leqslant\\delta$\n\n## 12.14\n\n$$\nm \\geqslant \\frac{1}{\\epsilon}\\left(\\ln |\\mathcal{H}|+\\ln \\frac{1}{\\delta}\\right)\n$$\n\n[推导]：\n$$\n\\begin{aligned}\n\\vert\\mathcal{H}\\vert e^{-m \\epsilon} &\\leqslant \\delta\\\\\ne^{-m \\epsilon} &\\leqslant \\frac{\\delta}{\\vert\\mathcal{H}\\vert}\\\\\n-m \\epsilon &\\leqslant \\ln\\delta-\\ln\\vert\\mathcal{H}\\vert\\\\\nm &\\geqslant \\frac{1}{\\epsilon}\\left(\\ln |\\mathcal{H}|+\\ln \\frac{1}{\\delta}\\right)\n\\end{aligned}\n$$\n[解析]：这个式子告诉我们，在假设空间$\\mathcal{H}$是PAC可学习的情况下，输出假设$h$的泛化误差$\\epsilon$随样本数目$m$增大而收敛到0，收敛速率为$O(\\frac{1}{m})$。这也是我们在机器学习中的一个共识，即可供模型训练的观测集样本数量越多，机器学习模型的泛化性能越好。\n\n\n\n## 12.15\n\n$$\nP(\\widehat{E}(h)-E(h) \\geqslant \\epsilon) \\leqslant \\exp \\left(-2 m \\epsilon^{2}\\right)\n$$\n\n\n\n[解析]：参见12.5\n\n\n\n## 12.16\n\n$$\nP(E(h)-\\widehat{E}(h) \\geqslant \\epsilon) \\leqslant \\exp \\left(-2 m \\epsilon^{2}\\right)\n$$\n\n\n\n[解析]：参见12.5\n\n\n\n## 12.17\n\n$$\nP(|E(h)-\\widehat{E}(h)| \\geqslant \\epsilon) \\leqslant 2 \\exp \\left(-2 m \\epsilon^{2}\\right)\n$$\n\n\n\n[解析]：参见12.6\n\n\n\n## 12.18\n\n$$\n\\widehat{E}(h)-\\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}} \\leqslant E(h) \\leqslant \\widehat{E}(h)+\\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}\n$$\n\n[推导]：令$\\delta=2e^{-2m\\epsilon^2}$，则$\\epsilon=\\sqrt{\\frac{\\ln(2/\\delta)}{2m}}$，由式12.17\n$$\n\\begin{aligned}\nP(|E(h)-\\widehat{E}(h)| \\geqslant \\epsilon) &\\leqslant 2 \\exp \\left(-2 m \\epsilon^{2}\\right)\\\\\nP(|E(h)-\\widehat{E}(h)| \\geqslant \\epsilon) &\\leqslant \\delta\\\\\nP(|E(h)-\\widehat{E}(h)| \\leqslant \\epsilon) &\\geqslant 1 - \\delta\\\\\nP(-\\epsilon \\leqslant E(h)-\\widehat{E}(h) \\leqslant \\epsilon) &\\geqslant 1 - \\delta\\\\\nP(\\widehat{E}(h) -\\epsilon \\leqslant E(h) \\leqslant \\widehat{E}(h)+\\epsilon) &\\geqslant 1 - \\delta\\\\\n\\end{aligned}\n$$\n代入 $\\epsilon=\\sqrt{\\frac{\\ln(2/\\delta)}{2m}}$得证。\n\n这个式子进一步阐明了当观测集样本数量足够大的时候，$h$的经验误差是其泛化误差很好的近似。\n\n\n\n## 12.19\n\n$$\nP\\left(|E(h)-\\widehat{E}(h)| \\leqslant \\sqrt{\\frac{\\ln |\\mathcal{H}|+\\ln (2 / \\delta)}{2 m}}\\right) \\geqslant 1-\\delta\n$$\n\n[推导]：\n\n令$h_1,h_2,\\dots,h_{\\vert\\mathcal{H}\\vert}$表示假设空间$\\mathcal{H}$中的假设，有\n$$\n\\begin{aligned} \n& P(\\exists h \\in \\mathcal{H}:|E(h)-\\widehat{E}(h)|>\\epsilon) \\\\\n=& P\\left(\\left(\\left|E_{h_{1}}-\\widehat{E}_{h_{1}}\\right|>\\epsilon\\right) \\vee \\ldots \\vee\\left(| E_{h_{|\\mathcal{H}|}}-\\widehat{E}_{h_{|\\mathcal{H}|} |>\\epsilon}\\right)\\right) \\\\ \\leqslant & \\sum_{h \\in \\mathcal{H}} P(|E(h)-\\widehat{E}(h)|>\\epsilon) \n\\end{aligned}\n$$\n这一步是很好理解的，存在一个假设$h$使得$|E(h)-\\widehat{E}(h)|>\\epsilon$的概率可以表示为对假设空间内所有的假设$h_i, i\\in 1,\\dots,\\vert\\mathcal{H}\\vert$，使得$\\left|E_{h_{i}}-\\widehat{E}_{h_{i}}\\right|>\\epsilon$这个事件成立的\"或\"事件。因为$P(A\\vee B)=P(A) + P(B) - P(A\\wedge B)$，而$P(A\\wedge B)\\geqslant 0$，所以最后一行的不等式成立。\n\n由式12.17：\n$$\n\\begin{aligned}\n&P(|E(h)-\\widehat{E}(h)| \\geqslant \\epsilon) \\leqslant 2 \\exp \\left(-2 m \\epsilon^{2}\\right)\\\\\n&\\Rightarrow \\sum_{h \\in \\mathcal{H}} P(|E(h)-\\widehat{E}(h)|>\\epsilon) \\leqslant 2|\\mathcal{H}| \\exp \\left(-2 m \\epsilon^{2}\\right)\n\\end{aligned}\n$$\n因此：\n$$\n\\begin{aligned}\nP(\\exists h \\in \\mathcal{H}:|E(h)-\\widehat{E}(h)|>\\epsilon) \n&\\leqslant  \\sum_{h \\in \\mathcal{H}} P(|E(h)-\\widehat{E}(h)|>\\epsilon)\\\\\n&\\leqslant 2|\\mathcal{H}| \\exp \\left(-2 m \\epsilon^{2}\\right)\n\\end{aligned}\n$$\n其对立事件：\n$$\n\\begin{aligned}\nP(\\forall h\\in\\mathcal{H}:\\vert E(h)-\\widehat{E}(h)\\vert\\leqslant\\epsilon)&=1-P(\\exists h \\in \\mathcal{H}:|E(h)-\\widehat{E}(h)|>\\epsilon)\\\\ &\\geqslant 1- 2|\\mathcal{H}| \\exp \\left(-2 m \\epsilon^{2}\\right)\n\\end{aligned}\n$$\n令$\\delta=2\\vert\\mathcal{H}\\vert e^{-2m\\epsilon^2}$，则$\\epsilon=\\sqrt{\\frac{\\ln |\\mathcal{H}|+\\ln (2 / \\delta)}{2 m}}$，代入上式中即可得到\n$$\nP\\left(\\forall h\\in\\mathcal{H}:\\vert E(h)-\\widehat{E}(h)\\vert\\leqslant\\sqrt{\\frac{\\ln |\\mathcal{H}|+\\ln (2 / \\delta)}{2 m}}\\right)\\geqslant 1- \\delta\n$$\n其中$\\forall h\\in\\mathcal{H}$这个前置条件可以省略。\n\n\n\n## 12.20\n\n$$\nP\\left(E(h)-\\min _{h^{\\prime} \\in \\mathcal{H}} E\\left(h^{\\prime}\\right) \\leqslant \\epsilon\\right) \\geqslant 1-\\delta\n$$\n\n[解析]：这个式子是”不可知PAC可学习“的定义式，不可知是指当目标概念$c$不在算法$\\mathcal{L}$所能生成的假设空间$\\mathcal{H}$里。可学习是指如果$\\mathcal{H}$中泛化误差最小的假设是$\\arg\\min_{h\\in \\mathcal{H}}E(h)$，且这个假设的泛化误差满足其与目标概念的泛化误差的差值不大于$\\epsilon$的概率不小于$1-\\delta$。我们称这样的假设空间$\\mathcal{H}$是不可知PAC可学习的。\n\n\n\n## 12.21\n\n$$\n\\Pi_{\\mathcal{H}}(m)=\\max _{\\left\\{\\boldsymbol{x}_{1}, \\ldots, \\boldsymbol{x}_{m}\\right\\} \\subseteq \\mathcal{X}}\\left|\\left\\{\\left(h\\left(\\boldsymbol{x}_{1}\\right), \\ldots, h\\left(\\boldsymbol{x}_{m}\\right)\\right) | h \\in \\mathcal{H}\\right\\}\\right|\n$$\n\n[解析]：这个是增长函数的定义式。增长函数$\\Pi_{\\mathcal{H}}(m)$表示假设空间$\\mathcal{H}$对m个样本所能赋予标签的最大可能的结果数。比如对于两个样本的二分类问题，一共有4中可能的标签组合$[[0, 0], [0, 1], [1, 0], [1, 1]]$，如果假设空间$\\mathcal{H}_1$能赋予这两个样本两种标签组合$[[0, 0], [1, 1]]$，则$\\Pi_{\\mathcal{H}_1}(2)=2$。显然，$\\mathcal{H}$对样本所能赋予标签的可能结果数越多，$\\mathcal{H}$的表示能力就越强。增长函数可以用来反映假设空间$\\mathcal{H}$的复杂度。\n\n\n\n## 12.22\n\n$$\nP(|E(h)-\\widehat{E}(h)|>\\epsilon) \\leqslant 4 \\Pi_{\\mathcal{H}}(2 m) \\exp \\left(-\\frac{m \\epsilon^{2}}{8}\\right)\n$$\n\n[解析]：这个式子的前提假设有误，应当写成对假设空间$\\mathcal{H}$，$m\\in\\mathbb{N}$，$0<\\epsilon<1$，**存在**$h\\in\\mathcal{H}$\n\n详细证明参见原论文 https://courses.engr.illinois.edu/ece544na/fa2014/vapnik71.pdf\n\n\n\n## 12.23\n\n$$\n\\mathrm{VC}(\\mathcal{H})=\\max \\left\\{m: \\Pi_{\\mathcal{H}}(m)=2^{m}\\right\\}\n$$\n\n[解析]：这是VC维的定义式：VC维的定义是能被$\\mathcal{H}$打散的最大示例集的大小。西瓜书中例12.1和例12.2 给出了形象的例子。注意，VC维的定义式上的底数2表示这个问题是2分类的问题。如果是$n$分类的问题，那么定义式中底数需要变为$n$。\n\n\n\n## 12.24\n\n$$\n\\Pi_{\\mathcal{H}}(m) \\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\n$$\n[解析]：首先解释下数学归纳法的起始条件\"当$m=1, d=0$或$d=1$时，定理成立\"，当$m=1,d=0$时，由VC维的定义(式12.23) $\\mathrm{VC}(\\mathcal{H})=\\max \\left\\{m: \\Pi_{\\mathcal{H}}(m)=2^{m}\\right\\}=0$ 可知$\\Pi_{\\mathcal{H}}(1)<2$，否则$d$可以取到1，又因为$\\Pi_{\\mathcal{H}}(m)$为整数，所以$\\Pi_{\\mathcal{H}}(1)\\in[0, 1]$，式12.24右边为$\\sum_{i=0}^{0}\\left(\\begin{array}{c}{1} \\\\ {i}\\end{array}\\right)=1$，因此不等式成立。当$m=1,d=1$时，因为一个样本最多只能有两个类别，所以$\\Pi_\\mathcal{H}(1)=2$，不等式右边为$\\sum_{i=0}^{1}\\left(\\begin{array}{c}{1} \\\\ {i}\\end{array}\\right)=2$，因此不等式成立。\n\n再介绍归纳过程，这里采样的归纳方法是假设式12.24对$(m-1, d-1)$和$(m-1, d)$成立，推导出其对$(m,d)$也成立。证明过程中引入观测集$D=\\left\\{\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\ldots, \\boldsymbol{x}_{m}\\right\\}$ 和观测集$D^\\prime=\\left\\{\\boldsymbol{x}_{1}, \\boldsymbol{x}_{2}, \\ldots, \\boldsymbol{x}_{m-1}\\right\\}$，其中$D$比$D^\\prime$多一个样本$x_m$，它们对应的假设空间可以表示为：\n$$\n\\begin{array}{l}{\\mathcal{H}_{| D}=\\left\\{\\left(h\\left(\\boldsymbol{x}_{1}\\right), h\\left(\\boldsymbol{x}_{2}\\right), \\ldots, h\\left(\\boldsymbol{x}_{m}\\right)\\right) | h \\in \\mathcal{H}\\right\\}} \\\\ {\\mathcal{H}_{| D^{\\prime}}=\\left\\{\\left(h\\left(\\boldsymbol{x}_{1}\\right), h\\left(\\boldsymbol{x}_{2}\\right), \\ldots, h\\left(\\boldsymbol{x}_{m-1}\\right)\\right) | h \\in \\mathcal{H}\\right\\}}\\end{array}\n$$\n如果假设$h\\in\\mathcal{H}$对$x_m$的分类结果为$+1$，或为$-1$，那么任何出现在$\\mathcal{H}_{\\vert D^\\prime}$中的串都会在$\\mathcal{H}_{\\vert D}$中出现一次或者两次。这里举个例子就很容易理解了，假设$m=3$：\n$$\n\\begin{aligned}\n\\mathcal{H}_{\\vert D}&=\\{(+,-,-),(+,+,-),(+,+,+),(-,+,-),(-,-,+)\\}\\\\\n\\mathcal{H}_{\\vert D^\\prime}&=\\{(+,+),(+,-),(-,+),(-,-)\\}\\\\\n\\end{aligned}\n$$\n其中串$(+,+)$在$\\mathcal{H}_{\\vert D}$中出现了两次$(+, +, +), (+, +, -)$，$\\mathcal{H}_{\\vert D^\\prime}$中得其他串$(+,-), (-, +), (-, -)$均只在$\\mathcal{H}_{\\vert D}$中出现了一次。这里的原因是每个样本是二分类的，所以多出的样本$x_m$要么取$+$，要么取$-$，要么都取到(至少两个假设$h$对$x_m$做出了不一致的判断)。\n\n记号$\\mathcal{H}_{D^\\prime\\vert D}$表示在$\\mathcal{H}_{\\vert D}$中出现了两次的$\\mathcal{H}_{\\vert D^\\prime}$组成的集合，比如在上例中$\\mathcal{H}_{D^\\prime\\vert D}=\\{(+,+)\\}$，有\n$$\n\\left|\\mathcal{H}_{| D}\\right|=\\left|\\mathcal{H}_{| D^{\\prime}}\\right|+\\left|\\mathcal{H}_{D^{\\prime} | D}\\right|\n$$\n由于$\\mathcal{H}_{\\vert D^\\prime}$表示限制在样本集$D^\\prime$上的假设空间$\\mathcal{H}$的表达能力(即所有假设对样本集$D^\\prime$所能赋予的标记种类数)，样本集$D^\\prime$的大小为$m-1$，根据增长函数的定义，假设空间$\\mathcal{H}$对包含$m-1$个样本的集合所能赋予的最大标记种类数为$\\Pi_{\\mathcal{H}}(m-1)$，因此$\\vert\\mathcal{H}_{\\vert D^\\prime}\\vert \\leqslant \\Pi_\\mathcal{H}(m-1)$。又根据数学归纳法的前提假设，有：\n$$\n\\left|\\mathcal{H}_{| D^{\\prime}}\\right| \\leqslant \\Pi_{\\mathcal{H}}(m-1) \\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right)\n$$\n由记号$\\mathcal{H}_{\\vert D^\\prime}$的定义可知，$\\vert\\mathcal{H}_{\\vert D^\\prime}\\vert \\geqslant \\left\\lfloor\\frac{\\vert\\mathcal{H}_{\\vert D}\\vert}{2}\\right\\rfloor$，又由于$\\vert\\mathcal{H}_{\\vert D^\\prime}\\vert$和$\\vert\\mathcal{H}_{D^\\prime\\vert D}\\vert$均为整数，因此$\\vert\\mathcal{H}_{D^\\prime\\vert D}\\vert \\leqslant \\left\\lfloor\\frac{\\vert\\mathcal{H}_{\\vert D}\\vert}{2}\\right\\rfloor$，由于样本集$D$的大小为$m$，根据增长函数的概念，有$\\left|\\mathcal{H}_{D^{\\prime}| D}\\right| \\leqslant \\left\\lfloor\\frac{\\vert\\mathcal{H}_{\\vert D}\\vert}{2}\\right\\rfloor\\leqslant \\Pi_{\\mathcal{H}}(m-1)$。\n\n假设$Q$表示能被$\\mathcal{H}_{D^\\prime\\vert D}$打散的集合，因为根据$\\mathcal{H}_{D^\\prime\\vert D}$的定义，$H_{D}$必对元素$x_m$给定了不一致的判定，因此$Q \\cup\\left\\{\\boldsymbol{x}_{m}\\right\\}$必能被$\\mathcal{H}_{\\vert D}$打散，由前提假设$\\mathcal{H}$的VC维为$d$，因此$\\mathcal{H}_{D^\\prime\\vert D}$的VC维最大为$d-1$，综上有\n$$\n\\left|\\mathcal{H}_{D^{\\prime}| D}\\right| \\leqslant \\Pi_{\\mathcal{H}}(m-1) \\leqslant \\sum_{i=0}^{d-1}\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right)\n$$\n因此：\n$$\n\\begin{aligned}\n\\left|\\mathcal{H}_{| D}\\right|&=\\left|\\mathcal{H}_{| D^{\\prime}}\\right|+\\left|\\mathcal{H}_{D^{\\prime} | D}\\right|\\\\\n&\\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right) + \\sum_{i=0}^{d-1}\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right)\\\\\n&=\\sum_{i=0}^d \\left(\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right) + \\left(\\begin{array}{c}{m-1} \\\\ {i-1}\\end{array}\\right)\\right)\\\\\n&=\\sum_{i=0}^{d}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\n\\end{aligned}\n$$\n注：最后一步依据组合公式，推导如下：\n$$\n\\begin{aligned}\\left(\\begin{array}{c}{m-1} \\\\ {i}\\end{array}\\right)+\\left(\\begin{array}{c}{m-1} \\\\ {i-1}\\end{array}\\right) &=\\frac{(m-1) !}{(m-1-i) ! i !}+\\frac{(m-1) !}{(m-1-i+1) !(i-1) !} \\\\ &=\\frac{(m-1) !(m-i)}{(m-i)(m-1-i) ! i !}+\\frac{(m-1) ! i}{(m-i) !(i-1) ! i} \\\\ &=\\frac{(m-1) !(m-i)+(m-1) ! i}{(m-i) ! i !} \\\\ &=\\frac{(m-1) !(m-i+i)}{(m-i) ! i !}=\\frac{(m-1) ! m}{(m-i) ! i !} \\\\ &=\\frac{m !}{(m-i) ! i !}=\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) \\end{aligned}\n$$\n\n## 12.25\n\n$$\n\\left|\\mathcal{H}_{| D}\\right|=\\left|\\mathcal{H}_{| D^{\\prime}}\\right|+\\left|\\mathcal{H}_{D^{\\prime} | D}\\right|\n$$\n\n\n\n[解析]：参见12.24\n\n\n\n## 12.26\n\n$$\n\\left|\\mathcal{H}_{| D^{\\prime}}\\right| \\leqslant \\Pi_{\\mathcal{H}}(m-1) \\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}\nm-1 \\\\\ni\n\\end{array}\\right)\n$$\n\n\n\n[解析]：参见12.24\n\n\n\n## 12.27\n\n$$\n\\left|\\mathcal{H}_{D^{\\prime} | D}\\right| \\leqslant \\Pi_{\\mathcal{H}}(m-1) \\leqslant \\sum_{i=0}^{d-1}\\left(\\begin{array}{c}\nm-1 \\\\\ni\n\\end{array}\\right)\n$$\n\n\n\n[解析]：参见12.24\n\n\n\n## 12.28\n\n$$\n\\Pi_{\\mathcal{H}}(m) \\leqslant\\left(\\frac{e \\cdot m}{d}\\right)^{d}\n$$\n[推导]：\n$$\n\\begin{aligned} \\Pi_{\\mathcal{H}}(m) & \\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right) \\\\ & \\leqslant \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\\left(\\frac{m}{d}\\right)^{d-i} \\\\ &=\\left(\\frac{m}{d}\\right)^{d} \\sum_{i=0}^{d}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\\left(\\frac{d}{m}\\right)^{i} \\\\ & \\leqslant\\left(\\frac{m}{d}\\right)^{d} \\sum_{i=0}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\\left(\\frac{d}{m}\\right)^{i} \\\\ \n&={\\left(\\frac{m}{d}\\right)}^d{\\left(1+\\frac{d}{m}\\right)}^m\\\\\n&<\\left(\\frac{e \\cdot m}{d}\\right)^{d} \\end{aligned}\n$$\n第一步到第二步和第三步到第四步均因为$m\\geqslant d$，第四步到第五步是由于[二项式定理](https://zh.wikipedia.org/wiki/二项式定理)：$(x+y)^{n}=\\sum_{k=0}^{n}\\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right) x^{n-k} y^{k}$，其中令$k=i, n=m, x=1, y = \\frac{d}{m}$得$\\left(\\frac{m}{d}\\right)^{d} \\sum_{i=0}^{m}\\left(\\begin{array}{c}{m} \\\\ {i}\\end{array}\\right)\\left(\\frac{d}{m}\\right)^{i}=\\left(\\frac{m}{d}\\right)^{d} (1+\\frac{d}{m})^m$，最后一步的不等式即需证明${\\left(1+\\frac{d}{m}\\right)}^m\\leqslant e^d$，因为${\\left(1+\\frac{d}{m}\\right)}^m={\\left(1+\\frac{d}{m}\\right)}^{\\frac{m}{d}d}$，根据[自然对数底数$e$的定义](https://en.wikipedia.org/wiki/E_(mathematical_constant))，${\\left(1+\\frac{d}{m}\\right)}^{\\frac{m}{d}d}< e^d$，注意原文中用的是$\\leqslant$，但是由于$e=\\lim _{\\frac{d}{m} \\rightarrow 0}\\left(1+\\frac{d}{m}\\right)^{\\frac{m}{d}}$的定义是一个极限，所以应该是用$<$。\n\n\n\n## 12.29\n\n$$\nP\\left(\nE(h)-\\widehat{E}(h) \\leqslant \\sqrt{\n\\frac{8d\\ln\\frac{2em}{d}+8\\ln\\frac{4}{\\delta}}{m}\n}\n\\right)\n\\geqslant 1-\\delta\n$$\n\n\n[推导]：这里应该是作者的笔误，根据式12.22，$E(h)-\\widehat{E}(h)$应当被绝对值符号包裹。将式12.28代入式12.22得\n$$\nP\\left(\\vert \nE(h)-\\widehat{E}(h) \\vert> \\epsilon\n\\right) \n\\leqslant 4{\\left(\\frac{2em}{d}\\right)}^d\\exp\\left(-\\frac{m\\epsilon^2}{8}\\right)\n$$\n令$4{\\left(\\frac{2em}{d}\\right)}^d\\exp\\left(-\\frac{m\\epsilon^2}{8}\\right)=\\delta$可解得\n$$\n\\delta=\\sqrt{\n\\frac{8d\\ln\\frac{2em}{d}+8\\ln\\frac{4}{\\delta}}{m}\n}\n$$\n代入式12.22，则定理得证。这个式子是用VC维表示泛化界，可以看出，泛化误差界只与样本数量$m$有关，收敛速率为$\\sqrt{\\frac{\\ln m}{m}}$ (书上简化为$\\frac{1}{\\sqrt{m}}$)。\n\n\n\n## 12.30\n\n$$\n\\widehat{E}(h)=\\min _{h^{\\prime} \\in \\mathcal{H}} \\widehat{E}\\left(h^{\\prime}\\right)\n$$\n\n[解析]：这个是经验风险最小化的定义式。即从假设空间中找出能使经验风险最小的假设。\n\n\n\n## 12.31\n\n$$\nE(g)=\\min _{h \\in \\mathcal{H}} E(h)\n$$\n\n\n\n[解析]：首先回忆PAC可学习的概念，见定义12.2，而可知/不可知PAC可学习之间的区别仅仅在于概念类$c$是否包含于假设空间$\\mathcal{H}$中。令\n$$\n\\begin{aligned}\n\\delta^\\prime = \\frac{\\delta}{2} \\\\\n\\sqrt{\\frac{\\left(\\ln 2 / \\delta^{\\prime}\\right)}{2 m}}=\\frac{\\epsilon}{2}\n\\end{aligned}\n$$\n\n结合这两个标记的转换，由推论12.1可知：\n$$\n\\widehat{E}(g)-\\frac{\\epsilon}{2} \\leqslant E(g) \\leqslant \\widehat{E}(g)+\\frac{\\epsilon}{2}\n$$\n至少以$1-\\delta/2$的概率成立。写成概率的形式即：\n$$\nP\\left(|E(g)-\\widehat{E}(g)| \\leqslant \\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2\n$$\n即$P\\left(\\left(E(g)-\\widehat{E}(g) \\leqslant \\frac{\\epsilon}{2}\\right) \\wedge\\left(E(g)-\\widehat{E}(g) \\geqslant-\\frac{\\epsilon}{2}\\right)\\right) \\geqslant 1-\\delta / 2$，因此$P\\left(E(g)-\\widehat{E}(g) \\leqslant \\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$且$P\\left(E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$成立。\n\n再令\n$$\n\\sqrt{\\frac{8 d \\ln \\frac{2 e m}{d}+8 \\ln \\frac{4}{\\delta^{\\prime}}}{m}}=\\frac{\\epsilon}{2}\n$$\n由式12.29可知\n$$\nP\\left(\\left\\vert \nE(h)-\\widehat{E}(h) \\right\\vert\\leqslant \\frac{\\epsilon}{2}\n\n\\right)\n\\geqslant 1-\\frac{\\delta}{2}\n$$\n同理，$P\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$且$P\\left(E(h)-\\widehat{E}(h) \\geqslant -\\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$成立。\n\n由$P\\left(E(g)-\\widehat{E}(g) \\geqslant - \\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$和$P\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\\right) \\geqslant 1-\\delta / 2$均成立可知\n\n则事件$E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2}$和事件$E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}$同时成立的概率为：\n$$\n\\begin{aligned}\n&P\\left(\n\\left(E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2} \\right)\\wedge\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\n\\right)\\right) \n\\\\= &\nP\\left(E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2}\\right) +\nP\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\\right)\n- P\\left(\\left(E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2} \\right)\\vee\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\n\\right)\\right) \n\\\\\\geqslant &1 - \\delta/2 + 1 - \\delta/2 - 1\n\\\\=& 1-\\delta\n\\end{aligned}\n$$\n即\n$$\nP\\left(\n\\left(E(g)-\\widehat{E}(g) \\geqslant -\\frac{\\epsilon}{2} \\right)\\wedge\\left(E(h)-\\widehat{E}(h) \\leqslant \\frac{\\epsilon}{2}\n\\right)\\right) \\geqslant 1-\\delta\n$$\n\n\n因此\n$$\nP\\left(\n\\widehat{E}(g)-E(g)+E(h)-\\widehat{E}(h)\\leqslant\\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} \n\\right) = P\\left(E(h)-E(g)\\leqslant\\widehat{E}(h)-\\widehat{E}(g)+\\epsilon\\right)\n\\geqslant 1 - \\delta\n$$\n再由$h$和$g$的定义，$h$表示假设空间中经验误差最小的假设，$g$表示泛化误差最小的假设，将这两个假设共用作用于样本集$D$，则一定有$\\widehat{E}(h)\\leqslant\\widehat{E}(g)$，因此上式可以简化为：\n$$\nP\\left(E(h)-E(g)\\leqslant\\epsilon\\right)\n\\geqslant 1 - \\delta\n$$\n根据式12.32和式12.34，可以求出$m$为关于$\\left(1/\\epsilon,1/\\delta,\\text{size}(x),\\text{size}(c)\\right)$的多项式，因此根据定理12.2，定理12.5，得到结论任何VC维有限的假设空间$\\mathcal{H}$都是(不可知)PAC可学习的。\n\n\n\n## 12.32\n\n$$\n\\sqrt{\\frac{\\left(\\ln 2 / \\delta^{\\prime}\\right)}{2 m}}=\\frac{\\epsilon}{2}\n$$\n\n[解析]：参见12.31\n\n\n\n## 12.34\n\n$$\n\\sqrt{\\frac{8 d \\ln \\frac{2 e m}{d}+8 \\ln \\frac{4}{\\delta^{\\prime}}}{m}}=\\frac{\\epsilon}{2}\n$$\n\n\n\n[解析]：参见12.31\n\n\n\n## 12.36\n\n$$\n\\begin{aligned} \\widehat{E}(h) &=\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{I}\\left(h\\left(\\boldsymbol{x}_{i}\\right) \\neq y_{i}\\right) \\\\ &=\\frac{1}{m} \\sum_{i=1}^{m} \\frac{1-y_{i} h\\left(\\boldsymbol{x}_{i}\\right)}{2} \\\\ &=\\frac{1}{2}-\\frac{1}{2 m} \\sum_{i=1}^{m} y_{i} h\\left(\\boldsymbol{x}_{i}\\right) \\end{aligned}\n$$\n[解析]：这里解释从第一步到第二步的推导，因为前提假设是2分类问题，$y_k\\in\\{-1, +1\\}$，因此$\\mathbb{I}\\left(h(x_i)\\neq y_i\\right)\\equiv \\frac{1-y_i h(x_i)}{2}$。这是因为假如$y_i=+1, h(x_i)=+1$或$y_i=-1, h(x_i)=-1$，有$\\mathbb{I}\\left(h(x_i)\\neq y_i\\right)=0= \\frac{1-y_i h(x_i)}{2}$；反之，假如$y_i=-1, h(x_i)=+1$或$y_i=+1, h(x_i)=-1$，有$\\mathbb{I}\\left(h(x_i)\\neq y_i\\right)=1= \\frac{1-y_i h(x_i)}{2}$。\n\n\n\n## 12.37\n\n$$\n\\underset{h \\in \\mathcal{H}}{\\arg \\max } \\frac{1}{m} \\sum_{i=1}^{m} y_{i} h\\left(\\boldsymbol{x}_{i}\\right)\n$$\n[解析]：由公式12.36可知，经验误差$\\widehat{E}(h)$和$\\frac{1}{m} \\sum_{i=1}^{m} y_{i} h\\left(\\boldsymbol{x}_{i}\\right)$呈反比的关系，因此假设空间中能使经验误差最小的假设$h$即是使$\\frac{1}{m} \\sum_{i=1}^{m} y_{i} h\\left(\\boldsymbol{x}_{i}\\right)$最大的$h$。\n\n\n\n## 12.38\n\n$$\n\\sup _{h \\in \\mathcal{H}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i} h\\left(\\boldsymbol{x}_{i}\\right)\n$$\n[解析]：上确界$\\sup$这个概念前面已经解释过，见式12.7的解析。由于$\\sigma_i$是随机变量，因此这个式子可以理解为求解和随机生成的标签(即$\\sigma$)最契合的假设(当$\\sigma_i$和$h(\\boldsymbol{x}_i)$完全一致时，他们的内积最大)。\n\n\n\n## 12.39\n\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}}\\left[\\sup _{h \\in \\mathcal{H}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i} h\\left(\\boldsymbol{x}_{i}\\right)\\right]\n$$\n[解析]：这个式子可以用来衡量假设空间$\\mathcal{H}$的表达能力，对变量$\\sigma$求期望可以理解为当变量$\\sigma$包含所有可能的结果时，假设空间$\\mathcal{H}$中最契合的假设$h$和变量的平均契合程度。因为前提假设是2分类的问题，因此$\\sigma_i$一共有$2^m$种，这些不同的$\\sigma_i$构成了数据集$D=\\{(x_1, y_1), (x_2, y_2),\\dots, (x_m, y_m)\\}$的”对分“(12.4节)，如果一个假设空间的表达能力越强，那么就越有可能对于每一种$\\sigma_i$，假设空间中都存在一个$h$使得$h(x_i)$和$\\sigma_i$非常接近甚至相同，对所有可能的$\\sigma_i$取期望即可衡量假设空间的整体表达能力，这就是这个式子的含义。\n\n\n\n## 12.40\n\n$$\n\\widehat{R}_{Z}(\\mathcal{F})=\\mathbb{E}_{\\boldsymbol{\\sigma}}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i} f\\left(\\boldsymbol{z}_{i}\\right)\\right]\n$$\n[解析]：对比式12.39，这里使用函数空间$\\mathcal{F}$代替了假设空间$\\mathcal{H}$，函数$f$代替了假设$h$，很容易理解，因为假设$h$即可以看做是作用在数据$x_i$上的一个映射，通过这个映射可以得到标签$y_i$。注意前提假设实值函数空间$\\mathcal{F}:\\mathcal{Z}\\rightarrow\\mathbb{R}$，即映射$f$将样本$z_i$映射到了实数空间，这个时候所有的$\\sigma_i$将是一个标量即$\\sigma_i\\in\\{+1, -1\\}$。\n\n\n\n## 12.41\n\n$$\nR_{m}(\\mathcal{F})=\\mathbb{E}_{Z \\subseteq \\mathcal{Z}:|Z|=m}\\left[\\widehat{R}_{Z}(\\mathcal{F})\\right]\n$$\n[解析]：这里所要求的是$\\mathcal{F}$关于分布$\\mathcal{D}$的Rademacher复杂度，因此从$\\mathcal{D}$中采出不同的样本$Z$，计算这些样本对应的Rademacher复杂度的期望。\n\n\n\n## 12.42\n\n$$\n\\begin{array}{l}{\\mathbb{E}[f(\\boldsymbol{z})] \\leqslant \\frac{1}{m} \\sum_{i=1}^{m} f\\left(\\boldsymbol{z}_{i}\\right)+2 R_{m}(\\mathcal{F})+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}} \\\\ {\\mathbb{E}[f(\\boldsymbol{z})] \\leqslant \\frac{1}{m} \\sum_{i=1}^{m} f\\left(\\boldsymbol{z}_{i}\\right)+2 \\widehat{R}_{Z}(\\mathcal{F})+3 \\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}}\\end{array}\n$$\n[解析]：首先令记号\n$$\n\\begin{aligned} \\widehat{E}_{Z}(f) &=\\frac{1}{m} \\sum_{i=1}^{m} f\\left(\\boldsymbol{z}_{i}\\right) \\\\ \\Phi(Z) &=\\sup _{f \\in \\mathcal{F}} \\left(\\mathbb{E}[f]-\\widehat{E}_{Z}(f)\\right) \\end{aligned}\n$$\n即$\\widehat{E}_{Z}(f)$表示函数$f$作为假设下的经验误差，$\\Phi(Z)$表示泛化误差和经验误差的差的上确界。再令$Z^\\prime$为只与$Z$有一个示例(样本)不同的训练集，不妨设$z_m\\in Z$和$z^\\prime_m\\in Z^\\prime$为不同的示例，那么有\n$$\n\\begin{aligned} \\Phi\\left(Z^{\\prime}\\right)-\\Phi(Z) &=\\sup _{f \\in \\mathcal{F}} \\left(\\mathbb{E}[f]-\\widehat{E}_{Z^{\\prime}}(f)\\right)-\\sup _{f \\in \\mathcal{F}} \\left(\\mathbb{E}[f]-\\widehat{E}_{Z}(f)\\right) \\\\ & \\leqslant \\sup _{f \\in \\mathcal{F}} \\left(\\widehat{E}_{Z}(f)-\\widehat{E}_{Z^{\\prime}}(f)\\right) \\\\ &=\\sup_{f\\in\\mathcal{F}}\\frac{\\sum^m_{i=1}f(z_i)-\\sum^m_{i=1}f(z^\\prime_i)}{m}\\\\&=\\sup _{f \\in \\mathcal{F}} \\frac{f\\left(z_{m}\\right)-f\\left(z_{m}^{\\prime}\\right)}{m} \\\\ & \\leqslant \\frac{1}{m} \\end{aligned}\n$$\n第一个不等式是因为[上确界的差不大于差的上确界](https://math.stackexchange.com/questions/246015/supremum-of-the-difference-of-two-functions)，第四行的等号由于$Z^\\prime$与$Z$只有$z_m$不相同，最后一行的不等式是因为前提假设$\\mathcal{F}:\\mathcal{Z}\\rightarrow [0,1]$，即$f(z_m),f(z_m^\\prime)\\in[0,1]$。\n\n同理\n$$\n\\Phi(Z)-\\Phi\\left(Z^{\\prime}\\right) =\\sup _{f \\in \\mathcal{F}} \\frac{f\\left(z_{m}^\\prime\\right)-f\\left(z_{m}\\right)}{m} \\leqslant \\frac{1}{m}\n$$\n综上二式有：\n$$\n\\left\\vert \\Phi(Z)-\\Phi\\left(Z^{\\prime}\\right)\\right\\vert \\leqslant \\frac{1}{m}\n$$\n将$\\Phi$看做函数$f$(注意这里的$f$不是$\\Phi$定义里的$f$)，那么可以套用McDiarmid不等式的结论式12.7\n$$\nP\\left(\\Phi(Z)-\\mathbb{E}_{Z}[\\Phi(Z)] \\geqslant \\epsilon\\right) \\leqslant \\exp \\left(\\frac{-2 \\epsilon^{2}}{\\sum_{i} c_{i}^{2}}\\right)\n$$\n令$ \\exp \\left(\\frac{-2 \\epsilon^{2}}{\\sum_{i} c_{i}^{2}}\\right)=\\delta$可以求得$\\epsilon=\\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}$，所以\n$$\nP\\left(\\Phi(Z)-\\mathbb{E}_{Z}[\\Phi(Z)] \\geqslant \\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\\right) \\leqslant \\delta\n$$\n由逆事件的概率定义得\n$$\nP\\left(\\Phi(Z)-\\mathbb{E}_{Z}[\\Phi(Z)] \\leqslant \\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\\right) \\geqslant 1-\\delta\n$$\n即书中式12.44的结论。\n\n下面来估计$\\mathbb{E}_{Z}[\\Phi(Z)]$的上界：\n$$\n\\begin{aligned} \\mathbb{E}_{Z}[\\Phi(Z)] &=\\mathbb{E}_{Z}\\left[\\sup _{f \\in \\mathcal{F}} \\left(\\mathbb{E}[f]-\\widehat{E}_{Z}(f)\\right)\\right] \\\\ &=\\mathbb{E}_{Z}\\left[\\sup _{f \\in \\mathcal{F}} \\mathbb{E}_{Z^{\\prime}}\\left[\\widehat{E}_{Z^{\\prime}}(f)-\\widehat{E}_{Z}(f)\\right]\\right] \\\\ & \\leqslant \\mathbb{E}_{Z, Z^{\\prime}}\\left[\\sup _{f \\in \\mathcal{F}}\\left( \\widehat{E}_{Z^{\\prime}}(f)-\\widehat{E}_{Z}(f)\\right)\\right] \\\\ &=\\mathbb{E}_{Z, Z^{\\prime}}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m}\\left(f\\left(\\boldsymbol{z}_{i}^{\\prime}\\right)-f\\left(\\boldsymbol{z}_{i}\\right)\\right)\\right] \\\\ &=\\mathbb{E}_{\\boldsymbol{\\sigma}, Z,Z^{\\prime}}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i}\\left(f\\left(\\boldsymbol{z}_{i}^{\\prime}\\right)-f\\left(\\boldsymbol{z}_{i}\\right)\\right)\\right] \\\\ &\\leqslant \\mathbb{E}_{\\boldsymbol{\\sigma}, Z^{\\prime}}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i} f\\left(\\boldsymbol{z}_{i}^{\\prime}\\right)\\right]+\\mathbb{E}_{\\boldsymbol{\\sigma}, Z}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m}-\\sigma_{i} f\\left(\\boldsymbol{z}_{i}\\right)\\right] \\\\ &=2 \\mathbb{E}_{\\boldsymbol{\\sigma}, Z}\\left[\\sup _{f \\in \\mathcal{F}} \\frac{1}{m} \\sum_{i=1}^{m} \\sigma_{i} f\\left(\\boldsymbol{z}_{i}\\right)\\right] \\\\ &=2 R_{m}(\\mathcal{F}) \\end{aligned}\n$$\n第二行等式是外面套了一个对服从分布$\\mathcal{D}$的示例集$Z^\\prime$求期望，因为$\\mathbb{E}_{Z^\\prime\\sim\\mathcal{D}}[\\widehat{E}_{Z^\\prime}(f)]=\\mathbb{E}(f)$，而采样出来的$Z^\\prime$和$Z$相互独立，因此有$\\mathbb{E}_{Z^\\prime\\sim\\mathcal{D}}[\\widehat{E}_{Z}(f)]=\\widehat{E}_{Z}(f)$。\n\n第三行不等式基于上确界函数$\\sup$是个凸函数，将$\\sup_{f\\in\\mathcal{F}}$看做是凸函数$f$，将$\\widehat{E}_{Z^{\\prime}}(f)-\\widehat{E}_{Z}(f)$看做变量$x$根据Jesen不等式(式12.4)，有$\\mathbb{E}_{Z}\\left[\\sup _{f \\in \\mathcal{F}} \\mathbb{E}_{Z^{\\prime}}\\left[\\widehat{E}_{Z^{\\prime}}(f)-\\widehat{E}_{Z}(f)\\right]\\right] \\leqslant \\mathbb{E}_{Z, Z^{\\prime}}\\left[\\sup _{f \\in \\mathcal{F}}\\left( \\widehat{E}_{Z^{\\prime}}(f)-\\widehat{E}_{Z}(f)\\right)\\right] $，其中$\\mathbb{E}_{Z, Z^{\\prime}}[\\cdot]$是$\\mathbb{E}_{Z}[\\mathbb{E}_{Z^\\prime}[\\cdot]]$的简写形式。\n\n第五行引入对Rademacher随机变量的期望，由于函数值空间是标量，因为$\\sigma_i$也是标量，即$\\sigma_i\\in\\{-1, +1\\}$，且$\\sigma_i$总以相同概率可以取到这两个值，因此可以引入$\\mathbb{E}_{\\sigma}$而不影响最终结果。\n\n第六行利用了[上确界的和不小于和的上确界](https://math.stackexchange.com/questions/246015/supremum-of-the-difference-of-two-functions)，因为第一项中只含有变量$z^\\prime$，所以可以将$\\mathbb{E}_Z$去掉，因为第二项中只含有变量$z$，所以可以将$\\mathbb{E}_{Z^\\prime}$去掉。\n\n第七行利用$\\sigma$是对称的，所以$-\\sigma$的分布和$\\sigma$完全一致，所以可以将第二项中的负号去除，又因为$Z$和$Z^\\prime$均是从$\\mathcal{D}$中$i.i.d.$采样得到的数据，因此可以将第一项中的$z^\\prime_i$替换成$z$，将$Z^\\prime$替换成$Z$。\n\n最后根据定义式12.41可得$\\mathbb{E}_{Z}[\\Phi(Z)]=2\\mathcal{R}_m(\\mathcal{F})$，式12.42得证。\n\n## 12.43\n\n$$\n\\mathbb{E}[f(\\boldsymbol{z})] \\leqslant \\frac{1}{m} \\sum_{i=1}^{m} f\\left(\\boldsymbol{z}_{i}\\right)+2 \\widehat{R}_{Z}(\\mathcal{F})+3 \\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}\n$$\n\n\n\n[解析]：参见12.42\n\n\n\n## 12.44\n\n$$\n\\Phi(Z) \\leqslant \\mathbb{E}_{Z}[\\Phi(Z)]+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\n$$\n\n\n\n[解析]：参见 12.42\n\n\n\n## 12.45\n\n$$\nR_{m}(\\mathcal{F}) \\leqslant \\widehat{R}_{Z}(\\mathcal{F})+\\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}\n$$\n\n\n\n[解析]：参见12.42\n\n\n\n## 12.46\n\n$$\n\\Phi(Z) \\leqslant 2 \\widehat{R}_{Z}(\\mathcal{F})+3 \\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}\n$$\n\n\n\n参见12.42\n\n\n\n## 12.52\n\n$$\nR_{m}(\\mathcal{H}) \\leqslant \\sqrt{\\frac{2 \\ln \\Pi_{\\mathcal{H}}(m)}{m}}\n$$\n[证明]：比较繁琐，同书上所示，参见[Mohri etc., 2012](https://cs.nyu.edu/~mohri/mlbook/)\n\n## 12.53\n\n$$\nE(h) \\leqslant \\widehat{E}(h)+\\sqrt{\\frac{2 d \\ln \\frac{e m}{d}}{m}}+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\n$$\n\n[解析]：根据式12.28有$\\Pi_{\\mathcal{H}}(m) \\leqslant\\left(\\frac{e \\cdot m}{d}\\right)^{d}$，根据式12.52有$R_{m}(\\mathcal{H}) \\leqslant \\sqrt{\\frac{2 \\ln \\Pi_{\\mathcal{H}}(m)}{m}}$，因此$\\Pi_{\\mathcal{H}}(m) \\leqslant \\sqrt{\\frac{2 d \\ln \\frac{e m}{d}}{m}}$，再根据式12.47 $E(h) \\leqslant \\widehat{E}(h)+R_{m}(\\mathcal{H})+\\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}$ 即证。\n\n\n\n## 12.57\n\n$$\n\\begin{array}{l}{\\quad\\left|\\ell\\left(\\mathfrak{L}_{D}, \\boldsymbol{z}\\right)-\\ell\\left(\\mathfrak{L}_{D^{i}}, \\boldsymbol{z}\\right)\\right|} \\\\ {\\leqslant\\left|\\ell\\left(\\mathfrak{L}_{D}, \\boldsymbol{z}\\right)-\\ell\\left(\\mathfrak{L}_{D^{\\backslash i}}, \\boldsymbol{z}\\right)\\right|+\\left|\\ell\\left(\\mathfrak{L}_{D^{i}, \\boldsymbol{z}}\\right)-\\ell\\left(\\mathfrak{L}_{D^{\\backslash i}, \\boldsymbol{z}}\\right)\\right|} \\\\ {\\leqslant 2 \\beta}\\end{array}\n$$\n\n[解析]：根据[三角不等式]([https://zh.wikipedia.org/zh-hans/%E4%B8%89%E8%A7%92%E4%B8%8D%E7%AD%89%E5%BC%8F](https://zh.wikipedia.org/zh-hans/三角不等式))，有$|a+b| \\leq|a|+|b|$，将$a=\\ell\\left(\\mathfrak{L}_{D}, \\boldsymbol{z}\\right)-\\ell\\left(\\mathfrak{L}_{D^{i}}\\right)$，$b=\\ell\\left(\\mathfrak{L}_{D^{i}, \\boldsymbol{z}}\\right)-\\ell\\left(\\mathfrak{L}_{D^{\\backslash i}, \\boldsymbol{z}}\\right)$代入即可得出第一个不等式，根据$D^{\\backslash i}$表示移除$D$中第$i$个样本，$D^i$表示替换$D$中第$i$个样本，那么$a,b$的变动均为一个样本，根据式12.57，$a\\leqslant\\beta, b\\leqslant\\beta$，因此$a +b \\leqslant 2\\beta$。\n\n\n\n## 12.58\n\n$$\n\\ell(\\mathfrak{L}, \\mathcal{D}) \\leqslant \\widehat{\\ell}(\\mathfrak{L}, D)+2 \\beta+(4 m \\beta+M) \\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\n$$\n\n\n\n[证明]：比较繁琐，同书上所示，参见[Mohri etc., 2012](https://cs.nyu.edu/~mohri/mlbook/)\n\n\n\n## 12.59\n\n$$\n\\ell(\\mathfrak{L}, \\mathcal{D}) \\leqslant \\ell_{l o o}(\\mathfrak{L}, D)+\\beta+(4 m \\beta+M) \\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\n$$\n\n\n\n[证明]：比较繁琐，同书上所示，参见[Mohri etc., 2012](https://cs.nyu.edu/~mohri/mlbook/)\n\n\n\n## 12.60\n\n$$\n\\ell(\\mathfrak{L}, \\mathcal{D}) \\leqslant \\widehat{\\ell}(\\mathfrak{L}, D)+\\frac{2}{m}+(4+M) \\sqrt{\\frac{\\ln (1 / \\delta)}{2 m}}\n$$\n\n\n\n[证明]：将$\\beta=\\frac{1}{m}$代入至式12.58即得证。\n\n\n\n## 定理12.9\n\n若学习算法$\\mathcal{L}$是ERM且是稳定的，则假设空间$\\mathcal{H}$可学习。\n\n[解析]：首先ERM表示算法$\\mathcal{L}$满足经验风险最小化(Empirical Risk Minimization)。由于$\\mathcal{L}$满足经验误差最小化，则可令$g$表示假设空间中具有最小泛化损失的假设，即\n$$\n\\ell(g, \\mathcal{D})=\\min _{h \\in \\mathcal{H}} \\ell(h, \\mathcal{D})\n$$\n再令\n$$\n\\begin{array}{l}{\\epsilon^{\\prime}=\\frac{\\epsilon}{2}} \\\\ {\\frac{\\delta}{2}=2 \\exp \\left(-2 m\\left(\\epsilon^{\\prime}\\right)^{2}\\right)}\\end{array}\n$$\n将$\\epsilon^\\prime=\\frac{\\epsilon}{2}$代入到${\\frac{\\delta}{2}=2 \\exp \\left(-2 m\\left(\\epsilon^{\\prime}\\right)^{2}\\right)}$可以解得$m=\\frac{2}{\\epsilon^{2}} \\ln \\frac{4}{\\delta}$，由Hoeffding不等式12.6，$$P\\left(\\left\\vert\\frac{1}{m} \\sum_{i=1}^{m} x_{i}-\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right)\\right\\vert \\geqslant \\epsilon\\right) \\leqslant 2 \\exp \\left(-2 m \\epsilon^{2}\\right)$$，其中$\\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}\\left(x_{i}\\right)=\\ell(g, \\mathcal{D})$，$\\frac{1}{m} \\sum_{i=1}^{m} x_{i}=\\widehat{\\ell}(g, \\mathcal{D})$，代入可得\n$$\nP(|\\ell(g, \\mathcal{D})-\\widehat{\\ell}(g, D)| \\geqslant \\frac{\\epsilon}{2})\\leqslant \\frac{\\delta}{2}\n$$\n根据逆事件的概率可得\n$$\nP(|\\ell(g, \\mathcal{D})-\\widehat{\\ell}(g, D)| \\leqslant \\frac{\\epsilon}{2})\\geqslant 1- \\frac{\\delta}{2}\n$$\n即文中$|\\ell(g, \\mathcal{D})-\\widehat{\\ell}(g, D)| \\leqslant \\frac{\\epsilon}{2}$至少以$1-\\delta/2$的概率成立。\n\n由$\\frac{2}{m}+(4+M) \\sqrt{\\frac{\\ln (2 / \\delta)}{2 m}}=\\frac{\\epsilon}{2}$可以求解出\n$$\n\\sqrt{m}=\\frac{(4+M) \\sqrt{\\frac{\\ln (2 / \\delta)}{2}}+\\sqrt{(4+M)^{2} \\frac{\\ln (2 / \\delta)}{2}-4 \\times \\frac{\\epsilon}{2} \\times(-2)}}{2 \\times \\frac{\\epsilon}{2}}\n$$\n即$m=O\\left(\\frac{1}{\\epsilon^{2}} \\ln \\frac{1}{\\delta}\\right)$。\n\n由$P(|\\ell(g, \\mathcal{D})-\\widehat{\\ell}(g, D)| \\leqslant \\frac{\\epsilon}{2})\\geqslant 1- \\frac{\\delta}{2}$可以按照同公式12.31中介绍的相同的方法推导出\n$$\nP(\\ell(\\mathfrak{L}, \\mathcal{D})-\\ell(g, \\mathcal{D})\\leqslant\\epsilon)\\geqslant 1-\\delta\n$$\n又因为$m$为与$\\left(1/\\epsilon,1/\\delta,\\text{size}(x),\\text{size}(c)\\right)$相关的多项式的值，因此根据定理12.2，定理12.5，得到结论$\\mathcal{H}$是(不可知)PAC可学习的。\n\n\n\n\n\n\n\n\n\n\n\n','2021-12-10 12:43:49','2021-12-19 13:22:58'),
	(13,1,'chapter13','## 13.1\n\n$$\np(\\boldsymbol{x})=\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\n$$\n\n[解析]： 高斯混合分布的定义式。\n\n## 13.2\n\n$$\n\\begin{aligned} f(\\boldsymbol{x}) &=\\underset{j \\in \\mathcal{Y}}{\\arg \\max } p(y=j | \\boldsymbol{x}) \\\\ &=\\underset{j \\in \\mathcal{Y}}{\\arg \\max } \\sum_{i=1}^{N} p(y=j, \\Theta=i | \\boldsymbol{x}) \\\\ &=\\underset{j \\in \\mathcal{Y}}{\\arg \\max } \\sum_{i=1}^{N} p(y=j | \\Theta=i, \\boldsymbol{x}) \\cdot p(\\Theta=i | \\boldsymbol{x}) \\end{aligned}\n$$\n\n[解析]：从公式第 1 行到第 2 行是对概率进行边缘化(marginalization)；通过引入$\\Theta$并对其求和 $\\sum_{i=1}^N$以抵消引入的影响。从公式第 2 行到第 3 行推导如下\n$$\n\\begin{aligned}p(y=j, \\Theta=i | \\boldsymbol{x}) &=\\frac{p(y=j, \\Theta=i, \\boldsymbol{x})}{p(\\boldsymbol{x})} \\\\&=\\frac{p(y=j, \\Theta=i, \\boldsymbol{x})}{p(\\Theta=i, \\boldsymbol{x})} \\cdot \\frac{p(\\Theta=i, \\boldsymbol{x})}{p(\\boldsymbol{x})} \\\\&=p(y=j | \\Theta=i, \\boldsymbol{x}) \\cdot p(\\Theta=i | \\boldsymbol{x})\\end{aligned}\n$$\n\n## 13.3\n\n$$\np(\\Theta=i | \\boldsymbol{x})=\\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}\n$$\n\n[解析]：根据 13.1 \n$$\np(\\boldsymbol{x})=\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\n$$\n因此\n$$\n\\begin{aligned}p(\\Theta=i | \\boldsymbol{x})&=\\frac{p(\\Theta=i , \\boldsymbol{x})}{P(x)}\\\\&=\\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}\\end{aligned}\n$$\n\n## 13.4\n\n$$\n\\begin{aligned} L L\\left(D_{l} \\cup D_{u}\\right)=& \\sum_{\\left(x_{j}, y_{j}\\right) \\in D_{l}} \\ln \\left(\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot p\\left(y_{j} | \\Theta=i, \\boldsymbol{x}_{j}\\right)\\right) \\\\ &+\\sum_{x_{j} \\in D_{u}} \\ln \\left(\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\\right) \\end{aligned}\n$$\n\n[解析]：第二项很好解释，当不知道类别信息的时候，样本$x_j$的概率可以用式 13.1 表示，所有无类别信息的样本$D_u$的似然是所有样本的乘积，因为$\\ln$函数是单调的，所以也可以将$\\ln$函数作用于这个乘积消除因为连乘产生的数值计算问题。第一项引入了样本的标签信息，由\n$$\np(y=j | \\Theta=i, \\boldsymbol{x})=\\left\\{\\begin{array}{ll}1, & i=j \\\\0, & i \\neq j\\end{array}\\right.\n$$\n可知，这项限定了样本$x_j$只可能来自于$y_j$所对应的高斯分布。\n\n## 13.5\n\n$$\n\\gamma_{j i}=\\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{i=1}^{N} \\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}\n$$\n\n[解析]：参见式 13.3，这项可以理解成样本$x_j$属于类别标签$i$(或者说由第$i$个高斯分布生成)的后验概率。其中$\\alpha_i,\\boldsymbol{\\mu}_{i}\\boldsymbol{\\Sigma}_i$可以通过有标记样本预先计算出来。即：\n$$\n\\begin{array}{l}\\alpha_{i}=\\frac{l_{i}}{\\left|D_{l}\\right|}, \\text { where }\\left|D_{l}\\right|=\\sum_{i=1}^{N} l_{i} \\\\\\boldsymbol{\\mu}_{i}=\\frac{1}{l_{i}} \\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{x}_{j} \\\\\\boldsymbol{\\Sigma}_{i}=\\frac{1}{l_{i}} \\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}\\end{array}\n$$\n\n\n## 13.6\n\n$$\n\\boldsymbol{\\mu}_{i}=\\frac{1}{\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i}+l_{i}}\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\boldsymbol{x}_{j}+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{x}_{j}\\right)\n$$\n\n[推导]：这项可以由$$\\cfrac{\\partial LL(D_l \\cup D_u) }{\\partial \\mu_i}=0$$而得，将式 13.4 的两项分别记为：\n$$\n\\begin{aligned}LL(D_l)&=\\sum_{(\\boldsymbol{x_j},y_j \\in D_l)}\\ln\\left(\\sum_{s=1}^{N}\\alpha_s \\cdot p(\\boldsymbol{x_j}\\vert \\boldsymbol{\\mu}_s,\\boldsymbol{\\Sigma}_s) \\cdot p(y_i|\\Theta = s,\\boldsymbol{x_j})\\right)\\\\&=\\sum_{(\\boldsymbol{x_j},y_j \\in D_l)}\\ln\\left(\\sum_{s=1}^{N}\\alpha_{y_j} \\cdot p(\\boldsymbol{x_j} \\vert \\boldsymbol{\\mu}_{y_j},\\boldsymbol{\\Sigma}_{y_j})\\right)\\\\LL(D_u)&=\\sum_{\\boldsymbol{x_j} \\in D_u} \\ln\\left(\\alpha_s \\cdot p(\\boldsymbol{x_j} | \\boldsymbol{\\mu}_s,\\boldsymbol{\\Sigma}_s)\\right)\\end{aligned}\n$$\n首先，$LL(D_l)$对$\\boldsymbol{\\mu_i}$求偏导，$LL(D_l)$求和号中只有$y_j=i$ 的项能留下来，即\n$$\n\\begin{aligned}\\frac{\\partial L L\\left(D_{l}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} &=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{\\partial \\ln \\left(\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\\right)}{\\partial \\boldsymbol{\\mu}_{i}} \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot \\frac{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right) \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\end{aligned}\n$$\n$LL(D_u)$对$\\boldsymbol{\\mu_i}$求导，参考 9.33 的推导：\n$$\n\\begin{aligned}\n\\frac{\\partial L L\\left(D_{u}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} &=\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\frac{\\alpha_{i}}{\\sum_{s=1}^{N} \\alpha_{s} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{s}, \\boldsymbol{\\Sigma}_{s}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right) \\\\\n&=\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\n\\end{aligned}\n$$\n\n\n综上，\n$$\n\\begin{aligned}\\frac{\\partial L L\\left(D_{l} \\cup D_{u}\\right)}{\\partial \\boldsymbol{\\mu}_{i}} &=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)+\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right) \\\\&=\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)+\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\right) \\\\&=\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{x}_{j}+\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{x}_{j}-\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{\\mu}_{i}-\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{\\mu}_{i}\\right)\\end{aligned}\n$$\n令$\\frac{\\partial L L\\left(D_{l} \\cup D_{u}\\right)}{\\partial \\boldsymbol{\\mu}_{i}}=0$，两边同时左乘$\\Sigma_i$并移项：\n$$\n\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{\\mu}_{i}+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{\\mu}_{i}=\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{x}_{j}+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{x}_{j}\n$$\n上式中，$\\boldsymbol{\\mu_i}$ 可以作为常量提到求和号外面，而$\\sum_{\\left(x_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} 1=l_{i}$，即第$i$类样本的有标记 样本数目，因此\n\n$$\n\\left(\\sum_{x_{j} \\in D_{u}} \\gamma_{j i}+\\sum_{\\left(x_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} 1\\right) \\boldsymbol{\\mu}_{i}=\\sum_{x_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{x}_{j}+\\sum_{\\left(x_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{x}_{j}\n$$\n\n\n即得式 13.6。\n\n## 13.7\n\n$$\n\\begin{aligned}\\boldsymbol{\\Sigma}_{i}=& \\frac{1}{\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i}+l_{i}}\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}\\right.\\\\&\\left.+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}\\right)\\end{aligned}\n$$\n\n[推导]：首先$LL(D_l)$对$\\boldsymbol{\\Sigma_i}$求偏导 ，类似于 13.6\n$$\n\\begin{aligned} \\frac{\\partial L L\\left(D_{l}\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}} &=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{\\partial \\ln \\left(\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}} \\\\ &=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot \\frac{\\partial p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}} \\\\\n&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\cdot\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1}\\\\\n&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1}\n\\end{aligned}\n$$\n然后$LL(D_u)$ 对$\\boldsymbol{\\Sigma_i}$求偏导，类似于 9.35\n$$\n\\frac{\\partial L L\\left(D_{u}\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}}=\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1}\n$$\n综合可得：\n$$\n\\begin{aligned} \\frac{\\partial L L\\left(D_{l} \\cup D_{u}\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}}=& \\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1} \\\\ &+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1} \\\\=&\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right)\\right.\\\\ &\\left.+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}-\\boldsymbol{I}\\right)\\right) \\cdot \\frac{1}{2} \\boldsymbol{\\Sigma}_{i}^{-1} \\end{aligned}\n$$\n令$\\frac{\\partial L L\\left(D_{l} \\cup D_{u}\\right)}{\\partial \\boldsymbol{\\Sigma}_{i}}=0$，两边同时右乘$2\\Sigma_i$并移项：\n$$\n\\begin{aligned} \\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}+& \\sum_{\\left(\\boldsymbol{x}_{j}, y_{j} \\in D_{l} \\wedge y_{j}=i\\right.} \\boldsymbol{\\Sigma}_{i}^{-1}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top} \\\\=& \\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot \\boldsymbol{I}+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\boldsymbol{I} \\\\ &=\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i}+l_{i}\\right) \\boldsymbol{I} \\end{aligned}\n$$\n两边同时左乘以$\\Sigma_i$：\n$$\n\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i} \\cdot\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}+\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i}\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}_{j}-\\boldsymbol{\\mu}_{i}\\right)^{\\top}=\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i}+l_{i}\\right) \\boldsymbol{\\Sigma}_{i}\n$$\n即得式 13.7。\n\n## 13.8\n\n$$\n\\alpha_{i}=\\frac{1}{m}\\left(\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\gamma_{j i}+l_{i}\\right)\n$$\n\n[推导]：类似于式 9.36，写出$LL(D_l \\cup D_u)$的拉格朗日形式\n$$\n\\begin{aligned}\\mathcal{L}\\left(D_{l} \\cup D_{u}, \\lambda\\right) &=L L\\left(D_{l} \\cup D_{u}\\right)+\\lambda\\left(\\sum_{s=1}^{N} \\alpha_{s}-1\\right) \\\\&=L L\\left(D_{l}\\right)+L L\\left(D_{u}\\right)+\\lambda\\left(\\sum_{s=1}^{N} \\alpha_{s}-1\\right)\\end{aligned}\n$$\n\n\n类似于式 9.37，对$\\alpha_i$求偏导。对于$LL(D_u)$，求导结果与式 9.37 的推导过程一样\n$$\n\\frac{\\partial L L\\left(D_{u}\\right)}{\\partial \\alpha_{i}}=\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\frac{1}{\\sum_{s=1}^{N} \\alpha_{s} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{s}, \\boldsymbol{\\Sigma}_{s}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\n$$\n\n\n对于$LL(D_l)$，类似于 13.6 和 13.7 的推导过程\n$$\n\\begin{aligned}\\frac{\\partial L L\\left(D_{l}\\right)}{\\partial \\alpha_{i}} &=\\sum_{\\left(x_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{\\partial \\ln \\left(\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\\right)}{\\partial \\alpha_{i}} \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot \\frac{\\partial\\left(\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)\\right)}{\\partial \\alpha_{i}} \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right) \\\\&=\\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} \\frac{1}{\\alpha_{i}}=\\frac{1}{\\alpha_{i}} \\cdot \\sum_{\\left(\\boldsymbol{x}_{j}, y_{j}\\right) \\in D_{l} \\wedge y_{j}=i} 1=\\frac{l_{i}}{\\alpha_{i}}\\end{aligned}\n$$\n\n\n上式推导过程中，重点注意变量是$\\alpha_i$ ，$p(x_j|\\mu_i,\\Sigma_i)$是常量；最后一行$\\alpha_i$相对于求和变量为常量，因此作为公因子提到求和号外面； $l_i$ 为第$i$类样本的有标记样本数目。\n\n综合两项结果：\n$$\n\\frac{\\partial \\mathcal{L}\\left(D_{l} \\cup D_{u}, \\lambda\\right)}{\\partial \\alpha_{i}}=\\frac{l_{i}}{\\alpha_{i}}+\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\frac{p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{s=1}^{N} \\alpha_{s} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{s}, \\boldsymbol{\\Sigma}_{s}\\right)}+\\lambda\n$$\n\n\n令$\\cfrac{\\partial LL(D_l \\cup D_u) }{\\partial \\alpha_i}=0$ 并且两边同乘以$\\alpha_i$，得\n$$\n\\alpha_{i} \\cdot \\frac{l_{i}}{\\alpha_{i}}+\\sum_{\\boldsymbol{x}_{j} \\in D_{u}} \\frac{\\alpha_{i} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{i}, \\boldsymbol{\\Sigma}_{i}\\right)}{\\sum_{s=1}^{N} \\alpha_{s} \\cdot p\\left(\\boldsymbol{x}_{j} | \\boldsymbol{\\mu}_{s}, \\boldsymbol{\\Sigma}_{s}\\right)}+\\lambda \\cdot \\alpha_{i}=0\n$$\n\n\n结合式 9.30 发现，求和号内即为后验概率$\\gamma_{ji}$,即\n$$\nl_i+\\sum_{x_i \\in D_u} \\gamma_{ji}+\\lambda \\alpha_i = 0\n$$\n对所有混合成分求和，得\n$$\n\\sum_{i=1}^N l_i+\\sum_{i=1}^N  \\sum_{x_i \\in D_u} \\gamma_{ji}+\\sum_{i=1}^N \\lambda \\alpha_i = 0\n$$\n\n\n这里$\\sum_{i=1}^N \\alpha_i =1$ ，因此$\\sum_{i=1}^N \\lambda \\alpha_i=\\lambda\\sum_{i=1}^N \\alpha_i=\\lambda$，根据 9.30 中$\\gamma_{ji}$表达式可知\n$$\n\\sum_{i=1}^N \\gamma_{ji} =  \\sum_{i =1}^{N} \\cfrac{\\alpha_i \\cdot  p(x_j|\\mu_i,\\Sigma_i)}{\\Sigma_{s=1}^N \\alpha_s \\cdot p(x_j| \\mu_s, \\Sigma_s)}=  \\cfrac{\\sum_{i =1}^{N}\\alpha_i \\cdot  p(x_j|\\mu_i,\\Sigma_i)}{\\sum_{s=1}^N \\alpha_s \\cdot p(x_j| \\mu_s, \\Sigma_s)}=1\n$$\n\n\n再结合加法满足交换律，所以\n$$\n\\sum_{i=1}^N  \\sum_{x_i \\in D_u} \\gamma_{ji}=\\sum_{x_i \\in D_u} \\sum_{i=1}^N  \\gamma_{ji} =\\sum_{x_i \\in D_u} 1=u\n$$\n\n\n以上分析过程中，$\\sum_{x_j\\in D_u}$ 形式与$\\sum_{j=1}^u$等价，其中u为未标记样本集的样本个数； $\\sum_{i=1}^Nl_i=l$其中$l$为有标记样本集的样本个数；将这些结果代入\n$$\n\\sum_{i=1}^N l_i+\\sum_{i=1}^N  \\sum_{x_i \\in D_u} \\gamma_{ji}+\\sum_{i=1}^N \\lambda \\alpha_i = 0\n$$\n\n\n解出$l+u+\\lambda = 0$  且$l+u =m$ 其中$m$为样本总个数，移项即得$\\lambda = -m$\n最后代入整理解得\n$$\nl_i + \\sum_{x_j \\in{D_u}} \\gamma_{ji}-\\lambda \\alpha_i = 0\n$$\n整理即得式 13.8。\n\n## 13.9\n\n$$\n\\min _{\\boldsymbol{w}, \\boldsymbol{b}, \\boldsymbol{y}, \\boldsymbol{\\xi}} \\frac{1}{2}\\|\\boldsymbol{w}\\|_{2}^{2}+C_{l} \\sum_{i=1}^{l} \\xi_{i}+C_{u} \\sum_{i=l+1}^{m} \\xi_{i}\\\\\n\\begin{aligned}\n\\text { s.t. } &y_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1-\\xi_{i}, \\quad i=1,2, \\ldots, l\\\\\n&\\hat{y}_{i}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{i}+b\\right) \\geqslant 1-\\xi_{i}, \\quad i=l+1, l+2, \\ldots, m\\\\\n&\\xi_{i} \\geqslant 0, \\quad i=1,2, \\dots, m\n\\end{aligned}\n$$\n\n[解析]：这个公式和公式 6.35 基本一致，除了引入了无标记样本的松弛变量$\\xi_i, i=l+1,\\cdots m$和对应的权重系数$C_u$和无标记样本的标记指派$\\hat{y}_i$。\n\n## 13.12\n\n$$\n\\begin{aligned}\nE(f) &=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-f\\left(\\boldsymbol{x}_{j}\\right)\\right)^{2} \\\\\n&=\\frac{1}{2}\\left(\\sum_{i=1}^{m} d_{i} f^{2}\\left(\\boldsymbol{x}_{i}\\right)+\\sum_{j=1}^{m} d_{j} f^{2}\\left(\\boldsymbol{x}_{j}\\right)-2 \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right)\\right) \\\\\n&=\\sum_{i=1}^{m} d_{i} f^{2}\\left(\\boldsymbol{x}_{i}\\right)-\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right) \\\\\n&=\\boldsymbol{f}^{\\mathrm{T}}(\\mathbf{D}-\\mathbf{W}) \\boldsymbol{f}\n\\end{aligned}\n$$\n\n[解析]：首先解释下这个能量函数的定义。原则上，我们希望能量函数$E(f)$越小越好，对于节点$i,j$，如果它们不相邻，则$(\\mathbf{W})_{i j}=0$，如果它们相邻，则最小化能量函数要求$f(x_i)$和$f(x_j)$尽量相似，和逻辑相符。下面进行公式的推导，首先由二项展开可得：\n$$\n\\begin{aligned}\nE(f) &=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j}\\left(f\\left(\\boldsymbol{x}_{i}\\right)-f\\left(\\boldsymbol{x}_{j}\\right)\\right)^{2} \\\\\n&=\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j}\\left(f^{2}\\left(\\boldsymbol{x}_{i}\\right)-2 f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right)+f^{2}\\left(\\boldsymbol{x}_{j}\\right)\\right) \\\\\n&=\\frac{1}{2}\\left( \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{i}\\right)+ \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{j}\\right)-2\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right)\\right)\n\\end{aligned}\n$$\n由于$\\mathbf{W}$是一个对称矩阵，可以通过变量替换得到\n$$\n\\begin{aligned}\n\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{j}\\right)&=\\sum_{j=1}^{m} \\sum_{i=1}^{m}(\\mathbf{W})_{j i} f^{2}\\left(\\boldsymbol{x}_{i}\\right)\\\\\n&=\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{i}\\right)\\\\\n&=\n\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{j}\\right)\n\\end{aligned}\n$$\n因此$E(f)$可化简为\n$$\n\\begin{aligned}\nE(f) &=  \\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f^{2}\\left(\\boldsymbol{x}_{i}\\right)-\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right)\n\\end{aligned}\n$$\n根据定义 $d_i=\\sum_{j=1}^{l+u}\\left(\\mathbf{W}\\right)_{ij}$，且$m=l+u$则\n$$\n\\begin{aligned}\nE(f)&=\\sum_{i=1}^{m} d_{i} f^{2}\\left(\\boldsymbol{x}_{i}\\right)-\\sum_{i=1}^{m} \\sum_{j=1}^{m}(\\mathbf{W})_{i j} f\\left(\\boldsymbol{x}_{i}\\right) f\\left(\\boldsymbol{x}_{j}\\right)\\\\\n&=\\boldsymbol{f}^{\\mathrm{T}}\\mathbf{D}\\boldsymbol{f}-\\boldsymbol{f}^{\\mathrm{T}}\\mathbf{W}\\boldsymbol{f}\\\\\n&=\\boldsymbol{f}^{\\mathrm{T}}(\\mathbf{D}-\\mathbf{W}) \\boldsymbol{f}\n\\end{aligned}\n$$\n\n## 13.13\n\n$$\n\\begin{aligned}\nE(f) &=\\left(\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{f}_{u}^{\\mathrm{T}}\\right)\\left(\\left[\\begin{array}{ll}\n\\mathbf{D}_{l l} & \\mathbf{0}_{l u} \\\\\n\\mathbf{0}_{u l} & \\mathbf{D}_{u u}\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n\\mathbf{W}_{l l} & \\mathbf{W}_{l u} \\\\\n\\mathbf{W}_{u l} & \\mathbf{W}_{u u}\n\\end{array}\\right]\\right)\\left[\\begin{array}{l}\n\\boldsymbol{f}_{l} \\\\\n\\boldsymbol{f}_{u}\n\\end{array}\\right] \\\\\n&=\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\mathbf{D}_{l l}-\\mathbf{W}_{l l}\\right) \\boldsymbol{f}_{l}-2 \\boldsymbol{f}_{u}^{\\mathrm{T}} \\mathbf{W}_{u l} \\boldsymbol{f}_{l}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\mathbf{D}_{u u}-\\mathbf{W}_{u u}\\right) \\boldsymbol{f}_{u}\n\\end{aligned}\n$$\n\n[解析]：这里第一项西瓜书中的符号有歧义，应该表示成$\\left[\\begin{array}{ll}\n\\boldsymbol{f}_{l}^{\\mathrm{T}} & \\boldsymbol{f}_{u}^{\\mathrm{T}}\n\\end{array}\\right]$即一个$\\mathbb{R}^{1\\times(l+u)}$的行向量，根据矩阵乘法的定义，有：\n$$\n\\begin{aligned}\nE(f) &=\\left[\\begin{array}{ll}\n\\boldsymbol{f}_{l}^{\\mathrm{T}} & \\boldsymbol{f}_{u}^{\\mathrm{T}}\n\\end{array}\\right]\\left[\\begin{array}{cc}\n\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l} & -\\boldsymbol{W}_{l u} \\\\\n-\\boldsymbol{W}_{u l} & \\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\boldsymbol{f}_{l} \\\\\n\\boldsymbol{f}_{u}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{ll}\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l}\\right)-\\boldsymbol{f}_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l} & -\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{W}_{l u}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right)\\end{array}\\right]\\left[\\begin{array}{l}\n\\boldsymbol{f}_{l} \\\\\n\\boldsymbol{f}_{u}\n\\end{array}\\right] \\\\\n&=\\left(\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l}\\right)-\\boldsymbol{f}_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l}\\right) \\boldsymbol{f}_{l}+\\left(-\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{W}_{l u}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right)\\right) \\boldsymbol{f}_{u} \\\\\n&=\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l}\\right) \\boldsymbol{f}_{l}-\\boldsymbol{f}_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l} \\boldsymbol{f}_{l}-\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{W}_{l u} \\boldsymbol{f}_{u}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right) \\boldsymbol{f}_{u} \\\\\n&=\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l}\\right) \\boldsymbol{f}_{l}-2 \\boldsymbol{f}_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l} \\boldsymbol{f}_{l}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right) \\boldsymbol{f}_{u}\n\\end{aligned}\n$$\n其中最后一步，$\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{W}_{l u} \\boldsymbol{f}_{u}=\\left(\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{W}_{l u} \\boldsymbol{f}_{u}\\right)^{\\mathrm{T}}=f_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l} \\boldsymbol{f}_{l}$，因为这个式子的结果是一个标量。\n\n## 13.14\n\n$$\n\\begin{aligned}\nE(f) &=\\left(\\boldsymbol{f}_{l}^{\\mathrm{T}} \\boldsymbol{f}_{u}^{\\mathrm{T}}\\right)\\left(\\left[\\begin{array}{ll}\n\\mathbf{D}_{l l} & \\mathbf{0}_{l u} \\\\\n\\mathbf{0}_{u l} & \\mathbf{D}_{u u}\n\\end{array}\\right]-\\left[\\begin{array}{ll}\n\\mathbf{W}_{l l} & \\mathbf{W}_{l u} \\\\\n\\mathbf{W}_{u l} & \\mathbf{W}_{u u}\n\\end{array}\\right]\\right)\\left[\\begin{array}{l}\n\\boldsymbol{f}_{l} \\\\\n\\boldsymbol{f}_{u}\n\\end{array}\\right] \\\\\n&=\\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\mathbf{D}_{l l}-\\mathbf{W}_{l l}\\right) \\boldsymbol{f}_{l}-2 \\boldsymbol{f}_{u}^{\\mathrm{T}} \\mathbf{W}_{u l} \\boldsymbol{f}_{l}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\mathbf{D}_{u u}-\\mathbf{W}_{u u}\\right) \\boldsymbol{f}_{u}\n\\end{aligned}\n$$\n\n\n\n[解析]：参考 13.13\n\n## 13.15\n\n$$\n\\boldsymbol{f}_{u}=\\left(\\mathbf{D}_{u u}-\\mathbf{W}_{u u}\\right)^{-1} \\mathbf{W}_{u l} \\boldsymbol{f}_{l}\n$$\n\n[解析]：由 13.13，有\n$$\n\\begin{aligned}\n\\frac{\\partial E(f)}{\\partial \\boldsymbol{f}_{u}} &=\\frac{\\partial \\boldsymbol{f}_{l}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{l l}-\\boldsymbol{W}_{l l}\\right) \\boldsymbol{f}_{l}-2 \\boldsymbol{f}_{u}^{\\mathrm{T}} \\boldsymbol{W}_{u l} \\boldsymbol{f}_{l}+\\boldsymbol{f}_{u}^{\\mathrm{T}}\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right) \\boldsymbol{f}_{u}}{\\partial \\boldsymbol{f}_{u}} \\\\\n&=-2 \\boldsymbol{W}_{u l} \\boldsymbol{f}_{l}+2\\left(\\boldsymbol{D}_{u u}-\\boldsymbol{W}_{u u}\\right) \\boldsymbol{f}_{u}\n\\end{aligned}\n$$\n令结果等于 0 即得 13.15。\n\n## 13.16\n\n$$\n\\begin{aligned}\n\\mathbf{P} &=\\mathbf{D}^{-1} \\mathbf{W}=\\left[\\begin{array}{cc}\n\\mathbf{D}_{l l}^{-1} & \\mathbf{0}_{l u} \\\\\n\\mathbf{0}_{u l} & \\mathbf{D}_{u u}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{ll}\n\\mathbf{W}_{l l} & \\mathbf{W}_{l u} \\\\\n\\mathbf{W}_{u l} & \\mathbf{W}_{u u}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{ll}\n\\mathbf{D}_{l l}^{-1} \\mathbf{W}_{l l} & \\mathbf{D}_{l l}^{-1} \\mathbf{W}_{l u} \\\\\n\\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u l} & \\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u u}\n\\end{array}\\right]\n\\end{aligned}\n$$\n\n[解析]：根据矩阵乘法的定义计算可得该式，其中需要注意的是，对角矩阵$\\mathbf{D}$的拟等于其各个对角元素的倒数。\n\n## 13.17\n\n$$\n\\begin{aligned}\n\\boldsymbol{f}_{u} &=\\left(\\mathbf{D}_{u u}\\left(\\mathbf{I}-\\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u u}\\right)\\right)^{-1} \\mathbf{W}_{u l} \\boldsymbol{f}_{l} \\\\\n&=\\left(\\mathbf{I}-\\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u u}\\right)^{-1} \\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u l} \\boldsymbol{f}_{l} \\\\\n&=\\left(\\mathbf{I}-\\mathbf{P}_{u u}\\right)^{-1} \\mathbf{P}_{u l} \\boldsymbol{f}_{l}\n\\end{aligned}\n$$\n\n[解析]：第一项到第二项是根据矩阵乘法逆的定义：$(\\mathbf{A}\\mathbf{B})^{-1}=\\mathbf{B}^{-1}\\mathbf{A}^{-1}$，在这个式子中​\n$$\n\\begin{aligned}\n\\mathbf{P}_{u u}&=\\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u u}\\\\\n\\mathbf{P}_{ul}&=\\mathbf{D}_{u u}^{-1} \\mathbf{W}_{u l}\n\\end{aligned}\n$$\n均可以根据$\\mathbf{W}_{ij}$计算得到，因此可以通过标记$\\mathbf{f}_l$计算未标记数据的标签$\\mathbf{f}_u$。\n\n## 13.20\n\n$$\n\\mathbf{F}^{*}=\\lim _{t \\rightarrow \\infty} \\mathbf{F}(t)=(1-\\alpha)(\\mathbf{I}-\\alpha \\mathbf{S})^{-1} \\mathbf{Y}\n$$\n\n[解析]：由 13.19\n$$\n\\mathbf{F}(t+1)=\\alpha \\mathbf{S} \\mathbf{F}(t)+(1-\\alpha) \\mathbf{Y}\n$$\n当 t取不同的值时，有：\n$$\n\\begin{aligned}\nt=0: \\mathbf{F}(1) &=\\alpha \\mathbf{S F}(0)+(1-\\alpha) \\mathbf{Y}\\\\\n&=\\alpha \\mathbf{S} \\mathbf{Y}+(1-\\alpha) \\mathbf{Y} \\\\\nt=1: \\mathbf{F}(2) &=\\alpha \\mathbf{S F}(1)+(1-\\alpha) \\mathbf{Y}=\\alpha \\mathbf{S}(\\alpha \\mathbf{S} \\mathbf{Y}+(1-\\alpha) \\mathbf{Y})+(1-\\alpha) \\mathbf{Y} \\\\\n&=(\\alpha \\mathbf{S})^{2} \\mathbf{Y}+(1-\\alpha)\\left(\\sum_{i=0}^{1}(\\alpha \\mathbf{S})^{i}\\right) \\mathbf{Y} \\\\\nt=2:\\mathbf{F}(3)&=\\alpha\\mathbf{S}\\mathbf{F}(2)+(1-\\alpha)\\mathbf{Y}\\\\&=\\alpha \\mathbf{S}\\left((\\alpha \\mathbf{S})^{2} \\mathbf{Y}+(1-\\alpha)\\left(\\sum_{i=0}^{1}(\\alpha \\mathbf{S})^{i}\\right) \\mathbf{Y}\\right)+(1-\\alpha) \\mathbf{Y} \\\\\n&=(\\alpha \\mathbf{S})^{3} \\mathbf{Y}+(1-\\alpha)\\left(\\sum_{i=0}^{2}(\\alpha \\mathbf{S})^{i}\\right) \\mathbf{Y}\\\\\n\\end{aligned}\n$$\n可以观察到规律\n$$\n\\mathbf{F}(t)=(\\alpha \\mathbf{S})^{t} \\mathbf{Y}+(1-\\alpha)\\left(\\sum_{i=0}^{t-1}(\\alpha \\mathbf{S})^{i}\\right) \\mathbf{Y}\n$$\n则\n$$\n\\mathbf{F}^{*}=\\lim _{t \\rightarrow \\infty}\\mathbf{F}(t)=\\lim _{t \\rightarrow \\infty}(\\alpha \\mathbf{S})^{t} \\mathbf{Y}+\\lim _{t \\rightarrow \\infty}(1-\\alpha)\\left(\\sum_{i=0}^{t-1}(\\alpha \\mathbf{S})^{i}\\right) \\mathbf{Y}\n$$\n其中第一项由于$\\mathbf{S}=\\mathbf{D}^{-\\frac{1}{2}} \\mathbf{W} \\mathbf{D}^{-\\frac{1}{2}}$的特征值介于[-1, 1]之间(这里省略详细推导，可以参见 https://en.wikipedia.org/wiki/Laplacian_matrix 其中对称拉普拉斯矩阵的特征值介于 0 和 2 之间)，而$\\alpha\\in(0,1)$，所以$\\lim _{t \\rightarrow \\infty}(\\alpha \\mathbf{S})^{t}=0$，第二项由等比数列公式\n$$\n\\lim _{t \\rightarrow \\infty} \\sum_{i=0}^{t-1}(\\alpha \\mathbf{S})^{i}=\\frac{\\mathbf{I}-\\lim _{t \\rightarrow \\infty}(\\alpha \\mathbf{S})^{t}}{\\mathbf{I}-\\alpha \\mathbf{S}}=\\frac{\\mathbf{I}}{\\mathbf{I}-\\alpha \\mathbf{S}}=(\\mathbf{I}-\\alpha \\mathbf{S})^{-1}\n$$\n综合可得式 13.20。\n\n\n\n','2021-12-10 12:43:49','2021-12-19 13:23:03');

INSERT INTO `learn_detail` (`id`, `learn_id`, `title`, `content`, `create_time`, `modify_time`)
VALUES
	(14,1,'chapter14','## 14.1\n\n$$\n\\begin{aligned}\nP\\left(x_{1}, y_{1}, \\ldots, x_{n}, y_{n}\\right)=P\\left(y_{1}\\right) P\\left(x_{1} | y_{1}\\right) \\prod_{i=2}^{n} P\\left(y_{i} | y_{i-1}\\right) P\\left(x_{i} | y_{i}\\right)\n\\end{aligned}\n$$\n\n[解析]：所有的相乘关系都表示概率的相互独立。三种概率$P\\left(y_{i}\\right), P\\left(x_{i} | y_{i}\\right), P\\left(y_{i} | y_{i-1}\\right)$ 分别表示初始状态概率，输出观测概率和条件转移概率。\n\n## 14.2\n\n$$\nP(\\mathbf{x})=\\frac{1}{Z} \\prod_{Q \\in C} \\psi_{Q}\\left(\\mathbf{x}_{Q}\\right)\n$$\n\n[解析]：因为各个团之间概率分布相互独立，因此它们连乘可以表示最终的概率。\n\n## 14.3\n\n$$\nP(\\mathbf{x})=\\frac{1}{Z^*} \\prod_{Q \\in C*} \\psi_{Q}\\left(\\mathbf{x}_{Q}\\right)\n$$\n\n[解析]：意义同式$14.2$, 区别在于此处的团为极大团。\n\n## 14.4\n\n$$\nP\\left(x_{A}, x_{B}, x_{C}\\right)=\\frac{1}{Z} \\psi_{A C}\\left(x_{A}, x_{C}\\right) \\psi_{B C}\\left(x_{B}, x_{C}\\right)\n$$\n\n[解析]：将图$14.3$分解成$x_{A}, x_{C}$ 和 $x_{B}, x_{C}$ 两个团。\n\n## 14.5\n\n$$\nP\\left(x_{A}, x_{B} | x_{C}\\right) =\\frac{\\psi_{A C}\\left(x_{A}, x_{C}\\right)}{\\sum_{x_{A}^{\\prime}} \\psi_{A C}\\left(x_{A}^{\\prime}, x_{C}\\right)} \\cdot \\frac{\\psi_{B C}\\left(x_{B}, x_{C}\\right)}{\\sum_{x_{B}^{\\prime}} \\psi_{B C}\\left(x_{B}^{\\prime}, x_{C}\\right)}\n$$\n\n[推导]：参见原书推导。\n\n## 14.6\n\n$$\nP\\left(x_{A} | x_{C}\\right) =\\frac{\\psi_{A C}\\left(x_{A,} x_{C}\\right)}{\\sum_{x_{A}} \\psi_{A C}\\left(x_{A}^{\\prime}, x_{C}\\right)}\n$$\n\n[推导]：参见原书推导。\n\n## 14.7\n\n$$\nP\\left(x_{A}, x_{B} | x_{C}\\right)=P\\left(x_{A} | x_{C}\\right) P\\left(x_{B} | x_{C}\\right)\n$$\n\n[解析]：可由14.5、14.6联立可得。\n\n## 14.8\n\n$$\n\\psi_{Q}\\left(\\mathbf{x}_{Q}\\right)=e^{-H_{Q}\\left(\\mathbf{x}_{Q}\\right)}\n$$\n\n[解析]：此为势函数的定义式，即将势函数写作指数函数的形式。指数函数满足非负性，且便于求导，因此在机器学习中具有广泛应用，例如西瓜书公式8.5和13.11。\n\n## 14.9\n\n$$\nH_{Q}\\left(\\mathbf{x}_{Q}\\right)=\\sum_{u, v \\in Q, u \\neq v} \\alpha_{u v} x_{u} x_{v}+\\sum_{v \\in Q} \\beta_{v} x_{v}\n$$\n\n[解析]：此为定义在变量$\\mathbf{x}_{Q}$上的函数$H_{Q}\\left(\\cdot\\right)$的定义式，第二项考虑单节点，第一项考虑每一对节点之间的关系。\n\n## 14.10\n\n$$\nP\\left(y_{v} | \\mathbf{x}, \\mathbf{y}_{V \\backslash\\{v\\}}\\right)=P\\left(y_{v} | \\mathbf{x}, \\mathbf{y}_{n(v)}\\right)\n$$\n\n[解析]：根据局部马尔科夫性，给定某变量的邻接变量，则该变量独立与其他变量，即该变量只与其邻接变量有关，所以式$14.10$中给定变量$v$ 以外的所有变量与仅给定变量$v$的邻接变量是等价的。\n\n## 14.14\n\n$$\n\\begin{aligned}\nP\\left(x_{5}\\right) &=\\sum_{x_{4}} \\sum_{x_{3}} \\sum_{x_{2}} \\sum_{x_{1}} P\\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\\right) \\\\\n&=\\sum_{x_{4}} \\sum_{x_{3}} \\sum_{x_{2}} \\sum_{x_{1}} P\\left(x_{1}\\right) P\\left(x_{2} | x_{1}\\right) P\\left(x_{3} | x_{2}\\right) P\\left(x_{4} | x_{3}\\right) P\\left(x_{5} | x_{3}\\right)\n\\end{aligned}\n$$\n\n[解析]：在消去变量的过程中，在消去每一个变量时需要保证其依赖的变量已经消去，因此消去顺序应该是有向概率图中的一条以目标节点为终点的拓扑序列。\n\n## 14.15\n\n$$\n\\begin{aligned}\nP\\left(x_{5}\\right) &=\\sum_{x_{3}} P\\left(x_{5} | x_{3}\\right) \\sum_{x_{4}} P\\left(x_{4} | x_{3}\\right) \\sum_{x_{2}} P\\left(x_{3} | x_{2}\\right) \\sum_{x_{1}} P\\left(x_{1}\\right) P\\left(x_{2} | x_{1}\\right)\\\\\n&=\\sum_{x_{3}} P\\left(x_{5} | x_{3}\\right) \\sum_{x_{4}} P\\left(x_{4} | x_{3}\\right) \\sum_{x_{2}} P\\left(x_{3} | x_{2}\\right) m_{12}\\left(x_{2}\\right)\n\\end{aligned}\n$$\n\n[解析]：变量消去的顺序为从右至左求和号的下标，应当注意$x_4$与$x_5$相互独立，因此可与$x_3$的消去顺序互换，对最终结果无影响。\n\n## 14.16\n\n$$\n\\begin{aligned}\nP\\left(x_{5}\\right) &=\\sum_{x_{3}} P\\left(x_{5} | x_{3}\\right) \\sum_{x_{4}} P\\left(x_{4} | x_{3}\\right) m_{23}\\left(x_{3}\\right) \\\\\n&=\\sum_{x_{3}} P\\left(x_{5} | x_{3}\\right) m_{23}\\left(x_{3}\\right) \\sum_{x_{4}} P\\left(x_{4} | x_{3}\\right) \\\\\n&=\\sum_{x_{3}} P\\left(x_{5} | x_{3}\\right) m_{23}\\left(x_{3}\\right) \\\\\n&=m_{35}\\left(x_{5}\\right)\n\\end{aligned}\n$$\n\n[解析]：注意到$\\sum_{x_{4}} P\\left(x_{4} | x_{3}\\right) = 1$。\n\n## 14.17\n\n$$\nP\\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\\right)=\\frac{1}{Z} \\psi_{12}\\left(x_{1}, x_{2}\\right) \\psi_{23}\\left(x_{2}, x_{3}\\right) \\psi_{34}\\left(x_{3}, x_{4}\\right) \\psi_{35}\\left(x_{3}, x_{5}\\right)\n$$\n\n[解析]：忽略图$14.7(a)$中的箭头，然后把无向图中的每条边的两个端点作为一个团将其分解为四个团因子的乘积。$Z$为规范化因子确保所有可能性的概率之和为$1$。\n\n## 14.18\n\n$$\n\\begin{aligned}\nP\\left(x_{5}\\right) &=\\frac{1}{Z} \\sum_{x_{3}} \\psi_{35}\\left(x_{3}, x_{5}\\right) \\sum_{x_{4}} \\psi_{34}\\left(x_{3}, x_{4}\\right) \\sum_{x_{2}} \\psi_{23}\\left(x_{2}, x_{3}\\right) \\sum_{x_{1}} \\psi_{12}\\left(x_{1}, x_{2}\\right) \\\\\n&=\\frac{1}{Z} \\sum_{x_{3}} \\psi_{35}\\left(x_{3}, x_{5}\\right) \\sum_{x_{4}} \\psi_{34}\\left(x_{3}, x_{4}\\right) \\sum_{x_{2}} \\psi_{23}\\left(x_{2}, x_{3}\\right) m_{12}\\left(x_{2}\\right) \\\\\n&=\\cdots \\\\\n&=\\frac{1}{Z} m_{35}\\left(x_{5}\\right)\n\\end{aligned}\n$$\n\n[解析]：原理同式$14.15$, 区别在于把条件概率替换为势函数。\n\n## 14.19\n\n$$\nm_{i j}\\left(x_{j}\\right)=\\sum_{x_{i}} \\psi\\left(x_{i}, x_{j}\\right) \\prod_{k \\in n(i) \\backslash j} m_{k i}\\left(x_{i}\\right)\n$$\n\n[解析]：该式表示从节点$i$传递到节点$j$的过程，求和号表示要考虑节点$i$的所有可能取值。连乘号解释见式$14.20$。应当注意这里连乘号的下标不包括节点$j$，节点$i$只需要把自己知道的关于$j$以外的消息告诉节点$j$即可。\n\n## 14.20\n\n$$\nP\\left(x_{i}\\right) \\propto \\prod_{k \\in n(i)} m_{k i}\\left(x_{i}\\right)\n$$\n\n[解析]：应当注意这里是正比于而不是等于，因为涉及到概率的规范化。可以这么解释，每个变量可以看作一个有一些邻居的房子，每个邻居根据其自己的见闻告诉你一些事情(消息)，任何一条消息的可信度应当与所有邻居都有相关性，此处这种相关性用乘积来表达。【引用http://helper.ipam.ucla.edu/publications/gss2013/gss2013_11344.pdf】\n\n## 14.22\n\n$$\n\\hat{f}=\\frac{1}{N} \\sum_{i=1}^{N} f\\left(x_{i}\\right)\n$$\n\n[推导]：假设$x$有M种不同的取值，$x_i$的采样数量为$m_i$(连续取值可以采用微积分的方法分割为离散的取值)，则\n$$\n\\begin{aligned}\n\\hat{f}&=\\frac{1}{N} \\sum_{j=1}^{M} f\\left(x_{j}\\right) \\cdot m_j \\\\\n&= \\sum_{j=1}^{M} f\\left(x_{j}\\right)\\cdot \\frac{m_j}{N} \\\\\n&\\approx \\sum_{j=1}^{M} f\\left(x_{j}\\right)\\cdot p(x_j)  \\\\\n&\\approx \\int f(x) p(x) dx\n\\end{aligned}\n$$\n\n## 14.26\n\n$$\np\\left(\\mathbf{x}^{t}\\right) T\\left(\\mathbf{x}^{t-1} \\mid \\mathbf{x}^{t}\\right)=p\\left(\\mathbf{x}^{t-1}\\right) T\\left(\\mathbf{x}^{t} \\mid \\mathbf{x}^{t-1}\\right)\n$$\n\n\n\n[解析]：假设变量$\\mathbf{x}$所在的空间有$n$个状态($s_1,s_2,..,s_n$), 定义在该空间上的一个转移矩阵$\\mathbf{T}\\in\\mathbb{R}^{n\\times n}$满足一定的条件则该马尔可夫过程存在一个稳态分布$\\boldsymbol{\\pi}$, 使得\n$$\n\\begin{aligned}\n\\boldsymbol{\\pi} \\mathbf{T}=\\boldsymbol{\\pi}\n\\end{aligned}\n$$\n其中, $\\boldsymbol{\\pi}$是一个是一个$n$维向量，代表$s_1,s_2,..,s_n$对应的概率. 反过来, 如果我们希望采样得到符合某个分布$\\boldsymbol{\\pi}$的一系列变量$\\mathbf{x}^1,\\mathbf{x}^2,..,\\mathbf{x}^t$, 应当采用哪一个转移矩阵$\\mathbf{T}\\in\\mathbb{R}^{n\\times n}$呢？\n\n事实上，转移矩阵只需要满足马尔可夫细致平稳条件\n$$\n\\begin{aligned}\n\\pi_i \\mathbf{T}_{ij}=\\pi_j \\mathbf{T}_{ji}\n\\end{aligned}\n$$\n即公式$14.26$，这里采用的符号与西瓜书略有区别以便于理解.  证明如下\n$$\n\\begin{aligned}\n\\boldsymbol{\\pi} \\mathbf{T}_{j\\cdot} = \\sum _i \\pi_i\\mathbf{T}_{ij} = \\sum _i \\pi_j\\mathbf{T}_{ji} = \\pi_j\n\\end{aligned}\n$$\n假设采样得到的序列为$\\mathbf{x}^1,\\mathbf{x}^2,..,\\mathbf{x}^{t-1},\\mathbf{x}^t$，则可以使用$MH$算法来使得$\\mathbf{x}^{t-1}$(假设为状态$s_i$)转移到$\\mathbf{x}^t$(假设为状态$s_j$)的概率满足式。\n\n## 14.27\n\n$$\np\\left(\\mathbf{x}^{t-1}\\right) Q\\left(\\mathbf{x}^{*} | \\mathbf{x}^{t-1}\\right) A\\left(\\mathbf{x}^{*} | \\mathbf{x}^{t-1}\\right)=p\\left(\\mathbf{x}^{*}\\right) Q\\left(\\mathbf{x}^{t-1} | \\mathbf{x}^{*}\\right) A\\left(\\mathbf{x}^{t-1} | \\mathbf{x}^{*}\\right)\n$$\n\n[解析]：这里把式$14.26$中的函数$T$ 拆分为两个函数$Q$和$A$之积，即先验概率和接受概率，便于实际算法的实现。\n\n## 14.28\n\n$$A(x^* | x^{t-1}) = \\min\\left ( 1,\\frac{p(x^*)Q(x^{t-1} | x^*) }{p(x^{t-1})Q(x^* | x^{t-1})} \\right )$$\n\n[推导]：这个公式其实是拒绝采样的一个trick，因为基于式$14.27$只需要\n$$\n\\begin{aligned}\n  A(x^* | x^{t-1}) &= p(x^*)Q(x^{t-1} | x^*)  \\\\\n  A(x^{t-1} | x^*) &= p(x^{t-1})Q(x^* | x^{t-1})\n \\end{aligned} \n$$\n即可满足式$14.26$，但是实际上等号右边的数值可能比较小，比如各为0.1和0.2，那么好不容易才到的样本只有百分之十几得到利用，所以不妨将接受率设为0.5和1，则细致平稳分布条件依然满足，样本利用率大大提高, 所以可以改进为\n$$\n\\begin{aligned} \nA(x^* | x^{t-1}) &=  \\frac{p(x^*)Q(x^{t-1} | x^*)}{norm}  \\\\  \nA(x^{t-1} | x^*) &= \\frac{p(x^{t-1})Q(x^* | x^{t-1}) }{norm}\n\\end{aligned} \n$$\n其中\n$$\n\\begin{aligned} \nnorm = \\max\\left (p(x^{t-1})Q(x^* | x^{t-1}),p(x^*)Q(x^{t-1} | x^*) \\right )\n\\end{aligned}  \n$$\n即西瓜书中的$14.28$。\n\n## 14.29\n\n$$\np(\\mathbf{x} | \\Theta)=\\prod_{i=1}^{N} \\sum_{\\mathbf{z}} p\\left(x_{i}, \\mathbf{z} | \\Theta\\right)\n$$\n\n[解析]：连乘号是因为$N$个变量的生成过程相互独立。求和号是因为每个变量的生成过程需要考虑中间隐变量的所有可能性，类似于边际分布的计算方式。\n\n## 14.30\n\n$$\n\\ln p(\\mathbf{x} | \\Theta)=\\sum_{i=1}^{N} \\ln \\left\\{\\sum_{\\mathbf{z}} p\\left(x_{i}, \\mathbf{z} | \\Theta\\right)\\right\\}\n$$\n\n[解析]：对式$14.29$取对数。\n\n## 14.31\n\n$$\n\\begin{aligned}\n\\Theta^{t+1} &=\\underset{\\Theta}{\\arg \\max } \\mathcal{Q}\\left(\\Theta ; \\Theta^{t}\\right) \\\\\n&=\\underset{\\Theta}{\\arg \\max } \\sum_{\\mathbf{z}} p\\left(\\mathbf{z} | \\mathbf{x}, \\Theta^{t}\\right) \\ln p(\\mathbf{x}, \\mathbf{z} | \\Theta)\n\\end{aligned}\n$$\n\n[解析]：EM算法中的M步，参见$7.6$节。\n\n## 14.32\n\n$${\\rm ln}p(x)=\\mathcal{L}(q)+{\\rm KL}(q \\parallel p)$$ \n\n[推导]：根据条件概率公式$p(x,z)=p(z|x)*p(x)$，可以得到$p(x)=\\frac{p(x,z)}{p(z|x)}$\n\n然后两边同时作用${\\rm ln}$函数，可得${\\rm ln}p(x)={\\rm ln}\\frac{p(x,z)}{p(z|x)}$    (1)\n\n因为$q(z)$是概率密度函数，所以$1=\\int q(z)dz$\n\n等式两边同时乘以${\\rm ln}p(x)$，因为${\\rm ln}p(x)$是不关于变量$z$的函数，所以${\\rm ln}p(x)$可以拿进积分里面，得到${\\rm ln}p(x)=\\int q(z){\\rm ln}p(x)dz$\n$$\n\\begin{aligned}\n{\\rm ln}p(x)&=\\int q(z){\\rm ln}p(x)dz \\\\\n &=\\int q(z){\\rm ln}\\frac{p(x,z)}{p(z|x)}\\\\\n &=\\int q(z){\\rm ln}\\bigg\\{\\frac{p(x,z)}{q(z)}\\cdot\\frac{q(z)}{p(z|x)}\\bigg\\} \\\\\n &=\\int q(z)\\bigg({\\rm ln}\\frac{p(x,z)}{q(z)}-{\\rm ln}\\frac{p(z|x)}{q(z)}\\bigg) \\\\\n  &=\\int q(z){\\rm ln}\\bigg\\{\\frac{p(x,z)}{q(z)}\\bigg\\}-\\int q(z){\\rm ln}\\frac{p(z|x)}{q(z)} \\\\\n  &=\\mathcal{L}(q)+{\\rm KL}(q \\parallel p)\\qquad\n\\end{aligned}\n$$\n最后一行是根据$\\mathcal{L}$和${\\rm KL}$的定义。\n## 14.33\n\n$$\n\\mathcal{L}(q)=\\int q(\\mathbf{z}) \\ln \\left\\{\\frac{p(\\mathbf{x}, \\mathbf{z})}{q(\\mathbf{z})}\\right\\} \\mathrm{d} \\mathbf{z}\n$$\n\n\n\n[解析]：见$14.32$解析。\n\n## 14.34\n\n$$\n\\mathrm{KL}(q \\| p)=-\\int q(\\mathrm{z}) \\ln \\frac{p(\\mathrm{z} | \\mathrm{x})}{q(\\mathrm{z})} \\mathrm{d} \\mathrm{z}\n$$\n\n\n\n[解析]：见$14.32$解析。\n\n## 14.35\n\n$$\nq(\\mathbf{z})=\\prod_{i=1}^{M} q_{i}\\left(\\mathbf{z}_{i}\\right)\n$$\n\n[解析]：再一次，条件独立的假设。可以看到，当问题复杂是往往简化问题到最简单最容易计算的局面，实际上往往效果不错。\n\n## 14.36\n$$\n\\begin{aligned}\n\\mathcal{L}(q)&=\\int \\prod_{i}q_{i}\\bigg\\{ {\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})-\\sum_{i}{\\rm ln}q_{i}\\bigg\\}d{\\rm\\mathbf{z}} \\\\\n&=\\int q_{j}\\bigg\\{\\int p(x,z)\\prod_{i\\ne j}q_{i}d{\\rm\\mathbf{z_{i}}}\\bigg\\}d{\\rm\\mathbf{z_{j}}}-\\int q_{j}{\\rm ln}q_{j}d{\\rm\\mathbf{z_{j}}}+{\\rm const} \\\\\n&=\\int q_{j}{\\rm ln}\\tilde{p}({\\rm \\mathbf{x},\\mathbf{z_{j}}})d{\\rm\\mathbf{z_{j}}}-\\int q_{j}{\\rm ln}q_{j}d{\\rm\\mathbf{z_{j}}}+{\\rm const}\n\\end{aligned}\n$$\n[推导]：\n$$\n\\mathcal{L}(q)=\\int \\prod_{i}q_{i}\\bigg\\{ {\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})-\\sum_{i}{\\rm ln}q_{i}\\bigg\\}d{\\rm\\mathbf{z}}=\\int\\prod_{i}q_{i}{\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})d{\\rm\\mathbf{z}}-\\int\\prod_{i}q_{i}\\sum_{i}{\\rm ln}q_{i}d{\\rm\\mathbf{z}}\n$$\n公式可以看做两个积分相减，我们先来看左边积分$\\int\\prod_{i}q_{i}{\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})d{\\rm\\mathbf{z}}$的推导。\n$$\n\\begin{aligned}\n\\int\\prod_{i}q_{i}{\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})d{\\rm\\mathbf{z}} &= \\int q_{j}\\prod_{i\\ne j}q_{i}{\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})d{\\rm\\mathbf{z}} \\\\\n&= \\int q_{j}\\bigg\\{\\int{\\rm ln}p({\\rm \\mathbf{x},\\mathbf{z}})\\prod_{i\\ne j}q_{i}d{\\rm\\mathbf{z_{i}}}\\bigg\\}d{\\rm\\mathbf{z_{j}}}\\qquad \n\\end{aligned}\n$$\n即先对$\\rm\\mathbf{z_{j}}$求积分，再对$\\rm\\mathbf{z_{i}}$求积分，这个就是教材中的$14.36$左边的积分部分。\n我们现在看下右边积分的推导$\\int\\prod_{i}q_{i}\\sum_{i}{\\rm ln}q_{i}d{\\rm\\mathbf{z}}$的推导。\n在此之前我们看下$\\int\\prod_{i}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z}}$的计算\n$$\n\\begin{aligned}\n\\int\\prod_{i}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z}}&= \\int q_{i^{\\prime}}\\prod_{i\\ne i^{\\prime}}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z}}\\qquad  \\\\\n&=\\int q_{i^{\\prime}}\\bigg\\{\\int\\prod_{i\\ne i^{\\prime}}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z_{i}}}\\bigg\\}d{\\rm\\mathbf{z_{i^{\\prime}}}}\n\\end{aligned}\n$$\n第一个等式是一个展开项，选取一个变量$q_{i^{\\prime}}, i^{\\prime}\\ne k$，由于\n$\\bigg\\{\\int\\prod_{i\\ne i^{\\prime}}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z_{i}}}\\bigg\\}$部分与变量$q_{i^{\\prime}}$无关，所以可以拿到积分外面。又因为$\\int q_{i^{\\prime}}d{\\rm\\mathbf{z_{i^{\\prime}}}}=1$，所以\n$$\n\\begin{aligned}\n\\int\\prod_{i}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z}}&=\\int\\prod_{i\\ne i^{\\prime}}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z_{i}}} \\\\\n&= \\int q_{k}{\\rm ln}q_{k}d{\\rm\\mathbf{z_k}}\\qquad \n\\end{aligned}\n$$\n即所有$k$以外的变量都可以通过上面的方式消除,有了这个结论，我们再来看公式\n$$\n\\begin{aligned}\n\\int\\prod_{i}q_{i}\\sum_{i}{\\rm ln}q_{i}d{\\rm\\mathbf{z}}&= \\int\\prod_{i}q_{i}{\\rm ln}q_{j}d{\\rm\\mathbf{z}} + \\sum_{k\\ne j}\\int\\prod_{i}q_{i}{\\rm ln}q_{k}d{\\rm\\mathbf{z}} \\\\\n&= \\int q_{j}{\\rm ln}q_{j}d{\\rm\\mathbf{z_j}} + \\sum_{k\\ne j}\\int q_{k}{\\rm ln}q_{k}d{\\rm\\mathbf{z_k}}\\qquad \\\\\n&= \\int q_{j}{\\rm ln}q_{j}d{\\rm\\mathbf{z_j}} + {\\rm const} \\qquad\n\\end{aligned}\n$$\n其中第二个等式是依据上述规律进行消除，最后将与$q_j$无关的部分写作$\\rm const$，这个就是$14.36$右边的积分部分。\n## 14.37\n\n$$\n\\ln \\tilde{p}\\left(\\mathbf{x}, \\mathbf{z}_{j}\\right)=\\mathbb{E}_{i \\neq j}[\\ln p(\\mathbf{x}, \\mathbf{z})]+\\text { const }\n$$\n\n\n\n[解析]：参见14.36\n\n## 14.38\n\n$$\n\\mathbb{E}_{i \\neq j}[\\ln p(\\mathbf{x}, \\mathbf{z})]=\\int \\ln p(\\mathbf{x}, \\mathbf{z}) \\prod_{i \\neq j} q_{i} \\mathrm{d} \\mathbf{z}_{i}\n$$\n\n\n\n[解析]：参见14.36\n\n## 14.39\n\n$$\n\\ln q_{j}^{*}\\left(\\mathbf{z}_{j}\\right)=\\mathbb{E}_{i \\neq j}[\\ln p(\\mathbf{x}, \\mathbf{z})]+\\mathrm{const}\n$$\n\n[解析]：散度取得极值的条件是两个概率分布相同，见附录$C.3$。\n\n## 14.40\n\n$$\n\\begin{aligned} \nq_j^*(\\mathbf{z}_j) = \\frac{ \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) }{\\int \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) \\mathrm{d}\\mathbf{z}_j}\n\\end{aligned}\n$$\n\n[推导]：由$14.39$去对数并积分\n$$\n\\begin{aligned} \n \\int q_j^*(\\mathbf{z}_j)\\mathrm{d}\\mathbf{z}_j &=\\int \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right )\\cdot\\exp(const) \\, \\mathrm{d}\\mathbf{z}_j \\\\\n &=\\exp(const) \\int \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) \\, \\mathrm{d}\\mathbf{z}_j \\\\\n &= 1\n \\end{aligned}\n$$\n所以\n$$\n\\exp(const)  = \\dfrac{1}{\\int \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) \\, \\mathrm{d}\\mathbf{z}_j}  \\\\\n$$\n\n$$\n\\begin{aligned} \n  q_j^*(\\mathbf{z}_j) &= \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right )\\cdot\\exp(const)  \\\\\n &= \\frac{ \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) }{\\int \\exp\\left ( \\mathbb{E}_{i\\neq j}[\\ln (p(\\mathbf{x},\\mathbf{z}))] \\right ) \\mathrm{d}\\mathbf{z}_j}\n \\end{aligned}\n$$\n\n## 14.41\n\n$$\np(\\boldsymbol W,\\boldsymbol z,\\boldsymbol \\beta,\\boldsymbol \\theta | \\boldsymbol \\alpha,\\boldsymbol \\eta) =\n\\prod_{t=1}^{T}p(\\boldsymbol \\theta_t | \\boldsymbol \\alpha)\n\\prod_{k=1}^{K}p(\\boldsymbol \\beta_k | \\boldsymbol \\eta) \n(\\prod_{n=1}^{N}P(w_{t,n} | z_{t,n}, \\boldsymbol \\beta_k)P( z_{t,n} | \\boldsymbol \\theta_t))\n$$\n\n\n\n[解析]：此式表示LDA模型下根据参数$\\alpha, \\eta$生成文档$W$的概率。其中$z, \\beta, \\theta$是生成过程的中间变量。具体的生成步骤可见概率图14.12，图中的箭头和式14.41中的条件概率中的因果项目一一对应。这里共有三个连乘符号，表示三个相互独立的概率关系。第一个连乘表示T个文档每个文档的话题分布都是相互独立的。第二个连乘表示K个话题每个话题下单词的分布是相互独立的。最后一个连乘号表示每篇文档中的所有单词的生成是相互独立的。\n\n## 14.42\n\n$$\np\\left(\\Theta_{t} | \\boldsymbol{\\alpha}\\right)=\\frac{\\Gamma\\left(\\sum_{k} \\alpha_{k}\\right)}{\\prod_{k} \\Gamma\\left(\\alpha_{k}\\right)} \\prod_{k} \\Theta_{t, k}^{\\alpha_{k}-1}\n$$\n\n[解析]：参见附录$C1.6$。\n\n## 14.43\n\n$$\nL L(\\boldsymbol{\\alpha}, \\boldsymbol{\\eta})=\\sum_{t=1}^{T} \\ln p\\left(\\boldsymbol{w}_{t} | \\boldsymbol{\\alpha}, \\boldsymbol{\\eta}\\right)\n$$\n\n[解析]：对数似然函数。参见$7.2$极大似然估计。\n\n## 14.44\n\n$$\np(\\mathbf{z}, \\boldsymbol{\\beta}, \\Theta | \\mathbf{W}, \\boldsymbol{\\alpha}, \\boldsymbol{\\eta})=\\frac{p(\\mathbf{W}, \\mathbf{z}, \\boldsymbol{\\beta}, \\boldsymbol{\\Theta} | \\boldsymbol{\\alpha}, \\boldsymbol{\\eta})}{p(\\mathbf{W} | \\boldsymbol{\\alpha}, \\boldsymbol{\\eta})}\n$$\n\n[解析]：分母为边际分布，需要对变量$\\mathbf{z}, \\boldsymbol{\\beta}, \\Theta$ 积分或者求和，所以往往难以直接求解。','2021-12-10 12:43:49','2021-12-19 13:23:10'),
	(15,1,'chapter15','## 15.2\n\n$$\n\\mathrm{LRS}=2 \\cdot\\left(\\hat{m}_{+} \\log _{2} \\frac{\\left(\\frac{\\hat{m}_{+}}{\\hat{m}_{+}+\\hat{m}_{-}}\\right)}{\\left(\\frac{m_{+}}{m_{+}+m_{-}}\\right)}+\\hat{m}_{-} \\log _{2} \\frac{\\left(\\frac{\\hat{m}_{-}}{\\hat{m}_{+}+\\hat{m}_{-}}\\right)}{\\left(\\frac{m_{-}}{m_{+}+m_{-}}\\right)}\\right)\n$$\n\n[解析]：似然率统计量(Likelihood Ratio Statistics)的定义式。\n\n## 15.3\n\n$$\n\\mathrm{F}_{-} \\text {Gain }=\\hat{m}_{+} \\times\\left(\\log _{2} \\frac{\\hat{m}_{+}}{\\hat{m}_{+}+\\hat{m}_{-}}-\\log _{2} \\frac{m_{+}}{m_{+}+m_{-}}\\right)\n$$\n\n[解析]：FOIL增益(FOIL gain)的定义式。\n\n## 15.6\n\n$$\n(A \\vee B)-\\{B\\}=A\n$$\n\n[解析]：析合范式的删除操作定义式，表示在$A$和$B$的析合式中删除成分$B$，得到成分$A$。\n\n## 15.7\n\n$$\nC=\\left(C_{1}-\\{L\\}\\right) \\vee\\left(C_{2}-\\{\\neg L\\}\\right)\n$$\n\n[解析]：$C=A\\vee B$，把$A=C_1 - \\{L\\}$和$L=C_2-\\{\\neg L\\}$代入即得。\n\n## 15.9\n\n$$\nC_{2}=\\left(C-\\left(C_{1}-\\{L\\}\\right)\\right) \\vee\\{\\neg L\\}\n$$\n\n[解析]：由式15.7可知\n$$\nC_2-\\{\\neg L\\} = C - (C_1 - \\{L\\})\n$$\n由式15.6 移项即证得。\n\n## 15.10\n\n$$\n\\frac{p \\leftarrow A \\wedge B  \\quad q \\leftarrow A}{p \\leftarrow q \\wedge B  \\quad q \\leftarrow A}\n$$\n\n[解析]：吸收(absorption)操作的定义。\n\n## 15.11\n\n$$\n\\frac{p \\leftarrow A \\wedge B \\quad p \\leftarrow A \\wedge q}{q \\leftarrow B \\quad p \\leftarrow A \\wedge q}\n$$\n\n[解析]：辨识(identification)操作的定义。\n\n## 15.12\n\n$$\n\\frac{p \\leftarrow A \\wedge B\\quad p \\leftarrow A \\wedge q }{q \\leftarrow B\\quad p \\leftarrow A \\wedge q \\quad q \\leftarrow C} \n$$\n\n[解析]：内构(intra-construction)操作的定义。\n\n## 15.13\n\n$$\n\\frac{p \\leftarrow A \\wedge B\\quad q \\leftarrow r \\wedge C}{p \\leftarrow r \\wedge B\\quad r \\leftarrow A \\quad q \\leftarrow r \\wedge C} \n$$\n\n[解析]：互构(inter-construction)操作的定义。\n\n## 15.14\n\n$$\nC=\\left(C_{1}-\\left\\{L_{1}\\right\\}\\right) \\theta \\vee\\left(C_{2}-\\left\\{L_{2}\\right\\}\\right) \\theta\n$$\n\n[解析]：由式15.7，分别对析合的两个子项进行归结即得证。\n\n## 15.16\n\n$$\nC_{2}=\\left(C-\\left(C_{1}-\\left\\{L_{1}\\right\\}\\right) \\theta_{1} \\vee\\left\\{\\neg L_{1} \\theta_{1}\\right\\}\\right)\\theta_{2}^{-1}\n$$\n\n[推导]：$\\theta_1$为作者笔误，由15.9\n$$\n\\begin{aligned}\nC_{2}&=\\left(C-\\left(C_{1}-\\{L_1\\}\\right)\\right) \\vee\\{L_2\\}\\\\\n\\end{aligned}\n$$\n因为 $L_2=(\\neg L_1\\theta_1)\\theta_2^{-1}$，替换得证。\n\n\n\n','2021-12-10 12:43:49','2021-12-19 13:23:15'),
	(16,1,'chapter16','## 16.2\n$$\nQ_{n}(k)=\\frac{1}{n}\\left((n-1)\\times Q_{n-1}(k)+v_{n}\\right)\n$$\n\n[推导]：\n$$\n\\begin{aligned}\nQ_{n}(k)&=\\frac{1}{n}\\sum_{i=1}^{n}v_{i}\\\\\n&=\\frac{1}{n}\\left(\\sum_{i=1}^{n-1}v_{i}+v_{n}\\right)\\\\\n&=\\frac{1}{n}\\left((n-1)\\times Q_{n-1}(k)+v_{n}\\right)\\\\\n&=Q_{n-1}(k)+\\frac{1}{n}\\left(v_n-Q_{n-1}(k)\\right)\n\\end{aligned}\n$$\n\n## 16.3\n\n$$\n\\begin{aligned}\n&Q_{n}(k)=\\frac{1}{n}\\left((n-1) \\times Q_{n-1}(k)+v_{n}\\right)\\\\\n&=Q_{n-1}(k)+\\frac{1}{n}\\left(v_{n}-Q_{n-1}(k)\\right)\n\\end{aligned}\n$$\n\n\n\n[推导]：参见 16.2\n\n## 16.4\n\n$$\nP(k)=\\frac{e^{\\frac{Q(k)}{\\tau }}}{\\sum_{i=1}^{K}e^{\\frac{Q(i)}{\\tau}}}\n$$\n\n[解析]：\n$$\nP(k)=\\frac{e^{\\frac{Q(k)}{\\tau }}}{\\sum_{i=1}^{K}e^{\\frac{Q(i)}{\\tau}}}\\propto e^{\\frac{Q(k)}{\\tau }}\\propto\\frac{Q(k)}{\\tau }\\propto\\frac{1}{\\tau}\n$$\n\n## 16.7\n\n$$\n\\begin{aligned}\nV_{T}^{\\pi}(x)&=\\mathbb{E}_{\\pi}[\\frac{1}{T}\\sum_{t=1}^{T}r_{t}\\mid x_{0}=x]\\\\\n&=\\mathbb{E}_{\\pi}[\\frac{1}{T}r_{1}+\\frac{T-1}{T}\\frac{1}{T-1}\\sum_{t=2}^{T}r_{t}\\mid x_{0}=x]\\\\\n&=\\sum_{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(\\frac{1}{T}R_{x\\rightarrow x{}\'}^{a}+\\frac{T-1}{T}\\mathbb{E}_{\\pi}[\\frac{1}{T-1}\\sum_{t=1}^{T-1}r_{t}\\mid x_{0}=x{}\'])\\\\\n&=\\sum_{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(\\frac{1}{T}R_{x\\rightarrow x{}\'}^{a}+\\frac{T-1}{T}V_{T-1}^{\\pi}(x{}\')])\n\\end{aligned}\n$$\n\n[解析]：\n\n因为\n$$\n\\pi(x,a)=P(action=a|state=x)\n$$\n表示在状态$x$下选择动作$a$的概率，又因为动作事件之间两两互斥且和为动作空间，由全概率展开公式\n$$\nP(A)=\\sum_{i=1}^{\\infty}P(B_{i})P(A\\mid B_{i})\n$$\n可得\n$$\n\\begin{aligned}\n&\\mathbb{E}_{\\pi}[\\frac{1}{T}r_{1}+\\frac{T-1}{T}\\frac{1}{T-1}\\sum_{t=2}^{T}r_{t}\\mid x_{0}=x]\\\\\n&=\\sum_{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(\\frac{1}{T}R_{x\\rightarrow x{}\'}^{a}+\\frac{T-1}{T}\\mathbb{E}_{\\pi}[\\frac{1}{T-1}\\sum_{t=1}^{T-1}r_{t}\\mid x_{0}=x{}\'])\n\\end{aligned}\n$$\n其中\n$$\nr_{1}=\\pi(x,a)P_{x\\rightarrow x{}\'}^{a}R_{x\\rightarrow x{}\'}^{a}\n$$\n最后一个等式用到了递归形式。\n\n\n\n## 16.8\n\n$$\nV_{\\gamma }^{\\pi}(x)=\\sum _{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(R_{x\\rightarrow x{}\'}^{a}+\\gamma V_{\\gamma }^{\\pi}(x{}\'))\n$$\n\n[推导]：\n$$\n\\begin{aligned}\nV_{\\gamma }^{\\pi}(x)&=\\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty }\\gamma^{t}r_{t+1}\\mid x_{0}=x]\\\\\n&=\\mathbb{E}_{\\pi}[r_{1}+\\sum_{t=1}^{\\infty}\\gamma^{t}r_{t+1}\\mid x_{0}=x]\\\\\n&=\\mathbb{E}_{\\pi}[r_{1}+\\gamma\\sum_{t=1}^{\\infty}\\gamma^{t-1}r_{t+1}\\mid x_{0}=x]\\\\\n&=\\sum _{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(R_{x\\rightarrow x{}\'}^{a}+\\gamma \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty }\\gamma^{t}r_{t+1}\\mid x_{0}=x{}\'])\\\\\n&=\\sum _{a\\in A}\\pi(x,a)\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{a}(R_{x\\rightarrow x{}\'}^{a}+\\gamma V_{\\gamma }^{\\pi}(x{}\'))\n\\end{aligned}\n$$\n\n## 16.10\n\n$$\n\\left\\{\\begin{array}{l}\nQ_{T}^{\\pi}(x, a)=\\sum_{x^{\\prime} \\in X} P_{x \\rightarrow x^{\\prime}}^{a}\\left(\\frac{1}{T} R_{x \\rightarrow x^{\\prime}}^{a}+\\frac{T-1}{T} V_{T-1}^{\\pi}\\left(x^{\\prime}\\right)\\right) \\\\\nQ_{\\gamma}^{\\pi}(x, a)=\\sum_{x^{\\prime} \\in X} P_{x \\rightarrow x^{\\prime}}^{a}\\left(R_{x \\rightarrow x^{\\prime}}^{a}+\\gamma V_{\\gamma}^{\\pi}\\left(x^{\\prime}\\right)\\right)\n\\end{array}\\right.\n$$\n\n[推导]：参见 16.7, 16.8\n\n## 16.14\n\n$$\nV^{*}(x)=\\max _{a \\in A} Q^{\\pi^{*}}(x, a)\n$$\n\n[解析]：为了获得最优的状态值函数$V$，这里取了两层最优，分别是采用最优策略$\\pi^{*}$和选取使得状态动作值函数$Q$最大的状态$\\max_{a\\in A}$。\n\n## 16.16\n\n$$\nV^{\\pi}(x)\\leq V^{\\pi{}\'}(x)\n$$\n\n[推导]：\n$$\n\\begin{aligned}\nV^{\\pi}(x)&\\leq Q^{\\pi}(x,\\pi{}\'(x))\\\\\n&=\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}(R_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}+\\gamma V^{\\pi}(x{}\'))\\\\\n&\\leq \\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}(R_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}+\\gamma Q^{\\pi}(x{}\',\\pi{}\'(x{}\')))\\\\\n&=\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}(R_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}+\\gamma \\sum_{x{}\'\\in X}P_{x{}\'\\rightarrow x{}\'}^{\\pi{}\'(x{}\')}(R_{x{}\'\\rightarrow x{}\'}^{\\pi{}\'(x{}\')}+\\gamma V^{\\pi}(x{}\')))\\\\\n&=\\sum_{x{}\'\\in X}P_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}(R_{x\\rightarrow x{}\'}^{\\pi{}\'(x)}+\\gamma V^{\\pi{}\'}(x{}\'))\\\\\n&=V^{\\pi{}\'}(x)\n\\end{aligned}\n$$\n其中，使用了动作改变条件\n$$\nQ^{\\pi}(x,\\pi{}\'(x))\\geq V^{\\pi}(x)\n$$\n以及状态-动作值函数\n$$\nQ^{\\pi}(x{}\',\\pi{}\'(x{}\'))=\\sum_{x{}\'\\in X}P_{x{}\'\\rightarrow x{}\'}^{\\pi{}\'(x{}\')}(R_{x{}\'\\rightarrow x{}\'}^{\\pi{}\'(x{}\')}+\\gamma V^{\\pi}(x{}\'))\n$$\n于是，当前状态的最优值函数为\n\n$$\nV^{\\ast}(x)=V^{\\pi{}\'}(x)\\geq V^{\\pi}(x)\n$$\n\n\n\n## 16.31\n\n$$\nQ_{t+1}^{\\pi}(x,a)=Q_{t}^{\\pi}(x,a)+\\alpha (R_{x\\rightarrow x{}\'}^{a}+\\gamma Q_{t}^{\\pi}(x{}\',a{}\')-Q_{t}^{\\pi}(x,a))\n$$\n\n[推导]：对比公式16.29\n$$\nQ_{t+1}^{\\pi}(x,a)=Q_{t}^{\\pi}(x,a)+\\frac{1}{t+1}(r_{t+1}-Q_{t}^{\\pi}(x,a))\n$$\n以及由\n$$\n\\frac{1}{t+1}=\\alpha\n$$\n可知，若下式成立，则公式16.31成立\n$$\nr_{t+1}=R_{x\\rightarrow x{}\'}^{a}+\\gamma Q_{t}^{\\pi}(x{}\',a{}\')\n$$\n而$r_{t+1}$表示$t+1$步的奖赏，即状态$x$变化到$x\'$的奖赏加上前面$t$步奖赏总和$Q_{t}^{\\pi}(x{}\',a{}\')$的$\\gamma$折扣，因此这个式子成立。\n\n\n\n','2021-12-10 12:43:49','2021-12-19 13:23:18'),
	(34,3,'第一章 强化学习概述','## Reinforcement Learning\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.1.png \':size=450\') \n\n**强化学习讨论的问题是一个智能体(agent) 怎么在一个复杂不确定的环境(environment)里面去极大化它能获得的奖励。** 示意图由两部分组成：agent 和 environment。在强化学习过程中，agent 跟 environment 一直在交互。Agent 在环境里面获取到状态，agent 会利用这个状态输出一个动作(action)，一个决策。然后这个决策会放到环境之中去，环境会根据 agent 采取的决策，输出下一个状态以及当前的这个决策得到的奖励。Agent 的目的就是为了尽可能多地从环境中获取奖励。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.2.png \':size=500\')\n\n**我们可以把强化学习跟监督学习做一个对比。**\n\n* 举个图片分类的例子，`监督学习(supervised learning)`就是说我们有一大堆标注的数据，比如车、飞机、凳子这些标注的图片，这些图片都要满足独立同分布(i.i.d.)，就是它们之间是没有关联的。\n\n* 然后我们训练一个分类器，比如说右边这个神经网络。为了分辨出这个图片是车辆还是飞机，训练过程中，我们把真实的标签给了这个网络。当这个网络做出一个错误的预测，比如现在输入了汽车的图片，它预测出来是飞机。我们就会直接告诉它，你这个预测是错误的，正确的标签应该是车。然后我们把这个错误写成一个`损失函数(loss function)`，通过反向传播(Backpropagation)来训练这个网络。\n* 所以在监督学习过程中，有两个假设：\n  * 输入的数据（标注的数据）都是没有关联的，尽可能没有关联。因为如果有关联的话，这个网络是不好学习的。\n  * 我们告诉学习器(learner)正确的标签是什么，这样它可以通过正确的标签来修正自己的预测。\n\n> 通常假设样本空间中全体样本服从一个未知分布，我们获得的每个样本都是独立地从这个分布上采样获得的，即独立同分布(independent and identically distributed，简称 i.i.d.)。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.3.png \':size=300\')\n\n在强化学习里面，这两点其实都不满足。举一个 Atari Breakout 游戏的例子，这是一个打砖块的游戏，控制木板左右移动把球反弹到上面来消除砖块。\n\n* 在游戏过程中，大家可以发现这个 agent 得到的观测不是个独立同分布的分布，上一帧下一帧其实有非常强的连续性。这就是说，得到的数据是相关的时间序列数据，不满足独立同分布。\n* 另外一点，在玩游戏的过程中，你并没有立刻获得反馈，没有告诉你哪个动作是正确动作。比如你现在把这个木板往右移，那么只会使得这个球往上或者往左上去一点，你并不会得到立刻的反馈。所以强化学习这么困难的原因是没有得到很好的反馈，然后你依然希望 agent 在这个环境里面学习。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.4.png \':size=500\')\n\n强化学习的训练数据就是这样一个玩游戏的过程。你从第一步开始，采取一个决策，比如说你把这个往右移，接到这个球了。第二步你又做出决策，得到的训练数据是一个玩游戏的序列。\n\n比如现在是在第三步，你把这个序列放进去，你希望这个网络可以输出一个决策，在当前的这个状态应该输出往右移或者往左移。这里有个问题：我们没有标签来说明你现在这个动作是正确还是错误，必须等到游戏结束才可能说明，这个游戏可能十秒过后才结束。现在这个动作到底对最后游戏结束能赢是否有帮助，其实是不清楚的。这里就面临`延迟奖励(Delayed Reward)`，所以就使得训练这个网络非常困难。\n\n**我们对比下强化学习和监督学习。**\n\n* 强化学习输入的是序列数据，而不是像监督学习里面这些样本都是独立的。\n* 学习器并没有被告诉你每一步正确的行为应该是什么。学习器需要自己去发现哪些行为可以得到最多的奖励，只能通过不停地尝试来发现最有利的动作。  \n*  Agent 获得自己能力的过程中，其实是通过不断地试错探索(trial-and-error exploration)。\n  * 探索(exploration)和利用(exploitation)是强化学习里面非常核心的一个问题。\n  * 探索：你会去尝试一些新的行为，这些新的行为有可能会使你得到更高的奖励，也有可能使你一无所有。\n  * 利用：采取你已知的可以获得最大奖励的行为，你就重复执行这个动作就可以了，因为你已经知道可以获得一定的奖励。\n  * 因此，我们需要在探索和利用之间取得一个权衡，这也是在监督学习里面没有的情况。\n* 在强化学习过程中，没有非常强的监督者(supervisor)，只有一个`奖励信号(reward signal)`，并且这个奖励信号是延迟的，就是环境会在很久以后告诉你之前你采取的行为到底是不是有效的。Agent 在这个强化学习里面学习的话就非常困难，因为你没有得到即时反馈。当你采取一个行为过后，如果是监督学习，你就立刻可以获得一个指引，就说你现在做出了一个错误的决定，那么正确的决定应该是谁。而在强化学习里面，环境可能会告诉你这个行为是错误的，但是它并没有告诉你正确的行为是什么。而且更困难的是，它可能是在一两分钟过后告诉你错误，它再告诉你之前的行为到底行不行。所以这也是强化学习和监督学习不同的地方。\n\n通过跟监督学习比较，我们可以总结出强化学习的一些特征。\n\n* 强化学习有这个 `试错探索(trial-and-error exploration)`，它需要通过探索环境来获取对环境的理解。\n* 强化学习 agent 会从环境里面获得延迟的奖励。\n* 在强化学习的训练过程中，时间非常重要。因为你得到的数据都是有时间关联的(sequential data)，而不是独立同分布的。在机器学习中，如果观测数据有非常强的关联，其实会使得这个训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量是独立同分布，这样就可以消除数据之间的相关性。\n* Agent 的行为会影响它随后得到的数据，这一点是非常重要的。在我们训练 agent 的过程中，很多时候我们也是通过正在学习的这个 agent 去跟环境交互来得到数据。所以如果在训练过程中，这个 agent 的模型很快死掉了，那会使得我们采集到的数据是非常糟糕的，这样整个训练过程就失败了。所以在强化学习里面一个非常重要的问题就是怎么让这个 agent 的行为一直稳定地提升。\n\n为什么我们关注强化学习，其中非常重要的一点就是强化学习得到的模型可以有超人类的表现。\n\n* 监督学习获取的这些监督数据，其实是让人来标注的。比如说 ImageNet 的图片都是人类标注的。那么我们就可以确定这个算法的上限(upper bound)就是人类的表现，人类的这个标注结果决定了它永远不可能超越人类。\n* 但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人的能力的这个表现，比如谷歌 DeepMind 的 AlphaGo 这样一个强化学习的算法可以把人类最强的棋手都打败。\n\n这里给大家举一些在现实生活中强化学习的例子。\n\n* 在自然界中，羚羊其实也是在做一个强化学习，它刚刚出生的时候，可能都不知道怎么站立，然后它通过试错的一个尝试，三十分钟过后，它就可以跑到每小时 36 公里，很快地适应了这个环境。\n* 你也可以把股票交易看成一个强化学习的问题，就怎么去买卖来使你的收益极大化。\n* 玩雅达利游戏或者一些电脑游戏，也是一个强化学习的过程。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.9.png \':size=350\')\n\n上图是强化学习的一个经典例子，就是雅达利的一个叫 Pong 的游戏。这个游戏就是把这个球拍到左边，然后左边这个选手需要把这个球拍到右边。训练好的一个强化学习 agent 和正常的选手有区别，强化学习的 agent 会一直在做这种无意义的一些振动，而正常的选手不会出现这样的行为。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.10.png \':size=450\')\n\n在这个 pong 的游戏里面，决策其实就是两个动作：往上或者往下。如果强化学习是通过学习一个 policy network 来分类的话，其实就是输入当前帧的图片，policy network 就会输出所有决策的可能性。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.11.png \':size=450\')\n\n对于监督学习，我们可以直接告诉 agent 正确的标签是什么。但在这种游戏情况下面，我们并不知道它的正确的标签是什么。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.12.png \':size=450\')\n\n在强化学习里面，我们是通过让它尝试去玩这个游戏，然后直到游戏结束过后，再去说你前面的一系列动作到底是正确还是错误。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.13.png \':size=450\')\n\n* 上图的过程是 `rollout` 的一个过程。Rollout 的意思是从当前帧去生成很多局的游戏。\n\n* 当前的 agent 去跟环境交互，你就会得到一堆观测。你可以把每一个观测看成一个`轨迹(trajectory)`。轨迹就是当前帧以及它采取的策略，即状态和动作的一个序列：\n  $$\n  \\tau=\\left(s_{0}, a_{0}, s_{1}, a_{1}, \\ldots\\right)\n  $$\n\n* 最后结束过后，你会知道你到底有没有把这个球击到对方区域，对方没有接住，你是赢了还是输了。我们可以通过观测序列以及最终奖励(eventual reward)来训练这个 agent ，使它尽可能地采取可以获得这个最终奖励的动作。\n\n* 一场游戏叫做一个 `episode(回合)` 或者 `trial(试验)`。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.14.png \':size=500\')\n\n强化学习是有一定的历史的，只是最近大家把强化学习跟深度学习结合起来，就形成了`深度强化学习(Deep Reinforcemet Learning)`。深度强化学习 = 深度学习 + 强化学习。这里做一个类比，把它类比于这个传统的计算机视觉以及深度计算机视觉。\n\n* 传统的计算机视觉由两个过程组成。\n  * 给定一张图，我们先要提取它的特征，用一些设计好的特征(feature)，比如说 HOG、DPM。\n  * 提取这些特征后，我们再单独训练一个分类器。这个分类器可以是 SVM、Boosting，然后就可以辨别这张图片是狗还是猫。 \n* 2012年，Krizhevsky等人提出了AlexNet，AlexNet在ImageNet分类比赛中取得冠军，迅速引起了人们对于卷积神经网络的广泛关注。\n大家就把特征提取以及分类两者合到一块儿去了，就是训练一个神经网络。这个神经网络既可以做特征提取，也可以做分类。它可以实现这种端到端的训练，它里面的参数可以在每一个阶段都得到极大的优化，这样就得到了一个非常重要的突破。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.15.png \':size=500\')\n\n我们可以把神经网络放到强化学习里面。\n\n* Standard RL：之前的强化学习，比如 TD-Gammon  玩 backgammon 这个游戏，它其实是设计特征，然后通过训练价值函数的一个过程，就是它先设计了很多手工的特征，这个手工特征可以描述现在整个状态。得到这些特征过后，它就可以通过训练一个分类网络或者分别训练一个价值估计函数来做出决策。\n* Deep RL：现在我们有了深度学习，有了神经网络，那么大家也把这个过程改进成一个端到端训练(end-to-end training)的过程。你直接输入这个状态，我们不需要去手工地设计这个特征，就可以让它直接输出动作。那么就可以用一个神经网络来拟合我们这里的价值函数或策略网络，省去了特征工程(feature engineering)的过程。\n\n为什么强化学习在这几年就用到各种应用中去，比如玩游戏以及机器人的一些应用，并且可以击败人类的最好棋手。\n\n这有如下几点原因：\n\n* 我们有了更多的算力(computation power)，有了更多的 GPU，可以更快地做更多的试错的尝试。\n* 通过这种不同尝试使得 agent 在这个环境里面获得很多信息，然后可以在这个环境里面取得很大的奖励。\n* 我们有了这个端到端的一个训练，可以把特征提取和价值估计或者决策一块来优化，这样就可以得到了一个更强的决策网络。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.17.png)\n\n 接下来给大家再看一些强化学习里面比较有意思的例子。\n\n1. **[DeepMind 研发的一个走路的 agent](https://www.youtube.com/watch?v=gn4nRCC9TwQ)。**这个 agent 往前走一步，你就会得到一个 reward。这个 agent 有不同的这个形态，可以学到很多有意思的功能。比如怎么跨越这个障碍物，就像那个蜘蛛那样的 agent 。怎么跨越障碍物，像这个人有双腿一样， 这个 agent 往前走。以及像这个人形的 agent，怎么在一个曲折的道路上面往前走。这个结果也是非常有意思，这个人形 agent 会把手举得非常高，因为它这个手的功能就是为了使它身体保持平衡，这样它就可以更快地在这个环境里面往前跑，而且这里你也可以增加这个环境的难度，加入一些扰动，这个 agent 就会变得更鲁棒。\n2.  **[机械臂抓取](https://ai.googleblog.com/2016/03/deep-learning-for-robots-learning-from.html)。**因为机械臂的应用自动去强化学习需要大量的 rollout，所以它这里就有好多机械臂，分布式系统可以让这个机械臂尝试抓取不同的物体。你发现这个盘子里面物体的形状、形态其实都是不同的，这样就可以让这个机械臂学到一个统一的行为。然后在不同的抓取物下面都可以采取最优的一个抓取特征。你的这个抓取的物件形态存在很多不同，一些传统的这个抓取算法就没法把所有物体都抓起来，因为你对每一个物体都需要做一个建模，这样的话就是非常花时间。但是通过强化学习，你就可以学到一个统一的抓取算法，在不同物体上它都可以适用。\n3.  **[OpenAI 做的一个机械臂翻魔方](https://www.youtube.com/watch?v=jwSbzNHGflM)。**这里它们 18 年的时候先设计了这个手指的一个机械臂，让它可以通过翻动手指，使得手中的这个木块达到一个预定的设定。人的手指其实非常精细，怎么使得这个机械手臂也具有这样灵活的能力就一直是个问题。它们通过这个强化学习在一个虚拟环境里面先训练，让 agent 能翻到特定的这个方向，再把它应用到真实的手臂之中。这在强化学习里面是一个比较常用的做法，就是你先在虚拟环境里面得到一个很好的 agent，然后再把它使用到真实的这个机器人中。因为真实的机械手臂通常都是非常容易坏，而且非常贵，你没法大批量地购买。2019 年对手臂进一步改进了，这个手臂可以玩魔方了。这个结果也非常有意思，到后面，这个魔方就被恢复成了个六面都是一样的结构了。\n4.  **[一个穿衣服的 agent](https://www.youtube.com/watch?v=ixmE5nt2o88) ，就是训练这个 agent 穿衣服。**因为很多时候你要在电影或者一些动画实现人穿衣服的场景，通过手写执行命令让机器人穿衣服其实非常困难。很多时候穿衣服也是一个非常精细的操作，那么它们这个工作就是训练这个强化学习 agent，然后就可以实现这个穿衣功能。你还可以在这里面加入一些扰动，然后 agent 可以抗扰动。可能会有失败的情况(failure case)， agent 就穿不进去，就卡在这个地方。\n\n## Introduction to Sequential Decision Making\n\n### Agent and Environment\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.18.png \':size=450\')\n\n接下来我们讲`序列决策(Sequential Decision Making)过程`。\n\n强化学习研究的问题是 agent 跟环境交互，上图左边画的是一个 agent，agent 一直在跟环境进行交互。这个 agent 把它输出的动作给环境，环境取得这个动作过后，会进行到下一步，然后会把下一步的观测跟它上一步是否得到奖励返还给 agent。\n\n通过这样的交互过程会产生很多观测，agent 的目的是从这些观测之中学到能极大化奖励的策略。\n\n### Reward\n\n奖励是由环境给的一个标量的反馈信号(scalar feedback signal)，这个信号显示了 agent 在某一步采取了某个策略的表现如何。\n\n强化学习的目的就是为了最大化 agent 可以获得的奖励，agent 在这个环境里面存在的目的就是为了极大化它的期望的累积奖励(expected cumulative reward)。\n\n不同的环境，奖励也是不同的。这里给大家举一些奖励的例子。\n\n* 比如说一个下象棋的选手，他的目的其实就为了赢棋。奖励是说在最后棋局结束的时候，他知道会得到一个正奖励或者负奖励。\n* 羚羊站立也是一个强化学习过程，它得到的奖励就是它是否可以最后跟它妈妈一块离开或者它被吃掉。\n* 在股票管理里面，奖励定义由你的股票获取的收益跟损失决定。\n* 在玩雅达利游戏的时候，奖励就是你有没有在增加游戏的分数，奖励本身的稀疏程度决定了这个游戏的难度。\n\n### Sequential Decision Making\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.21.png \':size=500\')\n\n在一个强化学习环境里面，agent 的目的就是选取一系列的动作来极大化它的奖励，所以这些采取的动作必须有长期的影响。但在这个过程里面，它的奖励其实是被延迟了，就是说你现在采取的某一步决策可能要等到时间很久过后才知道这一步到底产生了什么样的影响。\n\n这里一个示意图就是我们玩这个 Atari 的 Pong 游戏，你可能只有到最后游戏结束过后，才知道这个球到底有没有击打过去。中间你采取的 up 或 down 行为，并不会直接产生奖励。强化学习里面一个重要的课题就是近期奖励和远期奖励的一个权衡(trade-off)。怎么让 agent 取得更多的长期奖励是强化学习的问题。\n\n\n在跟环境的交互过程中，agent 会获得很多观测。在每一个观测会采取一个动作，它也会得到一个奖励。**所以历史是观测(observation)、行为、奖励的序列：**\n$$\nH_{t}=O_{1}, R_{1}, A_{1}, \\ldots, A_{t-1}, O_{t}, R_{t}\n$$\nAgent 在采取当前动作的时候会依赖于它之前得到的这个历史，**所以你可以把整个游戏的状态看成关于这个历史的函数：**\n$$\nS_{t}=f\\left(H_{t}\\right)\n$$\nQ: 状态和观测有什么关系?\n\nA: `状态(state)` $s$ 是对世界的完整描述，不会隐藏世界的信息。`观测(observation)` $o$ 是对状态的部分描述，可能会遗漏一些信息。在 deep RL 中，我们几乎总是用一个实值的向量、矩阵或者更高阶的张量来表示状态和观测。举个例子，我们可以用 RGB 像素值的矩阵来表示一个视觉的观测，我们可以用机器人关节的角度和速度来表示一个机器人的状态。\n\n环境有自己的函数 $S_{t}^{e}=f^{e}\\left(H_{t}\\right)$ 来更新状态，在 agent 的内部也有一个函数 $S_{t}^{a}=f^{a}\\left(H_{t}\\right)$ 来更新状态。当 agent 的状态跟环境的状态等价的时候，我们就说这个环境是 `full observability`，就是全部可以观测。换句话说，当 agent 能够观察到环境的所有状态时，我们称这个环境是`完全可观测的(fully observed)`。在这种情况下面，强化学习通常被建模成一个 Markov decision process(MDP)的问题。在 MDP 中， $O_{t}=S_{t}^{e}=S_{t}^{a}$。\n\n但是有一种情况是 agent 得到的观测并不能包含环境运作的所有状态，因为在这个强化学习的设定里面，环境的状态才是真正的所有状态。\n\n* 比如 agent 在玩这个 black jack 这个游戏，它能看到的其实是牌面上的牌。\n* 或者在玩雅达利游戏的时候，观测到的只是当前电视上面这一帧的信息，你并没有得到游戏内部里面所有的运作状态。\n\n也就是说当 agent 只能看到部分的观测，我们就称这个环境是`部分可观测的(partially observed)`。在这种情况下面，强化学习通常被建模成一个 POMDP 的问题。\n\n`部分可观测马尔可夫决策过程(Partially Observable Markov Decision Processes, POMDP)`是一个马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 $s$，只能知道部分观测值 $o$。比如在自动驾驶中，智能体只能感知传感器采集的有限的环境信息。\n\nPOMDP 可以用一个 7 元组描述：$(S,A,T,R,\\Omega,O,\\gamma)$，其中 $S$ 表示状态空间，为隐变量，$A$ 为动作空间，$T(s\'|s,a)$ 为状态转移概率，$R$ 为奖励函数，$\\Omega(o|s,a)$ 为观测概率，$O$ 为观测空间，$\\gamma$ 为折扣系数。\n\n## Action Spaces\n\n不同的环境允许不同种类的动作。在给定的环境中，有效动作的集合经常被称为`动作空间(action space)`。像 Atari 和 Go 这样的环境有`离散动作空间(discrete action spaces)`，在这个动作空间里，agent 的动作数量是有限的。在其他环境，比如在物理世界中控制一个 agent，在这个环境中就有`连续动作空间(continuous action spaces)` 。在连续空间中，动作是实值的向量。 \n\n例如，\n\n* 走迷宫机器人如果只有东南西北这 4 种移动方式，则其为离散动作空间；\n* 如果机器人向 $360^{\\circ}$ 中的任意角度都可以移动，则为连续动作空间。\n\n## Major Components of an RL Agent \n\n对于一个强化学习 agent，它可能有一个或多个如下的组成成分：\n\n*  `策略函数(policy function)`，agent 会用这个函数来选取下一步的动作。\n\n* `价值函数(value function)`，我们用价值函数来对当前状态进行估价，它就是说你进入现在这个状态，可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。\n\n* `模型(model)`，模型表示了 agent 对这个环境的状态进行了理解，它决定了这个世界是如何进行的。\n\n### Policy\n\n我们深入看这三个组成成分的一些细节。\n\nPolicy 是 agent 的行为模型，它决定了这个 agent 的行为，它其实是一个函数，把输入的状态变成行为。这里有两种 policy：\n\n* 一种是 `stochastic policy(随机性策略)`，它就是 $\\pi$ 函数 $\\pi(a | s)=P\\left[A_{t}=a | S_{t}=s\\right]$ 。当你输入一个状态 $s$ 的时候，输出是一个概率。这个概率就是你所有行为的一个概率，然后你可以进一步对这个概率分布进行采样，得到真实的你采取的行为。比如说这个概率可能是有 70% 的概率往左，30% 的概率往右，那么你通过采样就可以得到一个 action。\n* 一种是 `deterministic policy(确定性策略)`，就是说你这里有可能只是采取它的极大化，采取最有可能的动作，即 $a^{*}=\\arg \\underset{a}{\\max} \\pi(a \\mid s)$。 你现在这个概率就是事先决定好的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.26.png)\n\n从  Atari 游戏来看的话，策略函数的输入就是游戏的一帧，它的输出决定你是往左走或者是往右走。\n\n通常情况下，强化学习一般使用`随机性策略`。随机性策略有很多优点：\n\n* 在学习时可以通过引入一定随机性来更好地探索环境；\n\n* 随机性策略的动作具有多样性，这一点在多个智能体博弈时也非常重要。采用确定性策略的智能体总是对同样的环境做出相同的动作，会导致它的策略很容易被对手预测。\n\n### Value Function\n**价值函数是未来奖励的一个预测，用来评估状态的好坏**。\n\n价值函数里面有一个 `discount factor(折扣因子)`，我们希望尽可能在短的时间里面得到尽可能多的奖励。如果我们说十天过后，我给你 100 块钱，跟我现在给你 100 块钱，你肯定更希望我现在就给你 100 块钱，因为你可以把这 100 块钱存在银行里面，你就会有一些利息。所以我们就通过把这个折扣因子放到价值函数的定义里面，价值函数的定义其实是一个期望，如下式所示：\n$$\nv_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right], \\text { for all } s \\in \\mathcal{S}\n$$\n这里有一个期望 $\\mathbb{E}_{\\pi}$，这里有个小角标是 $\\pi$ 函数，这个 $\\pi$ 函数就是说在我们已知某一个策略函数的时候，到底可以得到多少的奖励。\n\n我们还有一种价值函数：Q 函数。Q 函数里面包含两个变量：状态和动作，其定义如下式所示：\n$$\nq_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right]=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s, A_{t}=a\\right]\n$$\n所以你未来可以获得多少的奖励，它的这个期望取决于你当前的状态和当前的行为。这个 Q 函数是强化学习算法里面要学习的一个函数。因为当我们得到这个 Q 函数后，进入某一种状态，它最优的行为就可以通过这个 Q 函数来得到。\n\n### Model\n第三个组成部分是模型，**模型决定了下一个状态会是什么样的，就是说下一步的状态取决于你当前的状态以及你当前采取的行为。**它由两个部分组成，\n\n* 概率：这个转移状态之间是怎么转移的，如下式所示：\n\n$$\n\\mathcal{P}_{s s^{\\prime}}^{a}=\\mathbb{P}\\left[S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right]\n$$\n\n* 奖励函数：当你在当前状态采取了某一个行为，可以得到多大的奖励，如下式所示：\n$$\n\\mathcal{R}_{s}^{a}=\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a\\right]\n$$\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.29.png \':size=300\')\n\n当我们有了这三个组成部分过后，就形成了一个 `马尔可夫决策过程(Markov Decision Process)`。这个决策过程可视化了状态之间的转移以及采取的行为。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.30.png \':size=300\')\n\n我们来看一个走迷宫的例子。\n\n* 要求 agent 从 start 开始，然后到达 goal 的位置。\n* 每走一步，你就会得到 -1 的奖励。\n* 可以采取的动作是往上下左右走。\n* 当前状态用现在 agent 所在的位置来描述。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.31.png \':size=300\')\n\n* 我们可以用不同的强化学习算法来解这个环境。\n* 如果采取的是 `基于策略的(policy-based)RL`，当学习好了这个环境过后，在每一个状态，我们就会得到一个最佳的行为。\n\n* 比如说现在在第一格开始的时候，我们知道它最佳行为是往右走，然后第二格的时候，得到的最佳策略是往上走，第三格是往右走。通过这个最佳的策略，我们就可以最快地到达终点。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.32.png \':size=300\')\n\n* 如果换成 `基于价值的(value-based)RL` 这个算法，利用价值函数来作为导向，我们就会得到另外一种表征，这里就表征了你每一个状态会返回一个价值。\n\n* 比如说你在 start 位置的时候，价值是 -16，因为你最快可以 16 步到达终点。因为每走一步会减一，所以你这里的价值是 -16。\n* 当我们快接近最后终点的时候，这个数字变得越来越大。在拐角的时候，比如要现在在第二格 -15。然后 agent 会看上下，它看到上面值变大了，变成 -14 了，它下面是 -16，那么 agent 肯定就会采取一个往上走的策略。所以通过这个学习的值的不同，我们可以抽取出现在最佳的策略。\n\n## Types of RL Agents\n\n**根据 agent 学习的东西不同，我们可以把 agent 进行归类。**\n\n* `基于价值的 agent(value-based agent)`。\n  * 这一类 agent 显式地学习的是价值函数，\n  * 隐式地学习了它的策略。策略是从我们学到的价值函数里面推算出来的。\n* `基于策略的 agent(policy-based agent)`。\n  * 这一类 agent 直接去学习 policy，就是说你直接给它一个状态，它就会输出这个动作的概率。\n  * 在基于策略的 agent 里面并没有去学习它的价值函数。\n* 把 value-based 和 policy-based 结合起来就有了 `Actor-Critic agent`。这一类 agent 把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个最佳的行为。\n\nQ: 基于策略迭代和基于价值迭代的强化学习方法有什么区别?\n\nA: 对于一个状态转移概率已知的马尔可夫决策过程，我们可以使用动态规划算法来求解；从决策方式来看，强化学习又可以划分为基于策略迭代的方法和基于价值迭代的方法。`决策方式`是智能体在给定状态下从动作集合中选择一个动作的依据，它是静态的，不随状态变化而变化。\n\n在`基于策略迭代`的强化学习方法中，智能体会`制定一套动作策略`（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。\n\n而在`基于价值迭代`的强化学习方法中，智能体不需要制定显式的策略，它`维护一个价值表格或价值函数`，并通过这个价值表格或价值函数来选取价值最大的动作。基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)。\n\n基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。\n\n**另外，我们是可以通过 agent 到底有没有学习这个环境模型来分类。**\n\n* `model-based(有模型)` RL agent，它通过学习这个状态的转移来采取动作。\n* ` model-free(免模型)` RL agent，它没有去直接估计这个状态的转移，也没有得到环境的具体转移变量。它通过学习价值函数和策略函数进行决策。Model-free 的模型里面没有一个环境转移的模型。\n\n我们可以用马尔可夫决策过程来定义强化学习任务，并表示为四元组 $<S,A,P,R>$，即状态集合、动作集合、状态转移函数和奖励函数。如果这四元组中所有元素均已知，且状态集合和动作集合在有限步数内是有限集，则机器可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境的状态和交互反应。\n\n具体来说，当智能体知道状态转移函数 $P(s_{t+1}|s_t,a_t)$ 和奖励函数 $R(s_t,a_t)$ 后，它就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，这样智能体就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略即可。这种学习方法称为`有模型学习`。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.35.png \':size=400\')\n\n上图是有模型强化学习的流程图。\n\n然而在实际应用中，智能体并不是那么容易就能知晓 MDP 中的所有元素的。**通常情况下，状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，这时就需要采用免模型学习。**免模型学习没有对真实环境进行建模，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新行为策略，这样反复迭代直到学习到最优策略。\n\nQ: 有模型强化学习和免模型强化学习有什么区别？\n\nA: 针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。\n\n* 有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；\n\n* 免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。\n\n总的来说，有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模。因此，一些有模型的强化学习方法，也可以在免模型的强化学习方法中使用。在实际应用中，如果不清楚该用有模型强化学习还是免模型强化学习，可以先思考一下，在智能体执行动作前，是否能对下一步的状态和奖励进行预测，如果可以，就能够对环境进行建模，从而采用有模型学习。\n\n免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。例如，在 Atari 平台上的 Space Invader 游戏中，免模型的深度强化学习需要大约 2 亿帧游戏画面才能学到比较理想的效果。相比之下，有模型学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中行训练。\n\n免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。\n\n有模型的强化学习方法可以对环境建模，使得该类方法具有独特魅力，即“想象能力”。在免模型学习中，智能体只能一步一步地采取策略，等待真实环境的反馈；而有模型学习可以在虚拟世界中预测出所有将要发生的事，并采取对自己最有利的策略。\n\n**目前，大部分深度强化学习方法都采用了免模型学习**，这是因为：\n\n* 免模型学习更为简单直观且有丰富的开源资料，像 DQN、AlphaGo 系列等都采用免模型学习；\n* 在目前的强化学习研究中，大部分情况下环境都是静态的、可描述的，智能体的状态是离散的、可观察的（如 Atari 游戏平台），这种相对简单确定的问题并不需要评估状态转移函数和奖励函数，直接采用免模型学习，使用大量的样本进行训练就能获得较好的效果。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.36.png \':size=400\')\n\n把几类模型放到同一个饼图里面。饼图有三个组成部分：价值函数、策略和模型。按一个 agent 具不具有三者中的两者或者一者可以把它分成很多类。\n\n## Learning and Planning\n\nLearning 和 Planning 是序列决策的两个基本问题。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/learning.png \':size=450\')\n\n在强化学习中，环境初始时是未知的，agent 不知道环境如何工作，agent 通过不断地与环境交互，逐渐改进策略。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/planning.png \':size=450\')\n\n在 plannning 中，环境是已知的，我们被告知了整个环境的运作规则的详细信息。Agent 能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。Agent 不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。\n\n在这个游戏中，规则是制定的，我们知道选择 left 之后环境将会产生什么变化。我们完全可以通过已知的变化规则，来在内部进行模拟整个决策过程，无需与环境交互。\n\n一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。\n\n## Exploration and Exploitation\n\n在强化学习里面，`探索` 和`利用` 是两个很核心的问题。\n\n* 探索是说我们怎么去探索这个环境，通过尝试不同的行为来得到一个最佳的策略，得到最大奖励的策略。\n\n* 利用是说我们不去尝试新的东西，就采取已知的可以得到很大奖励的行为。\n\n因为在刚开始的时候强化学习 agent 不知道它采取了某个行为会发生什么，所以它只能通过试错去探索。所以探索就是在试错来理解采取的这个行为到底可不可以得到好的奖励。利用是说我们直接采取已知的可以得到很好奖励的行为。所以这里就面临一个权衡，怎么通过牺牲一些短期的奖励来获得行为的理解，从而学习到更好的策略。\n\n下面举一些探索和利用的例子。\n\n* 以选择餐馆为例，\n  * 利用：我们直接去你最喜欢的餐馆，因为你去过这个餐馆很多次了，所以你知道这里面的菜都非常可口。\n  * 探索：你把手机拿出来，你直接搜索一个新的餐馆，然后去尝试它到底好不好吃。你有可能对这个新的餐馆非常不满意，钱就浪费了。\n\n* 以做广告为例，\n  * 利用：我们直接采取最优的这个广告策略。\n  * 探索：我们换一种广告策略，看看这个新的广告策略到底可不可以得到奖励。\n\n* 以挖油为例，\n  * 利用：我们直接在已知的地方挖油，我们就可以确保挖到油。\n  * 探索：我们在一个新的地方挖油，就有很大的概率，你可能不能发现任何油，但也可能有比较小的概率可以发现一个非常大的油田。\n* 以玩游戏为例，\n  * 利用：你总是采取某一种策略。比如说，你可能打街霸，你采取的策略可能是蹲在角落，然后一直触脚。这个策略很可能可以奏效，但可能遇到特定的对手就失效。\n  *  探索：你可能尝试一些新的招式，有可能你会发出大招来，这样就可能一招毙命。\n\n### K-armed Bandit\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.39.png \':size=280\')\n\n与监督学习不同，强化学习任务的最终奖赏是在多步动作之后才能观察到，这里我们不妨先考虑比较简单的情形：最大化单步奖赏，即仅考虑一步操作。需注意的是，即便在这样的简化情形下，强化学习仍与监督学习有显著不同，因为机器需通过尝试来发现各个动作产生的结果，而没有训练数据告诉机器应当做哪个动作。\n\n想要最大化单步奖赏需考虑两个方面：一是需知道每个动作带来的奖赏，二是要执行奖赏最大的动作。若每个动作对应的奖赏是一个确定值，那么尝试遍所有的动作便能找出奖赏最大的动作。然而，更一般的情形是，一个动作的奖赏值是来自于一个概率分布，仅通过一次尝试并不能确切地获得平均奖赏值。\n\n实际上，单步强化学习任务对应了一个理论模型，即` K-臂赌博机(K-armed bandit)`。K-臂赌博机也被称为 `多臂赌博机(Multi-armed bandit) `。如上图所示，K-摇臂赌博机有 K 个摇臂，赌徒在投入一个硬币后可选择按下其中一个摇臂，每个摇臂以一定的概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。\n\n* 若仅为获知每个摇臂的期望奖赏，则可采用`仅探索(exploration-only)法`：将所有的尝试机会平均分配给每个摇臂(即轮流按下每个摇臂)，最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计。\n\n* 若仅为执行奖赏最大的动作，则可采用`仅利用(exploitation-only)法`：按下目前最优的(即到目前为止平均奖赏最大的)摇臂，若有多个摇臂同为最优，则从中随机选取一个。\n\n显然，仅探索法能很好地估计每个摇臂的奖赏，却会失去很多选择最优摇臂的机会；仅利用法则相反，它没有很好地估计摇臂期望奖赏，很可能经常选不到最优摇臂。因此，这两种方法都难以使最终的累积奖赏最大化。\n\n事实上，探索(即估计摇臂的优劣)和利用(即选择当前最优摇臂)这两者是矛盾的，因为尝试次数(即总投币数)有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的`探索-利用窘境(Exploration-Exploitation dilemma)`。显然，想要累积奖赏最大，则必须在探索与利用之间达成较好的折中。\n\n## Experiment with Reinforcement Learning\n强化学习是一个理论跟实践结合的机器学习分支，需要去推导很多算法公式，去理解它算法背后的一些数学原理。另外一方面，上机实践通过实现算法，在很多实验环境里面去探索这个算法是不是可以得到预期效果也是一个非常重要的过程。\n\n在[这个链接](https://github.com/cuhkrlcourse/RLexample)里面，公布了一些 RL 相关的代码，利用了 Python 和深度学习的一些包(主要是用 PyTorch 为主)。\n\n你可以直接调用现有的包来实践。现在有很多深度学习的包可以用，比如 PyTorch、TensorFlow、Keras，熟练使用这里面的两三种，就可以实现非常多的功能。所以你并不需要从头去造轮子。\n\n [OpenAI](https://openai.com/) 是一个非盈利的人工智能研究公司。Open AI 公布了非常多的学习资源以及算法资源，他们之所以叫 Open AI，就是他们把所有开发的算法都 open source 出来。\n\n### Gym\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.44.png \':size=450\')\n\n[OpenAI Gym](https://gym.openai.com/) 是一个环境仿真库，里面包含了很多现有的环境。针对不同的场景，我们可以选择不同的环境，\n\n* 离散控制场景(输出的动作是可数的，比如 Pong 游戏中输出的向上或向下动作)：一般使用 Atari 环境评估\n* 连续控制场景(输出的动作是不可数的，比如机器人走路时不仅有方向，还要角度，角度就是不可数的，是一个连续的量 )：一般使用 mujoco 环境评估\n\n`Gym Retro` 是对 Gym 环境的进一步扩展，包含了更多的一些游戏。\n\n我们可以通过 pip 来安装 Gym:\n\n```bash\npip install gym\n```\n\n在 Python 环境中导入Gym，如果不报错，就可以认为 Gym 安装成功。\n\n```bash\n$python\n>>>import gym\n```\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.45.png \':size=450\')\n\n```python\nimport gym \nenv = gym.make(\"Taxi-v3\") \nobservation = env.reset() \nagent = load_agent() \nfor step in range(100):\n    action = agent(observation) \n    observation, reward, done, info = env.step(action)\n```\n\n强化学习的这个交互就是由 agent 跟环境进行交互。所以算法的 interface 也是用这个来表示。比如说我们现在安装了 OpenAI Gym。\n\n1. 我们就可以直接调入 Taxi-v3 的环境，就建立了这个环境。\n\n2. 初始化这个环境过后，就可以进行交互了。\n3. Agent 得到这个观测过后，它就会输出一个 action。\n4. 这个动作会被环境拿进去执行这个 step，然后环境就会往前走一步，返回新的 observation、reward 以及一个 flag variable `done` ，`done` 决定这个游戏是不是结束了。\n\n几行代码就实现了强化学习的框架。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.46.png \':size=400\')\n\n在 OpenAI Gym 里面有很经典的控制类游戏。\n\n* 比如说 Acrobot 就是把两节铁杖甩了立起来。\n* CartPole 是通过控制一个平板，让木棍立起来。\n* MountainCar 是通过前后移动这个车，让它到达这个旗子的位置。\n\n大家可以点[这个链接](https://gym.openai.com/envs/#classic_control)看一看这些环境。在刚开始测试强化学习的时候，可以选择这些简单环境，因为这些环境可以在一两分钟之内见到一个效果。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/1.47.png)\n\n这里我们看一下 CartPole 的这个环境。对于这个环境，有两个动作，Cart 往左移还是往右移。这里得到了观测：\n\n* 这个车当前的位置，\n* Cart 当前往左往右移的速度，\n* 这个杆的角度以及杆的最高点的速度。\n\n如果 observation 越详细，就可以更好地描述当前这个所有的状态。这里有 reward 的定义，如果能多保留一步，你就会得到一个奖励，所以你需要在尽可能多的时间存活来得到更多的奖励。当这个杆的角度大于某一个角度（没能保持平衡）或者这个车已经出到外面的时候，游戏就结束了，你就输了。所以这个 agent 的目的就是为了控制木棍，让它尽可能地保持平衡以及尽可能保持在这个环境的中央。\n\n```python\nimport gym  # 导入 Gym 的 Python 接口环境包\nenv = gym.make(\'CartPole-v0\')  # 构建实验环境\nenv.reset()  # 重置一个 episode\nfor _ in range(1000):\n    env.render()  # 显示图形界面\n    action = env.action_space.sample() # 从动作空间中随机选取一个动作\n    env.step(action) # 用于提交动作，括号内是具体的动作\nenv.close() # 关闭环境\n```\n\n注意：如果绘制了实验的图形界面窗口，那么关闭该窗口的最佳方式是调用`env.close()`。试图直接关闭图形界面窗口可能会导致内存不能释放，甚至会导致死机。\n\n当你执行这段代码时，机器人会完全无视那根本该立起来的杆子，驾驶着小车朝某个方向一通跑，直到不见踪影，这是因为我们还没开始训练机器人。\n\nGym 中的小游戏，大部分都可以用一个普通的实数或者向量来充当动作。打印 `env.action_space.sample()` 的返回值，能看到输出为 1 或者 0。\n\n`env.action_space.sample()`的含义是，在该游戏的所有动作空间里随机选择一个作为输出。在这个例子中，意思就是，动作只有两个：0 和 1，一左一右。\n\n`env.step()`这个方法的作用不止于此，它还有四个返回值，分别是`observation`、`reward`、`done`、`info`。\n\n* `observation(object)`是状态信息，是在游戏中观测到的屏幕像素值或者盘面状态描述信息。\n* `reward(float)`是奖励值，即 action 提交以后能够获得的奖励值。这个奖励值因游戏的不同而不同，但总体原则是，对完成游戏有帮助的动作会获得比较高的奖励值。\n* `done(boolean)`表示游戏是否已经完成。如果完成了，就需要重置游戏并开始一个新的 episode。\n*  `info(dict)`是一些比较原始的用于诊断和调试的信息，或许对训练有帮助。不过，OpenAI 团队在评价你提交的机器人时，是不允许使用这些信息的。\n\n在每个训练中都要使用的返回值有 observation、reward、done。但 observation 的结构会由于游戏的不同而发生变化。以 CartPole-v0 小游戏为例，我们修改下代码：\n\n```python\nimport gym  \nenv = gym.make(\'CartPole-v0\')  \nenv.reset()  \nfor _ in range(1000):\n    env.render()  \n    action = env.action_space.sample() \n    observation, reward, done, info = env.step(action)\n    print(observation)\nenv.close()\n```\n\n输出：\n\n```\n[ 0.01653398  0.19114579  0.02013859 -0.28050058]\n[ 0.0203569  -0.00425755  0.01452858  0.01846535]\n[ 0.02027175 -0.19958481  0.01489789  0.31569658]\n......\n```\n\n从输出可以看出这是一个四维的 Observation。在其他游戏中会有维度很多的情况。\n\n`env.step()`完成了一个完整的 $S \\to A \\to R \\to S\'$ 过程。我们只要不断观测这样的过程，并让机器在其中用相应的算法完成训练，就能得到一个高质量的强化学习模型。\n\n想要查看当前 Gym 库已经注册了哪些环境，可以使用以下代码：\n\n```python\nfrom gym import envs\nenv_specs = envs.registry.all()\nenvs_ids = [env_spec.id for env_spec in env_specs]\nprint(envs_ids)\n```\n\n每个环境都定义了自己的观测空间和动作空间。环境 env 的观测空间用`env.observation_space`表示，动作空间用 `env.action_space `表示。观测空间和动作空间既可以是离散空间（即取值是有限个离散的值），也可以是连续空间（即取值是连续的）。在 Gym 库中，离散空间一般用`gym.spaces.Discrete`类表示，连续空间用`gym.spaces.Box`类表示。\n\n例如，环境`\'MountainCar-v0\'`的观测空间是`Box(2,)`，表示观测可以用  2  个 float 值表示；环境`\'MountainCar-v0\'`的动作空间是`Dicrete(3)`，表示动作取值自`{0,1,2}`。对于离散空间，`gym.spaces.Discrete`类实例的成员 n 表示有几个可能的取值；对于连续空间，`Box`类实例的成员 low 和 high 表示每个浮点数的取值范围。\n\n### MountainCar-v0 Example\n\n接下来，我们通过一个例子来学习如何与 Gym 库进行交互。我们选取 `小车上山(MountainCar-v0)`作为例子。\n\n首先我们来看看这个任务的观测空间和动作空间：\n\n```python\nimport gym\nenv = gym.make(\'MountainCar-v0\')\nprint(\'观测空间 = {}\'.format(env.observation_space))\nprint(\'动作空间 = {}\'.format(env.action_space))\nprint(\'观测范围 = {} ~ {}\'.format(env.observation_space.low,\n        env.observation_space.high))\nprint(\'动作数 = {}\'.format(env.action_space.n))\n```\n\n输出：\n\n```\n观测空间 = Box(2,)\n动作空间 = Discrete(3)\n观测范围 = [-1.2  -0.07] ~ [0.6  0.07]\n动作数 = 3\n```\n\n由输出可知，观测空间是形状为 (2,) 的浮点型 np.array，动作空间是取 {0,1,2} 的 int 型数值。\n\n接下来考虑智能体。智能体往往是我们自己实现的。我们可以实现一个智能体类：`BespokeAgent类`，代码如下所示：\n\n```python\nclass BespokeAgent:\n    def __init__(self, env):\n        pass\n    \n    def decide(self, observation): # 决策\n        position, velocity = observation\n        lb = min(-0.09 * (position + 0.25) ** 2 + 0.03,\n                0.3 * (position + 0.9) ** 4 - 0.008)\n        ub = -0.07 * (position + 0.38) ** 2 + 0.07\n        if lb < velocity < ub:\n            action = 2\n        else:\n            action = 0\n        return action # 返回动作\n\n    def learn(self, *args): # 学习\n        pass\n    \nagent = BespokeAgent(env)\n```\n\n智能体的 `decide()` 方法实现了决策功能，而 `learn()` 方法实现了学习功能。`BespokeAgent`类是一个比较简单的类，它只能根据给定的数学表达式进行决策，不能有效学习。所以它并不是一个真正意义上的强化学习智能体类。但是，用于演示智能体和环境的交互已经足够了。\n\n接下来我们试图让智能体与环境交互，代码如下所示：\n\n```python\ndef play_montecarlo(env, agent, render=False, train=False):\n    episode_reward = 0. # 记录回合总奖励，初始化为0\n    observation = env.reset() # 重置游戏环境，开始新回合\n    while True: # 不断循环，直到回合结束\n        if render: # 判断是否显示\n            env.render() # 显示图形界面，图形界面可以用 env.close() 语句关闭\n        action = agent.decide(observation)\n        next_observation, reward, done, _ = env.step(action) # 执行动作\n        episode_reward += reward # 收集回合奖励\n        if train: # 判断是否训练智能体\n            agent.learn(observation, action, reward, done) # 学习\n        if done: # 回合结束，跳出循环\n            break\n        observation = next_observation\n    return episode_reward # 返回回合总奖励\n```\n\n上面代码中的 `play_montecarlo`  函数可以让智能体和环境交互一个回合。这个函数有 4 个参数：\n\n* `env` 是环境类\n* `agent` 是智能体类\n*  `render`是 bool 类型变量，指示在运行过程中是否要图形化显示。如果函数参数 render为 True，那么在交互过程中会调用 `env.render()` 以显示图形化界面，而这个界面可以通过调用 `env.close()` 关闭。\n* `train`是 bool 类型的变量，指示在运行过程中是否训练智能体。在训练过程中应当设置为 True，以调用 `agent.learn()` 函数；在测试过程中应当设置为 False，使得智能体不变。\n\n这个函数有一个返回值 `episode_reward`，是 float 类型的数值，表示智能体与环境交互一个回合的回合总奖励。\n\n接下来，我们使用下列代码让智能体和环境交互一个回合，并在交互过程中图形化显示，可用 `env.close()` 语句关闭图形化界面。\n\n```python\nenv.seed(0) # 设置随机数种子,只是为了让结果可以精确复现,一般情况下可删去\nepisode_reward = play_montecarlo(env, agent, render=True)\nprint(\'回合奖励 = {}\'.format(episode_reward))\nenv.close() # 此语句可关闭图形界面\n```\n\n输出：\n\n```\n回合奖励 = -105.0\n```\n\n为了系统评估智能体的性能，下列代码求出了连续交互 100 回合的平均回合奖励。\n\n```python\nepisode_rewards = [play_montecarlo(env, agent) for _ in range(100)]\nprint(\'平均回合奖励 = {}\'.format(np.mean(episode_rewards)))\n```\n\n输出：\n\n```\n平均回合奖励 = -102.61\n```\n\n小车上山环境有一个参考的回合奖励值 -110，如果当连续 100 个回合的平均回合奖励大于 -110，则认为这个任务被解决了。`BespokeAgent` 类对应的策略的平均回合奖励大概就在 -110 左右。\n\n测试 agent 在 Gym 库中某个任务的性能时，学术界一般最关心 100 个回合的平均回合奖励。至于为什么是 100 个回合而不是其他回合数（比如 128 个回合），完全是习惯使然，没有什么特别的原因。对于有些环境，还会指定一个参考的回合奖励值，当连续 100 个回合的奖励大于指定的值时，就认为这个任务被解决了。但是，并不是所有的任务都指定了这样的值。对于没有指定值的任务，就无所谓任务被解决了或者没有被解决。\n\n总结一下 Gym 的用法：使用 `env=gym.make(环境名)` 取出环境，使用 `env.reset()`初始化环境，使用`env.step(动作)`执行一步环境，使用 `env.render()`显示环境，使用 `env.close()` 关闭环境。\n\n最后提一下，Gym 有对应的[官方文档](https://gym.openai.com/docs/)，大家可以阅读文档来学习 Gym。\n\n##  References\n\n* [百面深度学习](https://book.douban.com/subject/35043939/)\n* [强化学习：原理与Python实现](https://book.douban.com/subject/34478302/)\n\n* [强化学习基础 David Silver 笔记](https://zhuanlan.zhihu.com/c_135909947)\n* [David Silver 强化学习公开课中文讲解及实践](https://zhuanlan.zhihu.com/reinforce)\n* [UCL Course on RL(David Silver)](https://www.davidsilver.uk/teaching/)\n\n* [白话强化学习与PyTorch](https://book.douban.com/subject/34809676/)\n\n* [OpenAI Spinning Up ](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#)\n\n* [神经网络与深度学习](https://nndl.github.io/)\n\n* [机器学习](https://book.douban.com/subject/26708119//)\n','2021-12-10 12:43:49','2021-12-19 19:21:22'),
	(35,3,'第一章 习题','## 1 Keywords\r\n\r\n- **强化学习（Reinforcement Learning）**：Agent可以在与复杂且不确定的Environment进行交互时，尝试使所获得的Reward最大化的计算算法。\r\n- **Action**: Environment接收到的Agent当前状态的输出。\r\n- **State**：Agent从Environment中获取到的状态。\r\n- **Reward**：Agent从Environment中获取的反馈信号，这个信号指定了Agent在某一步采取了某个策略以后是否得到奖励。\r\n- **Exploration**：在当前的情况下，继续尝试**新的**Action，其有可能会使你得到更高的这个奖励，也有可能使你一无所有。\r\n- **Exploitation**：在当前的情况下，继续尝试**已知的**可以获得最大Reward的过程，即重复执行这个 Action 就可以了。\r\n- **深度强化学习（Deep Reinforcement Learning）**：不需要手工设计特征，仅需要输入State让系统直接输出Action的一个end-to-end training的强化学习方法。通常使用神经网络来拟合 value function 或者 policy network。\r\n- **Full observability、fully observed和partially observed**：当Agent的状态跟Environment的状态等价的时候，我们就说现在Environment是full observability（全部可观测），当Agent能够观察到Environment的所有状态时，我们称这个环境是fully observed（完全可观测）。一般我们的Agent不能观察到Environment的所有状态时，我们称这个环境是partially observed（部分可观测）。\r\n- **POMDP（Partially Observable Markov Decision Processes）**：部分可观测马尔可夫决策过程，即马尔可夫决策过程的泛化。POMDP 依然具有马尔可夫性质，但是假设智能体无法感知环境的状态 $s$，只能知道部分观测值 $o$。\r\n- **Action space（discrete action spaces and continuous action spaces）**：在给定的Environment中，有效动作的集合经常被称为动作空间（Action space），Agent的动作数量是有限的动作空间为离散动作空间（discrete action spaces），反之，称为连续动作空间（continuous action spaces）。\r\n- **policy-based（基于策略的）**：Agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。\r\n- **valued-based（基于价值的）**：Agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。\r\n- **model-based（有模型结构）**：Agent通过学习状态的转移来采取措施。\r\n- **model-free（无模型结构）**：Agent没有去直接估计状态的转移，也没有得到Environment的具体转移变量。它通过学习 value function 和 policy function 进行决策。\r\n\r\n## 2 Questions\r\n\r\n- 强化学习的基本结构是什么？\r\n\r\n  答：本质上是Agent和Environment间的交互。具体地，当Agent在Environment中得到当前时刻的State，Agent会基于此状态输出一个Action。然后这个Action会加入到Environment中去并输出下一个State和当前的这个Action得到的Reward。Agent在Environment里面存在的目的就是为了极大它的期望积累的Reward。\r\n\r\n- 强化学习相对于监督学习为什么训练会更加困难？（强化学习的特征）\r\n\r\n  答：\r\n\r\n  1. 强化学习处理的多是序列数据，其很难像监督学习的样本一样满足**IID（独立同分布）**条件。\r\n\r\n  2. 强化学习有奖励的延迟（Delay Reward），即在Agent的action作用在Environment中时，Environment对于Agent的State的**奖励的延迟**（Delayed Reward），使得反馈不及时。\r\n  3. 相比于监督学习有正确的label，可以通过其修正自己的预测，强化学习相当于一个“试错”的过程，其完全根据Environment的“**反馈**”更新对自己最有利的Action。\r\n\r\n- 强化学习的基本特征有哪些？\r\n\r\n  答： \r\n\r\n  1. 有**trial-and-error exploration**的过程，即需要通过探索Environment来获取对这个Environment的理解。\r\n  2. 强化学习的Agent会从Environment里面获得**延迟**的Reward。\r\n  3. 强化学习的训练过程中**时间**非常重要，因为数据都是有时间关联的，而不是像监督学习一样是IID分布的。\r\n  4. 强化学习中Agent的Action会**影响**它随后得到的**反馈**。\r\n\r\n- 近几年强化学习发展迅速的原因？\r\n\r\n  答：\r\n\r\n  1. **算力（GPU、TPU）的提升**，我们可以更快地做更多的 trial-and-error 的尝试来使得Agent在Environment里面获得很多信息，取得更大的Reward。\r\n\r\n  2. 我们有了深度强化学习这样一个端到端的训练方法，可以把特征提取和价值估计或者决策一起优化，这样就可以得到一个更强的决策网络。\r\n\r\n- 状态和观测有什么关系？\r\n\r\n  答：状态（state）是对世界的**完整描述**，不会隐藏世界的信息。观测（observation）是对状态的**部分描述**，可能会遗漏一些信息。在深度强化学习中，我们几乎总是用一个实值向量、矩阵或者更高阶的张量来表示状态和观测。\r\n\r\n- 对于一个强化学习 Agent，它由什么组成？\r\n\r\n  答：\r\n\r\n  1. **策略函数（policy function）**，Agent会用这个函数来选取它下一步的动作，包括**随机性策略（stochastic policy）**和**确定性策略（deterministic policy）**。\r\n\r\n  2. **价值函数（value function）**，我们用价值函数来对当前状态进行评估，即进入现在的状态，到底可以对你后面的收益带来多大的影响。当这个价值函数大的时候，说明你进入这个状态越有利。\r\n\r\n  3. **模型（model）**，其表示了 Agent 对这个Environment的状态进行的理解，它决定了这个系统是如何进行的。\r\n\r\n- 根据强化学习 Agent 的不同，我们可以将其分为哪几类？\r\n\r\n  答：\r\n\r\n  1. **基于价值函数的Agent**。 显式学习的就是价值函数，隐式的学习了它的策略。因为这个策略是从我们学到的价值函数里面推算出来的。\r\n  2. **基于策略的Agent**。它直接去学习 policy，就是说你直接给它一个 state，它就会输出这个动作的概率。然后在这个 policy-based agent 里面并没有去学习它的价值函数。\r\n  3. 然后另外还有一种 Agent 是把这两者结合。把 value-based 和 policy-based 结合起来就有了 **Actor-Critic agent**。这一类 Agent 就把它的策略函数和价值函数都学习了，然后通过两者的交互得到一个更佳的状态。\r\n\r\n- 基于策略迭代和基于价值迭代的强化学习方法有什么区别?\r\n\r\n  答：\r\n  \r\n  1. 基于策略迭代的强化学习方法，agent会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励；基于价值迭代的强化学习方法，agent不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。\r\n  2. 基于价值迭代的方法只能应用在不连续的、离散的环境下（如围棋或某些游戏领域），对于行为集合规模庞大、动作连续的场景（如机器人控制领域），其很难学习到较好的结果（此时基于策略迭代的方法能够根据设定的策略来选择连续的动作)；\r\n  3. 基于价值迭代的强化学习算法有 Q-learning、 Sarsa 等，而基于策略迭代的强化学习算法有策略梯度算法等。\r\n  4. 此外， Actor-Critic 算法同时使用策略和价值评估来做出决策，其中，智能体会根据策略做出动作，而价值函数会对做出的动作给出价值，这样可以在原有的策略梯度算法的基础上加速学习过程，取得更好的效果。\r\n\r\n- 有模型（model-based）学习和免模型（model-free）学习有什么区别？\r\n\r\n  答：针对是否需要对真实环境建模，强化学习可以分为有模型学习和免模型学习。\r\n  有模型学习是指根据环境中的经验，构建一个虚拟世界，同时在真实环境和虚拟世界中学习；免模型学习是指不对环境进行建模，直接与真实环境进行交互来学习到最优策略。总的来说，有模型学习相比于免模型学习仅仅多出一个步骤，即对真实环境进行建模。免模型学习通常属于数据驱动型方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略。免模型学习的泛化性要优于有模型学习，原因是有模型学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型学习算法的泛化性。\r\n\r\n- 强化学习的通俗理解\r\n\r\n  答：environment 跟 reward function 不是我们可以控制的，environment 跟 reward function 是在开始学习之前，就已经事先给定的。我们唯一能做的事情是调整 actor 里面的 policy，使得 actor 可以得到最大的 reward。Actor 里面会有一个 policy， 这个 policy 决定了actor 的行为。Policy 就是给一个外界的输入，然后它会输出 actor 现在应该要执行的行为。\r\n  \r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官: 看来你对于RL还是有一定了解的,那么可以用一句话谈一下你对于强化学习的认识吗?\r\n\r\n  答: 强化学习包含环境,动作和奖励三部分,其本质是agent通过与环境的交互,使得其作出的action所得到的决策得到的总的奖励达到最大,或者说是期望最大。\r\n\r\n- 高冷的面试官: 你认为强化学习与监督学习和无监督学习有什么区别?\r\n\r\n  答: 首先强化学习和无监督学习是不需要标签的,而监督学习需要许多有标签的样本来进行模型的构建;对于强化学习与无监督学习,无监督学习是直接对于给定的数据进行建模,寻找数据(特征)给定的隐藏的结构,一般对应的聚类问题,而强化学习需要通过延迟奖励学习策略来得到\"模型\"对于正确目标的远近(通过奖励惩罚函数进行判断),这里我们可以将奖励惩罚函数视为正确目标的一个稀疏、延迟形式。另外强化学习处理的多是序列数据,样本之间通常具有强相关性，但其很难像监督学习的样本一样满足IID条件。\r\n\r\n- 高冷的面试官: 根据你上面介绍的内容,你认为强化学习的使用场景有哪些呢?\r\n\r\n  答: 七个字的话就是多序列决策问题。或者说是对应的模型未知,需要通过学习逐渐逼近真实模型的问题并且当前的动作会影响环境的状态,即服从马尔可夫性的问题。同时应满足所有状态是可重复到达的(满足可学习型的)。\r\n\r\n- 高冷的面试官: 强化学习中所谓的损失函数与DL中的损失函数有什么区别呀?\r\n\r\n  答: DL中的loss function目的是使预测值和真实值之间的差距最小,而RL中的loss function是是奖励和的期望最大。\r\n\r\n- 高冷的面试官: 你了解model-free和model-based吗?两者有什么区别呢?\r\n\r\n  答: 两者的区别主要在于是否需要对于真实的环境进行建模, model-free不需要对于环境进行建模,直接与真实环境进行交互即可,所以其通常需要较大的数据或者采样工作来优化策略,这也帮助model-free对于真实环境具有更好的泛化性能; 而model-based 需要对于环境进行建模,同时再真实环境与虚拟环境中进行学习,如果建模的环境与真实环境的差异较大,那么会限制其泛化性能。现在通常使用model-free进行模型的构建工作。\r\n\r\n','2021-12-10 12:43:49','2021-12-19 19:21:49'),
	(36,3,'第二章 马尔科夫决策模型（MDP）','## 概述\n\n本章给大家介绍马尔可夫决策过程。\n\n* 在介绍马尔可夫决策过程之前，先介绍它的简化版本：马尔可夫链以及马尔可夫奖励过程，通过跟这两种过程的比较，我们可以更容易理解马尔可夫决策过程。\n* 第二部分会介绍马尔可夫决策过程中的 `policy evaluation`，就是当给定一个决策过后，怎么去计算它的价值函数。\n* 第三部分会介绍马尔可夫决策过程的控制，具体有两种算法：`policy iteration` 和 `value iteration`。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.2.png)\n\n上图介绍了在强化学习里面 agent 跟 environment 之间的交互，agent 在得到环境的状态过后，它会采取动作，它会把这个采取的动作返还给环境。环境在得到 agent 的动作过后，它会进入下一个状态，把下一个状态传回 agent。在强化学习中，agent 跟环境就是这样进行交互的，这个交互过程是可以通过马尔可夫决策过程来表示的，所以马尔可夫决策过程是强化学习里面的一个基本框架。\n\n在马尔可夫决策过程中，它的环境是全部可以观测的(`fully observable`)。但是很多时候环境里面有些量是不可观测的，但是这个部分观测的问题也可以转换成一个 MDP 的问题。\n\n在介绍马尔可夫决策过程(Markov Decision Process，MDP)之前，先给大家梳理一下马尔可夫过程(Markov Process，MP)、马尔可夫奖励过程(Markov Reward Processes，MRP)。这两个过程是马尔可夫决策过程的一个基础。\n\n## Markov Process(MP)\n\n### Markov Property\n\n如果一个状态转移是符合马尔可夫的，那就是说一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。\n\n我们设状态的历史为 $h_{t}=\\left\\{s_{1}, s_{2}, s_{3}, \\ldots, s_{t}\\right\\}$（$h_t$ 包含了之前的所有状态），如果一个状态转移是符合马尔可夫的，也就是满足如下条件：\n$$\np\\left(s_{t+1} \\mid s_{t}\\right) =p\\left(s_{t+1} \\mid h_{t}\\right) \\tag{1}\n$$\n\n$$\np\\left(s_{t+1} \\mid s_{t}, a_{t}\\right) =p\\left(s_{t+1} \\mid h_{t}, a_{t}\\right) \\tag{2}\n$$\n\n从当前 $s_t$ 转移到 $s_{t+1}$ 这个状态，它是直接就等于它之前所有的状态转移到 $s_{t+1}$。如果某一个过程满足`马尔可夫性质(Markov Property)`，就是说未来的转移跟过去是独立的，它只取决于现在。**马尔可夫性质是所有马尔可夫过程的基础。**\n\n### Markov Process/Markov Chain\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.5.png \':size=500\')\n\n首先看一看`马尔可夫链(Markov Chain)`。举个例子，这个图里面有四个状态，这四个状态从 $s_1,s_2,s_3,s_4$ 之间互相转移。比如说从 $s_1$ 开始，\n\n*  $s_1$ 有 0.1 的概率继续存活在 $s_1$ 状态，\n* 有 0.2 的概率转移到 $s_2$， \n* 有 0.7 的概率转移到 $s_4$ 。\n\n如果 $s_4$ 是我们当前状态的话，\n\n* 它有 0.3 的概率转移到 $s_2$ ，\n* 有 0.2 的概率转移到 $s_3$ ，\n* 有 0.5 的概率留在这里。\n\n我们可以用`状态转移矩阵(State Transition Matrix)` $P$ 来描述状态转移 $p\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s\\right)$，如下式所示。\n$$\nP=\\left[\\begin{array}{cccc}\nP\\left(s_{1} \\mid s_{1}\\right) & P\\left(s_{2} \\mid s_{1}\\right) & \\ldots & P\\left(s_{N} \\mid s_{1}\\right) \\\\\nP\\left(s_{1} \\mid s_{2}\\right) & P\\left(s_{2} \\mid s_{2}\\right) & \\ldots & P\\left(s_{N} \\mid s_{2}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP\\left(s_{1} \\mid s_{N}\\right) & P\\left(s_{2} \\mid s_{N}\\right) & \\ldots & P\\left(s_{N} \\mid s_{N}\\right)\n\\end{array}\\right]\n$$\n状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概念。所以它每一行其实描述了是从一个节点到达所有其它节点的概率。\n\n### Example of MP\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.6.png)\n\n上图是一个马尔可夫链的例子，我们这里有七个状态。比如说从 $s_1$ 开始到 $s_2$ ，它有 0.4 的概率，然后它有 0.6 的概率继续存活在它当前的状态。 $s_2$ 有 0.4 的概率到左边，有 0.4 的概率到 $s_3$ ，另外有 0.2 的概率存活在现在的状态，所以给定了这个状态转移的马尔可夫链后，我们可以对这个链进行采样，这样就会得到一串的轨迹。\n\n下面我们有三个轨迹，都是从同一个起始点开始。假设还是从 $s_3$ 这个状态开始，\n\n* 第一条链先到了 $s_4$， 又到了 $s_5$，又往右到了 $s_6$ ，然后继续存活在 $s_6$ 状态。\n* 第二条链从 $s_3$ 开始，先往左走到了 $s_2$ 。然后它又往右走，又回到了$s_3$ ，然后它又往左走，然后再往左走到了 $s_1$ 。\n* 通过对这个状态的采样，我们生成了很多这样的轨迹。\n\n## Markov Reward Process(MRP)\n\n**`马尔可夫奖励过程(Markov Reward Process, MRP)` 是马尔可夫链再加上了一个奖励函数。**在 MRP 中，转移矩阵和状态都是跟马尔可夫链一样的，多了一个`奖励函数(reward function)`。**奖励函数 $R$ 是一个期望**，就是说当你到达某一个状态的时候，可以获得多大的奖励，然后这里另外定义了一个 discount factor $\\gamma$ 。如果状态数是有限的，$R$ 可以是一个向量。\n\n### Example of MRP\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.8.png)\n\n这里是我们刚才看的马尔可夫链，如果把奖励也放上去的话，就是说到达每一个状态，我们都会获得一个奖励。这里我们可以设置对应的奖励，比如说到达 $s_1$ 状态的时候，可以获得 5 的奖励，到达 $s_7$ 的时候，可以得到 10 的奖励，其它状态没有任何奖励。因为这里状态是有限的，所以我们可以用向量 $R=[5,0,0,0,0,0,10]$ 来表示这个奖励函数，这个向量表示了每个点的奖励大小。\n\n我们通过一个形象的例子来理解 MRP。我们把一个纸船放到河流之中，那么它就会随着这个河流而流动，它自身是没有动力的。所以你可以把 MRP 看成是一个随波逐流的例子，当我们从某一个点开始的时候，这个纸船就会随着事先定义好的状态转移进行流动，它到达每个状态过后，我们就有可能获得一些奖励。\n\n### Return and Value function\n\n这里我们进一步定义一些概念。\n\n*  `Horizon` 是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数决定的。\n\n* `Return(回报)` 说的是把奖励进行折扣后所获得的收益。Return 可以定义为奖励的逐步叠加，如下式所示：\n\n$$\nG_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\ldots+\\gamma^{T-t-1} R_{T}\n$$\n\n这里有一个叠加系数，越往后得到的奖励，折扣得越多。这说明我们其实更希望得到现有的奖励，未来的奖励就要把它打折扣。\n\n* 当我们有了 return 过后，就可以定义一个状态的价值了，就是 `state value function`。对于 MRP，state value function 被定义成是 return 的期望，如下式所示：\n  $$\n  \\begin{aligned}\n  V_{t}(s) &=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s\\right] \\\\\n  &=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots+\\gamma^{T-t-1} R_{T} \\mid s_{t}=s\\right]\n  \\end{aligned}\n  $$\n\n$G_t$ 是之前定义的 `discounted return`，我们这里取了一个期望，期望就是说从这个状态开始，你有可能获得多大的价值。所以这个期望也可以看成是对未来可能获得奖励的当前价值的一个表现，就是当你进入某一个状态过后，你现在就有多大的价值。\n\n### Why Discount Factor\n\n**这里解释一下为什么需要 discount factor。**\n\n* 有些马尔可夫过程是带环的，它并没有终结，我们想避免这个无穷的奖励。\n* 我们并没有建立一个完美的模拟环境的模型，也就是说，我们对未来的评估不一定是准确的，我们不一定完全信任我们的模型，因为这种不确定性，所以我们对未来的预估增加一个折扣。我们想把这个不确定性表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励。\n* 如果这个奖励是有实际价值的，我们可能是更希望立刻就得到奖励，而不是后面再得到奖励（现在的钱比以后的钱更有价值）。\n* 在人的行为里面来说的话，大家也是想得到即时奖励。\n* 有些时候可以把这个系数设为 0，$\\gamma=0$：我们就只关注了它当前的奖励。我们也可以把它设为 1，$\\gamma=1$：对未来并没有折扣，未来获得的奖励跟当前获得的奖励是一样的。\n\nDiscount factor 可以作为强化学习 agent 的一个超参数来进行调整，然后就会得到不同行为的 agent。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.11.png)\n\n这里我们再来看一看，在这个 MRP 里面，如何计算它的价值。这个 MRP 依旧是这个状态转移。它的奖励函数是定义成这样，它在进入第一个状态的时候会得到 5 的奖励，进入第七个状态的时候会得到 10 的奖励，其它状态都没有奖励。\n\n我们现在可以计算每一个轨迹得到的奖励，比如我们对于这个 $s_4,s_5,s_6,s_7$ 轨迹的奖励进行计算，这里折扣系数是 0.5。\n\n* 在 $s_4$ 的时候，奖励为零。\n\n* 下一个状态 $s_5$ 的时候，因为我们已经到了下一步了，所以我们要把 $s_5$ 进行一个折扣，$s_5$ 本身也是没有奖励的。\n* 然后是到 $s_6$，也没有任何奖励，折扣系数应该是 $\\frac{1}{4}$ 。\n* 到达 $s_7$ 后，我们获得了一个奖励，但是因为 $s_7$ 这个状态是未来才获得的奖励，所以我们要进行三次折扣。\n\n所以对于这个轨迹，它的 return 就是一个 1.25，类似地，我们可以得到其它轨迹的 return 。\n\n这里就引出了一个问题，当我们有了一些轨迹的实际 return，怎么计算它的价值函数。比如说我们想知道 $s_4$ 状态的价值，就是当你进入 $s_4$ 后，它的价值到底如何。一个可行的做法就是说我们可以产生很多轨迹，然后把这里的轨迹都叠加起来。比如我们可以从 $s_4$ 开始，采样生成很多轨迹，都把它的 return 计算出来，然后可以直接把它取一个平均作为你进入 $s_4$ 它的价值。这其实是一种计算价值函数的办法，通过这个蒙特卡罗采样的办法计算 $s_4$ 的状态。接下来会进一步介绍蒙特卡罗算法。\n\n### Bellman Equation\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.12.png)\n\n但是这里我们采取了另外一种计算方法，我们从这个价值函数里面推导出 `Bellman Equation（贝尔曼等式）`，如下式所示：\n$$\nV(s)=\\underbrace{R(s)}_{\\text {Immediate reward }}+\\underbrace{\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)}_{\\text {Discounted sum of future reward }}\n$$\n其中：\n\n*  $s\'$ 可以看成未来的所有状态。\n* 转移 $P(s\'|s)$  是指从当前状态转移到未来状态的概率。\n* $V(s\')$ 代表的是未来某一个状态的价值。我们从当前这个位置开始，有一定的概率去到未来的所有状态，所以我们要把这个概率也写上去，这个转移矩阵也写上去，然后我们就得到了未来状态，然后再乘以一个 $\\gamma$，这样就可以把未来的奖励打折扣。\n* 第二部分可以看成是未来奖励的折扣总和(Discounted sum of future reward)。\n\n**Bellman Equation 定义了当前状态跟未来状态之间的这个关系。**\n\n未来打了折扣的奖励加上当前立刻可以得到的奖励，就组成了这个 Bellman Equation。\n\n#### Law of Total Expectation\n\n在推导 Bellman equation 之前，我们可以仿照`Law of Total Expectation(全期望公式)`的证明过程来证明下面的式子：\n$$\n\\mathbb{E}[V(s_{t+1})|s_t]=\\mathbb{E}[\\mathbb{E}[G_{t+1}|s_{t+1}]|s_t]=E[G_{t+1}|s_t]\n$$\n\n> Law of total expectation 也被称为 law of iterated expectations(LIE)。如果 $A_i$ 是样本空间的有限或可数的划分(partition)，则全期望公式可以写成如下形式：\n> $$\n> \\mathrm{E}(X)=\\sum_{i} \\mathrm{E}\\left(X \\mid A_{i}\\right) \\mathrm{P}\\left(A_{i}\\right)\n> $$\n\n**证明：**\n\n为了记号简洁并且易读，我们丢掉了下标，令 $s=s_t,g\'=G_{t+1},s\'=s_{t+1}$。我们可以根据条件期望的定义来重写这个回报的期望为：\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right] &=\\mathbb{E}\\left[g^{\\prime} \\mid s^{\\prime}\\right] \\\\\n&=\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime}\\right)\n\\end{aligned}\n$$\n> 如果 $X$ 和 $Y$ 都是离散型随机变量，则条件期望（Conditional Expectation）$E(X|Y=y)$的定义如下式所示：\n> $$\n> \\mathrm{E}(X \\mid Y=y)=\\sum_{x} x P(X=x \\mid Y=y)\n> $$\n\n令 $s_t=s$，我们对 $\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right]$ 求期望可得：\n$$\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbb{E}\\left[G_{t+1} \\mid s_{t+1}\\right] \\mid s_{t}\\right] \n&=\\mathbb{E} \\left[\\mathbb{E}\\left[g^{\\prime} \\mid s^{\\prime}\\right] \\mid s\\right]\\\\\n&=\\mathbb{E} \\left[\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime}\\right)\\mid s\\right]\\\\\n&= \\sum_{s^{\\prime}}\\sum_{g^{\\prime}} g^{\\prime}~p\\left(g^{\\prime} \\mid s^{\\prime},s\\right)p(s^{\\prime} \\mid s)\\\\\n&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime} \\mid s^{\\prime}, s\\right) p\\left(s^{\\prime} \\mid s\\right) p(s)}{p(s)} \\\\\n&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime} \\mid s^{\\prime}, s\\right) p\\left(s^{\\prime}, s\\right)}{p(s)} \\\\\n&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} \\frac{g^{\\prime} p\\left(g^{\\prime}, s^{\\prime}, s\\right)}{p(s)} \\\\\n&=\\sum_{s^{\\prime}} \\sum_{g^{\\prime}} g^{\\prime} p\\left(g^{\\prime}, s^{\\prime} \\mid s\\right) \\\\\n&=\\sum_{g^{\\prime}} \\sum_{s^{\\prime}} g^{\\prime} p\\left(g^{\\prime}, s^{\\prime} \\mid s\\right) \\\\\n&=\\sum_{g^{\\prime}} g^{\\prime} p\\left(g^{\\prime} \\mid s\\right) \\\\\n&=\\mathbb{E}\\left[g^{\\prime} \\mid s\\right]=\\mathbb{E}\\left[G_{t+1} \\mid s_{t}\\right]\n\\end{aligned}\n$$\n\n#### Bellman Equation Derivation\n\nBellman equation 的推导过程如下：\n$$\n\\begin{aligned}\nV(s)&=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s\\right]\\\\\n&=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid s_{t}=s\\right]  \\\\\n&=\\mathbb{E}\\left[R_{t+1}|s_t=s\\right] +\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\ldots \\mid s_{t}=s\\right]\\\\\n&=R(s)+\\gamma \\mathbb{E}[G_{t+1}|s_t=s] \\\\\n&=R(s)+\\gamma \\mathbb{E}[V(s_{t+1})|s_t=s]\\\\\n&=R(s)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)\n\\end{aligned}\n$$\n\n>Bellman Equation 就是当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，也叫作“动态规划方程”。\n\n**Bellman Equation 定义了状态之间的迭代关系，如下式所示。**\n$$\nV(s)=R(s)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s\\right) V\\left(s^{\\prime}\\right)\n$$\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.13.png)\n\n假设有一个马尔可夫转移矩阵是右边这个样子，Bellman Equation 描述的就是当前状态到未来状态的一个转移。假设我们当前是在 $s_1$， 那么它只可能去到三个未来的状态：有 0.1 的概率留在它当前这个位置，有 0.2 的概率去到 $s_2$ 状态，有 0.7 的概率去到 $s_4$ 的状态，所以我们要把这个转移乘以它未来的状态的价值，再加上它的 immediate reward 就会得到它当前状态的价值。**所以 Bellman Equation 定义的就是当前状态跟未来状态的一个迭代的关系。**\n\n我们可以把 Bellman Equation 写成一种矩阵的形式，如下式所示。\n$$\n\\left[\\begin{array}{c}\nV\\left(s_{1}\\right) \\\\\nV\\left(s_{2}\\right) \\\\\n\\vdots \\\\\nV\\left(s_{N}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{c}\nR\\left(s_{1}\\right) \\\\\nR\\left(s_{2}\\right) \\\\\n\\vdots \\\\\nR\\left(s_{N}\\right)\n\\end{array}\\right]+\\gamma\\left[\\begin{array}{cccc}\nP\\left(s_{1} \\mid s_{1}\\right) & P\\left(s_{2} \\mid s_{1}\\right) & \\ldots & P\\left(s_{N} \\mid s_{1}\\right) \\\\\nP\\left(s_{1} \\mid s_{2}\\right) & P\\left(s_{2} \\mid s_{2}\\right) & \\ldots & P\\left(s_{N} \\mid s_{2}\\right) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nP\\left(s_{1} \\mid s_{N}\\right) & P\\left(s_{2} \\mid s_{N}\\right) & \\ldots & P\\left(s_{N} \\mid s_{N}\\right)\n\\end{array}\\right]\\left[\\begin{array}{c}\nV\\left(s_{1}\\right) \\\\\nV\\left(s_{2}\\right) \\\\\n\\vdots \\\\\nV\\left(s_{N}\\right)\n\\end{array}\\right]\n$$\n首先有这个转移矩阵。我们当前这个状态是一个向量  $[V(s_1),V(s_2),\\cdots,V(s_N)]^T$。我们可以写成迭代的形式。我们每一行来看的话，$V$ 这个向量乘以了转移矩阵里面的某一行，再加上它当前可以得到的 reward，就会得到它当前的价值。\n\n当我们把 Bellman Equation 写成矩阵形式后，可以直接求解：\n$$\n\\begin{aligned}\nV &= R+ \\gamma PV \\\\\nIV &= R+ \\gamma PV \\\\\n(I-\\gamma P)V &=R \\\\\nV&=(I-\\gamma P)^{-1}R\n\\end{aligned}\n$$\n\n我们可以直接得到一个`解析解(analytic solution)`:\n$$\nV=(I-\\gamma P)^{-1} R\n$$\n我们可以通过矩阵求逆把这个 V 的这个价值直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以当状态非常多的时候，比如说从十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的矩阵，这样一个大矩阵的话求逆是非常困难的，**所以这种通过解析解去求解的方法只适用于很小量的 MRP。**\n\n### Iterative Algorithm for Computing Value of a MRP\n\n接下来我们来求解这个价值函数。**我们可以通过迭代的方法来解这种状态非常多的 MRP(large MRPs)，**比如说：\n\n* 动态规划的方法，\n* 蒙特卡罗的办法(通过采样的办法去计算它)，\n* 时序差分学习(Temporal-Difference Learning)的办法。 `Temporal-Difference Learning` 叫 `TD Leanring`，它是动态规划和蒙特卡罗的一个结合。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.16.png)\n\n**首先我们用蒙特卡罗(Monte Carlo)的办法来计算它的价值函数。**蒙特卡罗就是说当得到一个 MRP 过后，我们可以从某一个状态开始，把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的折扣的奖励 $g$ 算出来。算出来过后就可以把它积累起来，得到 return $G_t$。 当积累到一定的轨迹数量过后，直接用 $G_t$ 除以轨迹数量，就会得到它的价值。\n\n比如说我们要算 $s_4$ 状态的价值。\n\n* 我们就可以从 $s_4$ 状态开始，随机产生很多轨迹，就是说产生很多小船，把小船扔到这个转移矩阵里面去，然后它就会随波逐流，产生轨迹。\n* 每个轨迹都会得到一个 return，我们得到大量的 return，比如说一百个、一千个 return ，然后直接取一个平均，那么就可以等价于现在 $s_4$ 这个价值，因为 $s_4$ 的价值 $V(s_4)$  定义了你未来可能得到多少的奖励。这就是蒙特卡罗采样的方法。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.17.png)\n\n**我们也可以用这个动态规划的办法**，一直去迭代它的 Bellman equation，让它最后收敛，我们就可以得到它的一个状态。所以在这里算法二就是一个迭代的算法，通过 `bootstrapping(自举)`的办法，然后去不停地迭代这个 Bellman Equation。当这个最后更新的状态跟你上一个状态变化并不大的时候，更新就可以停止，我们就可以输出最新的 $V\'(s)$ 作为它当前的状态。所以这里就是把 Bellman Equation 变成一个 Bellman Update，这样就可以得到它的一个价值。\n\n动态规划的方法基于后继状态值的估计来更新状态值的估计（算法二中的第 3 行用 $V\'$ 来更新 $V$ ）。也就是说，它们根据其他估算值来更新估算值。我们称这种基本思想为 bootstrapping。\n\n>Bootstrap 本意是“解靴带”；这里是在使用徳国文学作品《吹牛大王历险记》中解靴带自助(拔靴自助)的典故，因此将其译为“自举”。\n\n## Markov Decision Process(MDP)\n\n### MDP\n\n**相对于 MRP，`马尔可夫决策过程(Markov Decision Process)`多了一个 `decision`，其它的定义跟 MRP 都是类似的**:\n\n* 这里多了一个决策，多了一个动作。\n* 状态转移也多了一个条件，变成了 $P\\left(s_{t+1}=s^{\\prime} \\mid s_{t}=s, a_{t}=a\\right)$。你采取某一种动作，然后你未来的状态会不同。未来的状态不仅是依赖于你当前的状态，也依赖于在当前状态 agent 采取的这个动作。\n* 对于这个价值函数，它也是多了一个条件，多了一个你当前的这个动作，变成了 $R\\left(s_{t}=s, a_{t}=a\\right)=\\mathbb{E}\\left[r_{t} \\mid s_{t}=s, a_{t}=a\\right]$。你当前的状态以及你采取的动作会决定你在当前可能得到的奖励多少。\n\n### Policy in MDP\n\n* Policy 定义了在某一个状态应该采取什么样的动作。\n\n* 知道当前状态过后，我们可以把当前状态带入 policy function，然后就会得到一个概率，即 \n$$\n\\pi(a \\mid s)=P\\left(a_{t}=a \\mid s_{t}=s\\right)\n$$\n\n概率就代表了在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示。\n\n* 另外这个策略也可能是确定的，它有可能是直接输出一个值。或者就直接告诉你当前应该采取什么样的动作，而不是一个动作的概率。\n\n* 假设这个概率函数应该是稳定的(stationary)，不同时间点，你采取的动作其实都是对这个 policy function 进行采样。\n\n我们可以将 MRP 转换成 MDP。已知一个 MDP 和一个 policy $\\pi$ 的时候，我们可以把 MDP 转换成 MRP。\n\n在 MDP 里面，转移函数 $P(s\'|s,a)$  是基于它当前状态以及它当前的 action。因为我们现在已知它 policy function，就是说在每一个状态，我们知道它可能采取的动作的概率，那么就可以直接把这个 action 进行加和，直接把这个 a 去掉，那我们就可以得到对于 MRP 的一个转移，这里就没有 action。\n\n$$\n P^{\\pi}\\left(s^{\\prime} \\mid s\\right)=\\sum_{a \\in A} \\pi(a \\mid s) P\\left(s^{\\prime} \\mid s, a\\right)\n$$\n\n对于这个奖励函数，我们也可以把 action 拿掉，这样就会得到一个类似于 MRP 的奖励函数。\n\n$$\nR^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) R(s, a)\n$$\n\n### Comparison of MP/MRP and MDP\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.21.png)\n\n\n\n**这里我们看一看，MDP 里面的状态转移跟 MRP 以及 MP 的一个差异。**\n\n* 马尔可夫过程的转移是直接就决定。比如当前状态是 s，那么就直接通过这个转移概率决定了下一个状态是什么。\n* 但对于 MDP，它的中间多了一层这个动作 a ，就是说在你当前这个状态的时候，首先要决定的是采取某一种动作，那么你会到了某一个黑色的节点。到了这个黑色的节点，因为你有一定的不确定性，当你当前状态决定过后以及你当前采取的动作过后，你到未来的状态其实也是一个概率分布。**所以在这个当前状态跟未来状态转移过程中这里多了一层决策性，这是 MDP 跟之前的马尔可夫过程很不同的一个地方。**在马尔可夫决策过程中，动作是由 agent 决定，所以多了一个 component，agent 会采取动作来决定未来的状态转移。\n\n### Value function for MDP\n\n顺着 MDP 的定义，我们可以把 `状态-价值函数(state-value function)`，就是在 MDP 里面的价值函数也进行一个定义，它的定义是跟 MRP 是类似的，如式 (3)  所示：\n$$\nv^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid s_{t}=s\\right] \\tag{3}\n$$\n但是这里 expectation over policy，就是这个期望是基于你采取的这个 policy ，就当你的 policy 决定过后，**我们通过对这个 policy 进行采样来得到一个期望，那么就可以计算出它的这个价值函数。**\n\n这里我们另外引入了一个 `Q 函数(Q-function)`。Q 函数也被称为 `action-value function`。**Q 函数定义的是在某一个状态采取某一个动作，它有可能得到的这个 return 的一个期望**，如式 (4) 所示：\n$$\nq^{\\pi}(s, a)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid s_{t}=s, A_{t}=a\\right] \\tag{4}\n$$\n这里期望其实也是 over policy function。所以你需要对这个 policy function 进行一个加和，然后得到它的这个价值。\n**对 Q 函数中的动作函数进行加和，就可以得到价值函数**，如式 (5) 所示：\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{5}\n$$\n#### Q-function Bellman Equation\n\n此处我们给出 Q 函数的 Bellman equation：\n\n$$\n\\begin{aligned}\nq(s,a)&=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s,a_{t}=a\\right]\\\\\n&=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid s_{t}=s,a_{t}=a\\right]  \\\\\n&=\\mathbb{E}\\left[R_{t+1}|s_{t}=s,a_{t}=a\\right] +\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\ldots \\mid s_{t}=s,a_{t}=a\\right]\\\\\n&=R(s,a)+\\gamma \\mathbb{E}[G_{t+1}|s_{t}=s,a_{t}=a] \\\\\n&=R(s,a)+\\gamma \\mathbb{E}[V(s_{t+1})|s_{t}=s,a_{t}=a]\\\\\n&=R(s,a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s,a\\right) V\\left(s^{\\prime}\\right)\n\\end{aligned}\n$$\n\n\n### Bellman Expectation Equation\n\n**我们可以把状态-价值函数和 Q 函数拆解成两个部分：即时奖励(immediate reward) 和后续状态的折扣价值(discounted value of successor state)。**\n\n通过对状态-价值函数进行一个分解，我们就可以得到一个类似于之前 MRP 的 Bellman Equation，这里叫 `Bellman Expectation Equation`，如式 (6) 所示：\n$$\nv^{\\pi}(s)=E_{\\pi}\\left[R_{t+1}+\\gamma v^{\\pi}\\left(s_{t+1}\\right) \\mid s_{t}=s\\right] \\tag{6}\n$$\n对于 Q 函数，我们也可以做类似的分解，也可以得到 Q 函数的 Bellman Expectation Equation，如式 (7) 所示：\n$$\nq^{\\pi}(s, a)=E_{\\pi}\\left[R_{t+1}+\\gamma q^{\\pi}\\left(s_{t+1}, A_{t+1}\\right) \\mid s_{t}=s, A_{t}=a\\right] \\tag{7}\n$$\n**Bellman expectation equation 定义了你当前状态跟未来状态之间的一个关联。**\n\n我们进一步进行一个简单的分解。\n\n我们先给出等式 (8)：\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{8}\n$$\n再给出等式 (9)：\n$$\nq^{\\pi}(s, a)=R_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right) \\tag{9}\n$$\n**等式 (8) 和等式 (9) 代表了价值函数跟 Q 函数之间的一个关联。**\n\n也可以把等式 (9) 插入等式 (8) 中，得到等式 (10)：\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right) \\tag{10}\n$$\n**等式 (10) 代表了当前状态的价值跟未来状态价值之间的一个关联。**\n\n我们把等式 (8) 插入到等式 (9)，就可以得到等式 (11)：\n$$\nq^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{11}\n$$\n**等式 (11) 代表了当前时刻的 Q 函数跟未来时刻的 Q 函数之间的一个关联。**\n\n**等式  (10) 和等式 (11)  是 Bellman expectation equation 的另一种形式。**\n\n### Backup Diagram\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.25.png)\n\n这里有一个概念叫 `backup`。Backup 类似于 bootstrapping 之间这个迭代关系，就对于某一个状态，它的当前价值是跟它的未来价值线性相关的。\n\n我们把上面这样的图称为 `backup diagram(备份图)`，因为它们图示的关系构成了更新或备份操作的基础，而这些操作是强化学习方法的核心。这些操作将价值信息从一个状态（或状态-动作对）的后继状态（或状态-动作对）转移回它。\n\n每一个空心圆圈代表一个状态，每一个实心圆圈代表一个状态-动作对。\n\n\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right) \\tag{12}\n$$\n如式 (12) 所示，我们这里有两层加和：\n\n* 第一层加和就是这个叶子节点，往上走一层的话，我们就可以把未来的价值($s\'$ 的价值) backup 到黑色的节点。\n* 第二层加和是对 action 进行加和。得到黑色节点的价值过后，再往上 backup 一层，就会推到根节点的价值，即当前状态的价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/state_value_function_backup.png \':size=650\')\n\n上图是状态-价值函数的计算分解图，上图 B 计算公式为\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s) q^{\\pi}(s, a) \\tag{i}\n$$\n上图 B 给出了状态-价值函数与 Q 函数之间的关系。上图 C 计算 Q 函数为\n$$\nq^{\\pi}(s,a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right) \\tag{ii}\n$$\n\n将式 (ii) 代入式 (i) 可得：\n$$\nv^{\\pi}(s)=\\sum_{a \\in A} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi}\\left(s^{\\prime}\\right)\\right)\n$$\n**所以 backup diagram 定义了未来下一时刻的状态-价值函数跟上一时刻的状态-价值函数之间的关联。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.26.png)\n\n对于 Q 函数，我们也可以进行这样的一个推导。现在的根节点是这个 Q 函数的一个节点。Q 函数对应于黑色的节点。我们下一时刻的 Q 函数是叶子节点，有四个黑色节点。\n$$\nq^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{13}\n$$\n如式 (13) 所示，我们这里也有两个加和：\n\n* 第一层加和是先把这个叶子节点从黑色节点推到这个白色的节点，进了它的这个状态。\n* 当我们到达某一个状态过后，再对这个白色节点进行一个加和，这样就把它重新推回到当前时刻的一个 Q 函数。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/q_function_backup.png \':size=650\')\n\n在上图 C 中，\n$$\nv^{\\pi}\\left(s^{\\prime}\\right)=\\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) \\tag{iii}\n$$\n将式 (iii) 代入式 (ii) 可得到 Q 函数：\n$$\nq^{\\pi}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) q^{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\n$$\n**所以这个等式就决定了未来 Q 函数跟当前 Q 函数之间的这个关联。**\n\n### Policy Evaluation(Prediction)\n\n* 当我们知道一个 MDP 以及要采取的策略 $\\pi$ ，计算价值函数 $v^{\\pi}(s)$ 的过程就是 `policy evaluation`。就像我们在评估这个策略，我们会得到多大的奖励。\n* **Policy evaluation 在有些地方也被叫做 `(value) prediction`，也就是预测你当前采取的这个策略最终会产生多少的价值。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.28.png)\n\n* MDP，你其实可以把它想象成一个摆渡的人在这个船上面，她可以控制这个船的移动，这样就避免了这个船随波逐流。因为在每一个时刻，这个人会决定采取什么样的一个动作，这样会把这个船进行导向。\n\n* MRP 跟 MP 的话，这个纸的小船会随波逐流，然后产生轨迹。\n* MDP 的不同就是有一个 agent 去控制这个船，这样我们就可以尽可能多地获得奖励。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.29.png)\n\n我们再看下 policy evaluation 的例子，怎么在决策过程里面计算它每一个状态的价值。\n\n* 假设环境里面有两种动作：往左走和往右走。\n* 现在的奖励函数应该是关于动作以及状态两个变量的一个函数。但我们这里规定，不管你采取什么动作，只要到达状态 $s_1$，就有 5 的奖励。只要你到达状态 $s_7$ 了，就有 10 的奖励，中间没有任何奖励。\n* 假设我们现在采取的一个策略，这个策略是说不管在任何状态，我们采取的策略都是往左走。假设价值折扣因子是零，那么对于确定性策略(deterministic policy)，最后估算出的价值函数是一致的，即\n\n$$\nV^{\\pi}=[5,0,0,0,0,0,10]\n$$\n\nQ: 怎么得到这个结果？\n\nA: 我们可以直接在去 run 下面这个 iterative equation：\n$$\nv_{k}^{\\pi}(s)=r(s, \\pi(s))+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, \\pi(s)\\right) v_{k-1}^{\\pi}\\left(s^{\\prime}\\right)\n$$\n就把 Bellman expectation equation 拿到这边来，然后不停地迭代，最后它会收敛。收敛过后，它的值就是它每一个状态的价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.30.png)\n\n再来看一个例子(practice 1)，如果折扣因子是 0.5，我们可以通过下面这个等式进行迭代：\n$$\nv_{t}^{\\pi}(s)=\\sum_{a} P(\\pi(s)=a)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v_{t-1}^{\\pi}\\left(s^{\\prime}\\right)\\right)\n$$\n然后就会得到它的状态价值。\n\n另外一个例子(practice 2)，就是说我们现在采取的 policy 在每个状态下，有 0.5 的概率往左走，有 0.5 的概率往右走，那么放到这个状态里面去如何计算。其实也是把这个 Bellman expectation equation 拿出来，然后进行迭代就可以算出来了。一开始的时候，我们可以初始化，不同的 $v(s\')$ 都会有一个值，放到 Bellman expectation equation 里面去迭代，然后就可以算出它的状态价值。\n\n### Prediction and Control\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.31.png)\n\nMDP 的 `prediction` 和 `control` 是 MDP 里面的核心问题。\n\n* 预测问题：\n  * 输入：MDP $<S,A,P,R,\\gamma>$ 和 policy $\\pi$  或者 MRP $<S,P^{\\pi},R^{\\pi},\\gamma>$。\n  * 输出：value function $v^{\\pi}$。\n  * Prediction 是说给定一个 MDP 以及一个 policy $\\pi$ ，去计算它的 value function，就对于每个状态，它的价值函数是多少。\n\n* 控制问题：\n  * 输入：MDP  $<S,A,P,R,\\gamma>$。\n  * 输出：最佳价值函数(optimal value function) $v^*$ 和最佳策略(optimal policy) $\\pi^*$。\n  * Control 就是说我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数以及最佳策略。\n* 在 MDP 里面，prediction 和 control 都可以通过动态规划去解决。\n* 要强调的是，这两者的区别就在于，\n  * 预测问题是**给定一个 policy**，我们要确定它的 value function 是多少。\n  * 而控制问题是在**没有 policy 的前提下**，我们要确定最优的 value function 以及对应的决策方案。\n* **实际上，这两者是递进的关系，在强化学习中，我们通过解决预测问题，进而解决控制问题。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/prediction_example.png)\n\n**举一个例子来说明 prediction 与 control 的区别。**\n\n首先是**预测问题**：\n\n* 在上图的方格中，我们规定从 A $\\to$ A\' 可以得到 +10 的奖励，从 B $\\to$ B\' 可以得到 +5 的奖励，其它步骤的奖励为 -1。\n* 现在，我们给定一个 policy：在任何状态中，它的行为模式都是随机的，也就是上下左右的概率各 25%。\n* 预测问题要做的就是，在这种决策模式下，我们的 value function 是什么。上图 b 是对应的 value function。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/control_example.png)\n\n\n\n接着是**控制问题**：\n\n* 在控制问题中，问题背景与预测问题相同，唯一的区别就是：不再限制 policy。也就是说行为模式是未知的，我们要自己确定。\n* 所以我们通过解决控制问题，求得每一个状态的最优的 value function（如上图 b 所示），也得到了最优的 policy（如上图 c 所示）。\n\n* 控制问题要做的就是，给定同样的条件，在所有可能的策略下最优的价值函数是什么？最优策略是什么？\n\n### Dynamic Programming\n\n`动态规划(Dynamic Programming，DP)`适合解决满足如下两个性质的问题：\n\n* `最优子结构(optimal substructure)`。最优子结构意味着，我们的问题可以拆分成一个个的小问题，通过解决这个小问题，最后，我们能够通过组合小问题的答案，得到大问题的答案，即最优的解。\n* `重叠子问题(Overlapping subproblems)`。重叠子问题意味着，子问题出现多次，并且子问题的解决方案能够被重复使用。\n\nMDP 是满足动态规划的要求的，\n\n* 在 Bellman equation 里面，我们可以把它分解成一个递归的结构。当我们把它分解成一个递归的结构的时候，如果我们的子问题子状态能得到一个值，那么它的未来状态因为跟子状态是直接相连的，那我们也可以继续推算出来。\n* 价值函数就可以储存并重用它的最佳的解。\n\n动态规划应用于 MDP 的规划问题(planning)而不是学习问题(learning)，我们必须对环境是完全已知的(Model-Based)，才能做动态规划，直观的说，就是要知道状态转移概率和对应的奖励才行\n\n动态规划能够完成预测问题和控制问题的求解，是解 MDP prediction 和 control 一个非常有效的方式。\n\n### Policy Evaluation on MDP\n\n**Policy evaluation 就是给定一个 MDP 和一个 policy，我们可以获得多少的价值。**就对于当前这个策略，我们可以得到多大的 value function。\n\n这里有一个方法是说，我们直接把这个 `Bellman Expectation Backup` 拿过来，变成一个迭代的过程，这样反复迭代直到收敛。这个迭代过程可以看作是 `synchronous backup` 的过程。\n\n> 同步备份(synchronous backup)是指每一次的迭代都会完全更新所有的状态，这样对于程序资源需求特别大。异步备份(asynchronous backup)的思想就是通过某种方式，使得每一次迭代不需要更新所有的状态，因为事实上，很多的状态也不需要被更新。\n\n$$\nv_{t+1}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v_{t}\\left(s^{\\prime}\\right)\\right) \\tag{14}\n$$\n* 等式 (14) 说的是说我们可以把 Bellman Expectation Backup 转换成一个动态规划的迭代。\n* 当我们得到上一时刻的 $v_t$ 的时候，就可以通过这个递推的关系来推出下一时刻的值。\n* 反复去迭代它，最后它的值就是从 $v_1,v_2$ 到最后收敛过后的这个值。这个值就是当前给定的 policy 对应的价值函数。\n\nPolicy evaluation 的核心思想就是把如下式所示的 Bellman expectation backup 拿出来反复迭代，然后就会得到一个收敛的价值函数的值。\n$$\nv_{t+1}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v_{t}\\left(s^{\\prime}\\right)\\right) \\tag{15}\n$$\n因为已经给定了这个函数的 policy  function，那我们可以直接把它简化成一个 MRP 的表达形式，这样的话，形式就更简洁一些，就相当于我们把这个 $a$  去掉，如下式所示：\n$$\nv_{t+1}(s)=R^{\\pi}(s)+\\gamma P^{\\pi}\\left(s^{\\prime} \\mid s\\right) v_{t}\\left(s^{\\prime}\\right) \\tag{16}\n$$\n这样它就只有价值函数跟转移函数了。通过去迭代这个更简化的一个函数，我们也可以得到它每个状态的价值。因为不管是在 MRP 以及 MDP，它的价值函数包含的这个变量都是只跟这个状态有关，就相当于进入某一个状态，未来可能得到多大的价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.35.png)\n\n* 比如现在的环境是一个 small gridworld。这个 agent 的目的是从某一个状态开始，然后到达终点状态。它的终止状态就是左上角跟右下角，这里总共有 14 个状态，因为我们把每个位置用一个状态来表示。\n* 这个 agent 采取的动作，它的 policy function 就直接先给定了，它在每一个状态都是随机游走，它们在每一个状态就是上下左右行走。它在边缘状态的时候，比如说在第四号状态的时候，它往左走的话，它是依然存在第四号状态，我们加了这个限制。\n\n* 这里我们给的奖励函数就是说你每走一步，就会得到 -1 的奖励，所以 agent 需要尽快地到达终止状态。\n* 状态之间的转移也是确定的。比如从第六号状态往上走，它就会直接到达第二号状态。很多时候有些环境是 `概率性的(probabilistic)`， 就是说 agent 在第六号状态，它选择往上走的时候，有可能地板是滑的，然后它可能滑到第三号状态或者第一号状态，这就是有概率的一个转移。但这里把这个环境进行了简化，从六号往上走，它就到了二号。\n* 所以直接用这个迭代来解它，因为我们已经知道每一个概率以及它的这个概率转移，那么就直接可以进行一个简短的迭代，这样就会算出它每一个状态的价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.36.png)\n\n我们再来看一个动态的例子，首先推荐斯坦福大学的一个网站：[GridWorld: Dynamic Programming Demo](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html) ，这个网站模拟了单步更新的过程中，所有格子的一个状态价值的变化过程。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.37.png \':size=550\')\n\n这里有很多格子，每个格子都代表了一个状态。在每个格子里面有一个初始值零。然后在每一个状态，它还有一些箭头，这个箭头就是说它在当前这个状态应该采取什么样的策略。我们这里采取一个随机的策略，不管它在哪一个状态，它上下左右的概率都是相同的。比如在某个状态，它都有上下左右 0.25 的概率采取某一个动作，所以它的动作是完全随机的。\n\n在这样的环境里面，我们想计算它每一个状态的价值。我们也定义了它的 reward function，你可以看到有些状态上面有一个 R 的值。比如我们这边有些值是为负的，我们可以看到格子里面有几个 -1 的奖励，只有一个 +1 奖励的格子。在这个棋盘的中间这个位置，可以看到有一个 R 的值是 1.0，为正的一个价值函数。 所以每个状态对应了一个值，然后有一些状态没有任何值，就说明它的这个 reward function，它的奖励是为零的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.38.png \':size=550\')\n\n我们开始做这个 policy evaluation，policy evaluation 是一个不停迭代的过程。当我们初始化的时候，所有的 $v(s)$ 都是 0。我们现在迭代一次，迭代一次过后，你发现有些状态上面，值已经产生了变化。比如有些状态的值的 R 为 -1，迭代一次过后，它就会得到 -1 的这个奖励。对于中间这个绿色的，因为它的奖励为正，所以它是 +1 的状态。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.39.png \':size=550\')\n\n所以当迭代第一次的时候，$v(s)$ 某些状态已经有些值的变化。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.40.png \':size=550\')\n\n* 我们再迭代一次(one sweep)，然后发现它就从周围的状态也开始有值。因为周围状态跟之前有值的状态是临近的，所以它就相当于把旁边这个状态转移过来。所以当我们逐渐迭代的话，你会发现这个值一直在变换。\n\n* 等迭代了很多次过后，很远的这些状态的价值函数已经有些值了，而且你可以发现它这里整个过程呈现逐渐扩散开的一个过程，这其实也是 policy evaluation 的一个可视化。\n* 当我们每一步在进行迭代的时候，远的状态就会得到了一些值，就逐渐从一些已经有奖励的这些状态，逐渐扩散，当你 run 很多次过后，它就逐渐稳定下来，最后值就会确定不变，这样收敛过后，每个状态上面的值就是它目前得到的这个 value function 的值。\n\n### MDP Control\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.41.png)\n\nPolicy evaluation 是说给定一个 MDP 和一个 policy，我们可以估算出它的价值函数。**还有问题是说如果我们只有一个 MDP，如何去寻找一个最佳的策略，然后可以得到一个`最佳价值函数(Optimal Value Function)`。**\n\nOptimal Value Function 的定义如下式所示：\n$$\nv^{*}(s)=\\max _{\\pi} v^{\\pi}(s)\n$$\nOptimal Value Function 是说，我们去搜索一种 policy $\\pi$ 来让每个状态的价值最大。$v^*$ 就是到达每一个状态，它的值的极大化情况。\n\n在这种极大化情况上面，我们得到的策略就可以说它是`最佳策略(optimal policy)`，如下式所示：\n$$\n\\pi^{*}(s)=\\underset{\\pi}{\\arg \\max }~ v^{\\pi}(s)\n$$\nOptimal policy 使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个 optimal value function，就可以说某一个 MDP 的环境被解。在这种情况下，它的最佳的价值函数是一致的，就它达到的这个上限的值是一致的，但这里可能有多个最佳的 policy，就是说多个 policy 可以取得相同的最佳价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.42.png)\n\nQ: 怎么去寻找这个最佳的 policy ？\n\nA: 当取得最佳的价值函数过后，我们可以通过对这个 Q 函数进行极大化，然后得到最佳策略。当所有东西都收敛过后，因为 Q 函数是关于状态跟动作的一个函数，所以在某一个状态采取一个动作，可以使得这个 Q 函数最大化，那么这个动作就应该是最佳的动作。所以如果我们能优化出一个 Q 函数，就可以直接在这个 Q 函数上面取一个让 Q 函数最大化的 action 的值，就可以提取出它的最佳策略。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.43.png)\n\n最简单的策略搜索办法就是`穷举`。假设状态和动作都是有限的，那么每个状态我们可以采取这个 A 种动作的策略，那么总共就是 $|A|^{|S|}$ 个可能的 policy。那我们可以把策略都穷举一遍，然后算出每种策略的 value function，对比一下就可以得到最佳策略。\n\n但是穷举非常没有效率，所以我们要采取其他方法。**搜索最佳策略有两种常用的方法：policy iteration 和  value iteration**。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.44.png)\n\n**寻找这个最佳策略的过程就是 MDP control 过程**。MDP control 说的就是怎么去寻找一个最佳的策略来让我们得到一个最大的价值函数，如下式所示：\n$$\n\\pi^{*}(s)=\\underset{\\pi}{\\arg \\max } ~ v^{\\pi}(s)\n$$\n对于一个事先定好的 MDP 过程，当 agent 去采取最佳策略的时候，我们可以说最佳策略一般都是确定的，而且是稳定的(它不会随着时间的变化)。但是不一定是唯一的，多种动作可能会取得相同的这个价值。\n\n**我们可以通过 policy iteration 和 value iteration 来解 MDP 的控制问题。**\n\n### Policy Iteration\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.45.png)\n\n**Policy iteration 由两个步骤组成：policy evaluation 和 policy improvement。**\n\n* **第一个步骤是 policy evaluation**，当前我们在优化这个 policy $\\pi$，在优化过程中得到一个最新的 policy。我们先保证这个 policy 不变，然后去估计它出来的这个价值。给定当前的 policy function 来估计这个 v 函数。\n* **第二个步骤是 policy improvement**，得到 v 函数过后，我们可以进一步推算出它的 Q 函数。得到 Q 函数过后，我们直接在 Q 函数上面取极大化，通过在这个 Q 函数上面做一个贪心的搜索来进一步改进它的策略。\n* 这两个步骤就一直是在迭代进行，所以在 policy iteration 里面，在初始化的时候，我们有一个初始化的 $V$ 和 $\\pi$ ，然后就是在这两个过程之间迭代。\n* 左边这幅图上面的线就是我们当前 v 的值，下面的线是 policy 的值。\n  * 跟踢皮球一样，我们先给定当前已有的这个 policy function，然后去算它的 v。\n  * 算出 v 过后，我们会得到一个 Q 函数。Q 函数我们采取 greedy 的策略，这样就像踢皮球，踢回这个 policy 。\n  * 然后进一步改进那个 policy ，得到一个改进的 policy 过后，它还不是最佳的，我们再进行 policy evaluation，然后又会得到一个新的 value function。基于这个新的 value function 再进行 Q 函数的极大化，这样就逐渐迭代，然后就会得到收敛。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.46.png)\n\n这里再来看一下第二个步骤： `policy improvement`，我们是如何改进它的这个策略。得到这个 v 值过后，我们就可以通过这个 reward function 以及状态转移把它的这个 Q-function 算出来，如下式所示：\n$$\nq^{\\pi_{i}}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{\\pi_{i}}\\left(s^{\\prime}\\right)\n$$\n对于每一个状态，第二个步骤会得到它的新一轮的这个 policy ，就在每一个状态，我们去取使它得到最大值的 action，如下式所示：\n$$\n\\pi_{i+1}(s)=\\underset{a}{\\arg \\max } ~q^{\\pi_{i}}(s, a)\n$$\n**你可以把 Q 函数看成一个 Q-table:**\n\n* 横轴是它的所有状态，\n* 纵轴是它的可能的 action。\n\n得到 Q 函数后，`Q-table`也就得到了。\n\n那么对于某一个状态，每一列里面我们会取最大的那个值，最大值对应的那个 action 就是它现在应该采取的 action。所以 arg max 操作就说在每个状态里面采取一个 action，这个 action 是能使这一列的 Q 最大化的那个动作。\n\n#### Bellman Optimality Equation\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.47.png)\n\n当一直在采取 arg max 操作的时候，我们会得到一个单调的递增。通过采取这种 greedy，即 arg max 操作，我们就会得到更好的或者不变的 policy，而不会使它这个价值函数变差。所以当这个改进停止过后，我们就会得到一个最佳策略。\n\n当改进停止过后，我们取它最大化的这个 action，它直接就会变成它的价值函数，如下式所示：\n$$\nq^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)=\\max _{a \\in \\mathcal{A}} q^{\\pi}(s, a)=q^{\\pi}(s, \\pi(s))=v^{\\pi}(s)\n$$\n所以我们有了一个新的等式：\n$$\nv^{\\pi}(s)=\\max _{a \\in \\mathcal{A}} q^{\\pi}(s, a)\n$$\n上式被称为  `Bellman optimality equation`。从直觉上讲，Bellman optimality equation 表达了这样一个事实：最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。 \n\n**当 MDP 满足 Bellman optimality equation 的时候，整个 MDP 已经到达最佳的状态。**它到达最佳状态过后，对于这个 Q 函数，取它最大的 action 的那个值，就是直接等于它的最佳的 value function。只有当整个状态已经收敛过后，得到一个最佳的 policy 的时候，这个条件才是满足的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.49.png)\n\n最佳的价值函数到达过后，这个 Bellman optimlity equation 就会满足。\n\n满足过后，就有这个 max 操作，如第一个等式所示：\n$$\nv^{*}(s)=\\max _{a} q^{*}(s, a)\n$$\n当我们取最大的这个 action 的时候对应的值就是当前状态的最佳的价值函数。\n\n另外，我们给出第二个等式，即 Q 函数的 Bellman equation：\n$$\nq^{*}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\n$$\n**我们可以把第一个等式插入到第二个等式里面去**，如下式所示：\n$$\n\\begin{aligned}\nq^{*}(s, a)&=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right) \\\\\n&=R(s,a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\max _{a} q^{*}(s\', a\')\n\\end{aligned}\n$$\n我们就会得到 Q 函数之间的转移。它下一步这个状态，取了 max 这个值过后，就会跟它最佳的这个状态等价。\n\nQ-learning 是基于 Bellman Optimality Equation 来进行的，当取它最大的这个状态的时候（ $\\underset{a\'}{\\max} q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)$ ），它会满足下面这个等式：\n$$\nq^{*}(s, a)=R(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) \\max _{a^{\\prime}} q^{*}\\left(s^{\\prime}, a^{\\prime}\\right)\n$$\n\n我们还可以把第二个等式插入到第一个等式，如下式所示：\n$$\n\\begin{aligned}\nv^{*}(s)&=\\max _{a} q^{*}(s, a) \\\\\n&=\\max_{a} \\mathbb{E}[G_t|s_t=s,a_t=a]\\\\  \n&=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma G_{t+1}|s_t=s,a_t=a]\\\\\n&=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v^*(s_{t+1})|s_t=s,a_t=a]\\\\\n&=\\max_{a}\\mathbb{E}[R_{t+1}]+ \\max_a \\mathbb{E}[\\gamma v^*(s_{t+1})|s_t=s,a_t=a]\\\\\n&=\\max_{a} R(s,a) + \\max_a\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\\\\\n&=\\max_{a} \\left(R(s,a) + \\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) v^{*}\\left(s^{\\prime}\\right)\\right)\n\\end{aligned}\n$$\n我们就会得到状态-价值函数的一个转移。\n\n### Value Iteration\n\n#### Principle of Optimality\n\n我们从另一个角度思考问题，动态规划的方法将优化问题分成两个部分：\n\n* 第一步执行的是最优的 action；\n* 之后后继的状态每一步都按照最优的 policy 去做，那么我最后的结果就是最优的。\n\n**Principle of Optimality Theorem**:\n\n一个 policy $\\pi(s|a)$ 在状态 $s$ 达到了最优价值，也就是 $v^{\\pi}(s) = v^{*}(s)$ 成立，当且仅当：\n\n对于**任何**能够从 $s$ 到达的 $s\'$，都已经达到了最优价值，也就是，对于所有的 $s\'$，$v^{\\pi}(s\') = v^{*}(s\')$ 恒成立。\n\n#### Deterministic Value Iteration\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.50.png)\n\n\n\n**Value iteration 就是把 Bellman Optimality Equation 当成一个 update rule 来进行，**如下式所示：\n$$\nv(s) \\leftarrow \\max _{a \\in \\mathcal{A}}\\left(R(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} P\\left(s^{\\prime} \\mid s, a\\right) v\\left(s^{\\prime}\\right)\\right)\n$$\n之前我们说上面这个等式只有当整个 MDP 已经到达最佳的状态时才满足。但这里可以把它转换成一个 backup 的等式。Backup 就是说一个迭代的等式。**我们不停地去迭代 Bellman Optimality Equation，到了最后，它能逐渐趋向于最佳的策略，这是 value iteration 算法的精髓。**\n\n为了得到最佳的 $v^*$ ，对于每个状态的 $v^*$，我们直接把这个 Bellman Optimality Equation 进行迭代，迭代了很多次之后，它就会收敛。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.51.png)\n\n* 我们使用 value iteration 算法是为了得到一个最佳的策略。\n* 解法：我们可以直接把 `Bellman Optimality backup` 这个等式拿进来进行迭代，迭代很多次，收敛过后得到的那个值就是它的最佳的值。\n* 这个算法开始的时候，它是先把所有值初始化，通过每一个状态，然后它会进行这个迭代。把等式 (22) 插到等式 (23) 里面，就是 Bellman optimality backup 的那个等式。有了等式 (22) 和等式 (23) 过后，然后进行不停地迭代，迭代过后，然后收敛，收敛后就会得到这个 $v^*$ 。当我们有了 $v^*$ 过后，一个问题是如何进一步推算出它的最佳策略。\n* 提取最佳策略的话，我们可以直接用 arg max。就先把它的 Q 函数重构出来，重构出来过后，每一个列对应的最大的那个 action 就是它现在的最佳策略。这样就可以从最佳价值函数里面提取出最佳策略。\n* 我们只是在解决一个 planning 的问题，而不是强化学习的问题，因为我们知道环境如何变化。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.52.png)\n\n* value function 做的工作类似于 value 的反向传播，每次迭代做一步传播，所以中间过程的 policy 和 value function 是没有意义的。不像是 policy iteration，它每一次迭代的结果都是有意义的，都是一个完整的 policy。\n* 上图是一个可视化的过程，在一个 gridworld 中，我们设定了一个终点(goal)，也就是左上角的点。不管你在哪一个位置开始，我们都希望能够到终点（实际上这个终点是在迭代过程中不必要的，只是为了更好的演示）。Value iteration 的迭代过程像是一个从某一个状态（这里是我们的 goal）反向传播其他各个状态的过程。因为每次迭代只能影响到与之直接相关的状态。\n* 让我们回忆下 `Principle of Optimality Theorem`：当你这次迭代求解的某个状态 s 的 value function $v_{k+1}(s)$ 是最优解，它的前提是能够从该状态到达的所有状态 s\' 此时都已经得到了最优解；如果不是的话，它做的事情只是一个类似传递 value function 的过程。\n* 以上图为例，实际上，对于每一个状态，我们都可以看成一个终点。迭代由每一个终点开始，每次都根据 Bellman optimality equation 重新计算 value。如果它的相邻节点 value 发生变化，变得更好，那么它也会变得更好，一直到相邻节点都不变了。因此，**在我们迭代到** $v_7$ **之前，也就是还没将每个终点的最优的 value 传递给其他的所有状态之前，中间的几个 value function 只是一种暂存的不完整的数据，它不能代表每一个 state 的 value function，所以生成的 policy 是一个没有意义的 policy**。\n* 因为它是一个迭代过程，这里可视化了从  $v_1$ 到 $v_7$  每一个状态的值的变化，它的这个值逐渐在变化。而且因为它每走一步，就会得到一个负的值，所以它需要尽快地到达左上角，可以发现离它越远的，那个值就越小。\n* $v_7$ 收敛过后，右下角那个值是 -6，相当于它要走六步，才能到达最上面那个值。而且离目的地越近，它的价值越大。\n\n### Difference between Policy Iteration and Value Iteration\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.53.png)\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.54.png \':size=550\')\n\n**我们来看一个 MDP control 的 Demo。**\n\n* 首先来看 policy iteration。之前的例子在每个状态都是采取固定的随机策略，就每个状态都是 0.25 的概率往上往下往左往右，没有策略的改变。\n* 但是我们现在想做 policy iteration，就是每个状态的策略都进行改变。Policy iteration 的过程是一个迭代过程。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.55.png \':size=550\')\n\n我们先在这个状态里面 run 一遍 policy  evaluation，就得到了一个 value function，每个状态都有一个 value function。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.56.png \':size=550\')\n\n* **现在进行 policy improvement，点一下 policy update。**点一下 policy update 过后，你可以发现有些格子里面的 policy 已经产生变化。\n* 比如说对于中间这个 -1 的这个状态，它的最佳策略是往下走。当你到达这个状态后，你应该往下，这样就会得到最佳的这个值。\n* 绿色右边的这个方块的策略也改变了，它现在选取的最佳策略是往左走，也就是说在这个状态的时候，最佳策略应该是往左走。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.57.png \':size=550\')\n\n我们再 run 下一轮的 policy evaluation，你发现它的值又被改变了，很多次过后，它会收敛。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.58.png \':size=550\')\n\n我们再 run policy update，你发现每个状态里面的值基本都改变，它不再是上下左右随机在变了，它会选取一个最佳的策略。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.59.png \':size=550\')\n\n我们再 run 这个 policy evaluation，它的值又在不停地变化，变化之后又收敛了。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.60.png \':size=550\')\n\n\n我们再来 run 一遍 policy update。现在它的值又会有变化，就在每一个状态，它的这个最佳策略也会产生一些改变。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.61.png \':size=550\')\n\n再来在这个状态下面进行改变，现在你看基本没有什么变化，就说明整个 MDP 已经收敛了。所以现在它每个状态的值就是它当前最佳的 value function 的值以及它当前状态对应的这个 policy 就是最佳的 policy。\n\n比如说现在我们在右上角 0.38 的这个位置，然后它说现在应该往下走，我们往下走一步。它又说往下走，然后再往下走。现在我们有两个选择：往左走和往下走。我们现在往下走，随着这个箭头的指示，我们就会到达中间 1.20 的一个状态。如果能达到这个状态，我们就会得到很多 reward 。\n\n这个 Demo 说明了 policy iteration 可以把 gridworld 解决掉。解决掉的意思是说，不管在哪个状态，都可以顺着状态对应的最佳的策略来到达可以获得最多奖励的一个状态。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.62.png \':size=550\')\n\n**我们再用 value iteration 来解 MDP，点 Toggle value iteration。** \n\n* 当它的这个值确定下来过后，它会产生它的最佳状态，这个最佳状态提取的策略跟 policy iteration 得出来的最佳策略是一致的。\n* 在每个状态，我们跟着这个最佳策略走，就会到达可以得到最多奖励的一个状态。\n\n我们给出一个[ Demo](https://github.com/cuhkrlcourse/RLexample/tree/master/MDP)，这个 Demo 是为了解一个叫 `FrozenLake` 的例子，这个例子是 OpenAI Gym 里的一个环境，跟 gridworld 很像，不过它每一个状态转移是一个概率。\n\n**我们再来对比下 policy iteration 和 value iteration，这两个算法都可以解 MDP 的控制问题。**\n\n* Policy Iteration 分两步，首先进行 policy evaluation，即对当前已经搜索到的策略函数进行一个估值。得到估值过后，进行 policy improvement，即把 Q 函数算出来，我们进一步进行改进。不断重复这两步，直到策略收敛。\n* Value iteration 直接把 Bellman Optimality Equation 拿进来，然后去寻找最佳的 value function，没有 policy function 在这里面。当算出 optimal value function 过后，我们再来提取最佳策略。\n\n### Summary for Prediction and Control in MDP\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/2.65.png)\n\n总结如上表所示，就对于 MDP 里面的 prediction 和 control  都是用动态规划来解，我们其实采取了不同的 Bellman Equation。\n\n* 如果是一个 prediction 的问题，即 policy evaluation  的问题，直接就是不停地 run 这个 Bellman Expectation Equation，这样我们就可以去估计出给定的这个策略，然后得到价值函数。\n* 对于 control，\n  * 如果采取的算法是 policy  iteration，那这里用的是 Bellman Expectation Equation 。把它分成两步，先上它的这个价值函数，再去优化它的策略，然后不停迭代。这里用到的只是 Bellman Expectation Equation。\n  * 如果采取的算法是 value iteration，那这里用到的 Bellman Equation 就是 Bellman Optimality Equation，通过 arg max 这个过程，不停地去 arg max 它，最后它就会达到最优的状态。\n\n## References\n\n* [强化学习基础 David Silver 笔记](https://zhuanlan.zhihu.com/c_135909947)\n* [Reinforcement Learning: An Introduction (second edition)](https://book.douban.com/subject/30323890/)\n* [David Silver 强化学习公开课中文讲解及实践](https://zhuanlan.zhihu.com/reinforce)\n* [UCL Course on RL(David Silver)](https://www.davidsilver.uk/teaching/)\n* [Derivation of Bellman’s Equation](https://jmichaux.github.io/_notebook/2018-10-14-bellman/)\n* [深入浅出强化学习：原理入门](https://book.douban.com/subject/27624485//)','2021-12-10 12:43:49','2021-12-19 19:22:59'),
	(37,3,'第二章 习题','## 1 Keywords\n\n- **马尔可夫性质(Markov Property):** 如果某一个过程未来的转移跟过去是无关，只由现在的状态决定，那么其满足马尔可夫性质。换句话说，一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。\n- **马尔可夫链(Markov Chain):** 概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。\n- **状态转移矩阵(State Transition Matrix):** 状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在 $s_t$ 这个状态过后，到达下面所有状态的一个概念，它每一行其实描述了是从一个节点到达所有其它节点的概率。\n- **马尔可夫奖励过程(Markov Reward Process, MRP)：** 即马尔可夫链再加上了一个奖励函数。在 MRP之中，转移矩阵跟它的这个状态都是跟马尔可夫链一样的，多了一个奖励函数(reward function)。奖励函数是一个期望，它说当你到达某一个状态的时候，可以获得多大的奖励。\n- **horizon:** 定义了同一个 episode 或者是整个一个轨迹的长度，它是由有限个步数决定的。\n- **return:** 把奖励进行折扣(discounted)，然后获得的对应的收益。\n- **Bellman Equation（贝尔曼等式）:** 定义了当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，同时也被叫作“动态规划方程”。$V(s)=R(S)+ \\gamma \\sum_{s\' \\in S}P(s\'|s)V(s\')$ ，特别地，矩阵形式：$V=R+\\gamma PV$。\n- **Monte Carlo Algorithm（蒙特卡罗方法）：** 可用来计算价值函数的值。通俗的讲，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它让把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的 Discounted 的奖励 $g$ 直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，然后就会得到它的这个价值。\n- **Iterative Algorithm（动态规划方法）：** 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，这个更新就可以停止。\n- **Q函数 (action-value function)：** 其定义的是某一个状态某一个行为，对应的它有可能得到的 return 的一个期望（over policy function）。\n- **MDP中的prediction（即policy evaluation问题）：** 给定一个 MDP 以及一个 policy $\\pi$ ，去计算它的 value function，即每个状态它的价值函数是多少。其可以通过动态规划方法（Iterative Algorithm）解决。\n- **MDP中的control问题：** 寻找一个最佳的一个策略，它的 input 就是MDP，输出是通过去寻找它的最佳策略，然后同时输出它的最佳价值函数(optimal value function)以及它的这个最佳策略(optimal policy)。其可以通过动态规划方法（Iterative Algorithm）解决。\n- **最佳价值函数(Optimal Value Function)：** 我们去搜索一种 policy $\\pi$ ，然后我们会得到每个状态它的状态值最大的一个情况，$v^*$ 就是到达每一个状态，它的值的极大化情况。在这种极大化情况上面，我们得到的策略就可以说它是最佳策略(optimal policy)。optimal policy 使得每个状态，它的状态函数都取得最大值。所以当我们说某一个 MDP 的环境被解了过后，就是说我们可以得到一个 optimal value function，然后我们就说它被解了。\n\n## 2 Questions\n\n- 为什么在马尔可夫奖励过程（MRP）中需要有**discount factor**?\n\n  答：\n\n  1. 首先，是有些马尔可夫过程是**带环**的，它并没有终结，然后我们想**避免这个无穷的奖励**；\n  2. 另外，我们是想把这个**不确定性**也表示出来，希望**尽可能快**地得到奖励，而不是在未来某一个点得到奖励；\n  3. 接上面一点，如果这个奖励是它是有实际价值的了，我们可能是更希望立刻就得到奖励，而不是我们后面再得到奖励。\n  4. 还有在有些时候，这个系数也可以把它设为 0。比如说，当我们设为 0 过后，然后我们就只关注了它当前的奖励。我们也可以把它设为 1，设为 1 的话就是对未来并没有折扣，未来获得的奖励跟我们当前获得的奖励是一样的。\n\n  所以，这个系数其实是应该可以作为强化学习 agent 的一个 hyperparameter 来进行调整，然后就会得到不同行为的 agent。\n\n- 为什么矩阵形式的Bellman Equation的解析解比较难解？\n\n  答：通过矩阵求逆的过程，就可以把这个 V 的这个价值的解析解直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是 $O(N^3)$。所以就当我们状态非常多的时候，比如说从我们现在十个状态到一千个状态，到一百万个状态。那么当我们有一百万个状态的时候，这个转移矩阵就会是个一百万乘以一百万的一个矩阵。这样一个大矩阵的话求逆是非常困难的，所以这种通过解析解去解，只能对于很小量的MRP。\n\n- 计算贝尔曼等式（Bellman Equation）的常见方法以及区别？\n\n  答：\n\n  1. **Monte Carlo Algorithm（蒙特卡罗方法）：** 可用来计算价值函数的值。通俗的讲，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它让把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的 Discounted 的奖励 $g$ 直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，然后就会得到它的这个价值。\n  2. **Iterative Algorithm（动态规划方法）：** 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，通常是小于一个阈值 $\\gamma$ ，这个更新就可以停止。\n  3. **以上两者的结合方法：** 另外我们也可以通过 Temporal-Difference Learning 的那个办法。这个 `Temporal-Difference Learning` 叫 `TD Leanring`，就是动态规划和蒙特卡罗的一个结合。\n\n- 马尔可夫奖励过程（MRP）与马尔可夫决策过程 （MDP）的区别？\n\n  答：相对于 MRP，马尔可夫决策过程(Markov Decision Process)多了一个 decision，其它的定义跟 MRP 都是类似的。这里我们多了一个决策，多了一个 action ，那么这个状态转移也多了一个 condition，就是采取某一种行为，然后你未来的状态会不同。它不仅是依赖于你当前的状态，也依赖于在当前状态你这个 agent 它采取的这个行为会决定它未来的这个状态走向。对于这个价值函数，它也是多了一个条件，多了一个你当前的这个行为，就是说你当前的状态以及你采取的行为会决定你在当前可能得到的奖励多少。\n\n  另外，两者之间是有转换关系的。具体来说，已知一个 MDP 以及一个 policy $\\pi$ 的时候，我们可以把 MDP 转换成MRP。在 MDP 里面，转移函数 $P(s\'|s,a)$  是基于它当前状态以及它当前的 action，因为我们现在已知它 policy function，就是说在每一个状态，我们知道它可能采取的行为的概率，那么就可以直接把这个 action 进行加和，那我们就可以得到对于 MRP 的一个转移，这里就没有 action。同样地，对于奖励，我们也可以把 action 拿掉，这样就会得到一个类似于 MRP 的奖励。\n\n- MDP 里面的状态转移跟 MRP 以及 MP 的结构或者计算方面的差异？\n\n  答：\n\n  - 对于之前的马尔可夫链的过程，它的转移是直接就决定，就从你当前是 s，那么就直接通过这个转移概率就直接决定了你下一个状态会是什么。\n  - 但是对于 MDP，它的中间多了一层这个行为 a ，就是说在你当前这个状态的时候，你首先要决定的是采取某一种行为。然后因为你有一定的不确定性，当你当前状态决定你当前采取的行为过后，你到未来的状态其实也是一个概率分布。所以你采取行为以及你决定，然后你可能有有多大的概率到达某一个未来状态，以及另外有多大概率到达另外一个状态。所以在这个当前状态跟未来状态转移过程中这里多了一层决策性，这是MDP跟之前的马尔可夫过程很不同的一个地方。在马尔科夫决策过程中，行为是由 agent 决定，所以多了一个 component，agent 会采取行为来决定未来的状态转移。\n\n- 我们如何寻找最佳的policy，方法有哪些？\n\n  答：本质来说，当我们取得最佳的价值函数过后，我们可以通过对这个 Q 函数进行极大化，然后得到最佳的价值。然后，我们直接在这个Q函数上面取一个让这个action最大化的值，然后我们就可以直接提取出它的最佳的policy。\n\n  具体方法：\n\n  1. **穷举法（一般不使用）：**假设我们有有限多个状态、有限多个行为可能性，那么每个状态我们可以采取这个 A 种行为的策略，那么总共就是 $|A|^{|S|}$ 个可能的 policy。我们可以把这个穷举一遍，然后算出每种策略的 value function，然后对比一下可以得到最佳策略。但是效率极低。\n  2. **Policy iteration：** 一种迭代方法，有两部分组成，下面两个步骤一直在迭代进行，最终收敛：(有些类似于ML中EM算法（期望-最大化算法）)\n     - 第一个步骤是 **policy evaluation** ，即当前我们在优化这个 policy $\\pi$ ，所以在优化过程中得到一个最新的这个 policy 。\n     - 第二个步骤是 **policy improvement** ，即取得价值函数后，进一步推算出它的 Q 函数。得到 Q 函数过后，那我们就直接去取它的极大化。\n  3. **Value iteration:** 我们一直去迭代 Bellman Optimality Equation，到了最后，它能逐渐趋向于最佳的策略，这是 value iteration 算法的精髓，就是我们去为了得到最佳的 $v^*$ ，对于每个状态它的 $v^*$ 这个值，我们直接把这个 Bellman Optimality Equation 进行迭代，迭代了很多次之后它就会收敛到最佳的policy以及其对应的状态，这里面是没有policy function的。\n\n\n## 3 Something About Interview\n\n- 高冷的面试官: 请问马尔可夫过程是什么?马尔可夫决策过程又是什么?其中马尔可夫最重要的性质是什么呢?\n\n  答: 马尔可夫过程是是一个二元组 $ <S,P> $ ,S为状态的集合,P为状态转移概率矩阵;\n  而马尔可夫决策过程是一个五元组 $ <S,P,A,R,\\gamma> $,其中 $R$ 表示为从 $S$ 到 $S\'$ 能够获得的奖励期望, $\\gamma$为折扣因子, $A$ 为动作集合.\n  马尔可夫最重要的性质是下一个状态只与当前状态有关,与之前的状态无关,也就是 $P[S_{t+1} | S_t] = P[S_{t+1}|S_1,S_2,...,S_t]$\n\n- 高冷的面试官: 请问我们一般怎么求解马尔可夫决策过程?\n\n  答: 我们直接求解马尔可夫决策过程可以直接求解贝尔曼等式(动态规划方程),即$V(s)=R(S)+ \\gamma \\sum_{s\' \\in S}P(s\'|s)V(s\')$ ，特别地，矩阵形式：$V=R+\\gamma PV$.但是贝尔曼等式很难求解且计算复杂度较高,所以可以使用动态规划,蒙特卡洛,时间差分等方法求解.\n\n- 高冷的面试官: 请问如果数据流不满足马尔科夫性怎么办？应该如何处理?\n\n  答: 如果不满足马尔科夫性,即下一个状态与之前的状态也有关，若还仅仅用当前的状态来进行求解决策过程，势必导致决策的泛化能力变差。 为了解决这个问题，可以利用RNN对历史信息建模，获得包含历史信息的状态表征。表征过程可以 使用注意力机制等手段。最后在表征状态空间求解马尔可夫决策过程问题。\n\n- 高冷的面试官: 请分别写出基于状态值函数的贝尔曼方程以及基于动作值的贝尔曼方程.\n\n  答: \n\n  - 基于状态值函数的贝尔曼方程: $v_{\\pi}(s) = \\sum_{a}{\\pi(a|s)}\\sum_{s\',r}{p(s\',r|s,a)[r(s,a)+\\gamma v_{\\pi}(s\')]}$\n  - 基于动作值的贝尔曼方程: $q_{\\pi}(s,a)=\\sum_{s\',r}p(s\',r|s,a)[r(s\',a)+\\gamma v_{\\pi}(s\')]$\n\n- 高冷的面试官: 请问最佳价值函数(optimal value function) $v^*$ 和最佳策略(optimal policy) $\\pi^*$ 为什么等价呢？\n\n  答: 最佳价值函数的定义为： $v^* (s)=\\max_{\\pi} v^{\\pi}(s)$ 即我们去搜索一种 policy $\\pi$ 来让每个状态的价值最大。$v^*$ 就是到达每一个状态，它的值的极大化情况。在这种极大化情况上面，我们得到的策略就可以说它是最佳策略(optimal policy)，即 $ \\pi^{*}(s)=\\underset{\\pi}{\\arg \\max }~ v^{\\pi}(s) $. Optimal policy 使得每个状态的价值函数都取得最大值。所以如果我们可以得到一个 optimal value function，就可以说某一个 MDP 的环境被解。在这种情况下，它的最佳的价值函数是一致的，就它达到的这个上限的值是一致的，但这里可能有多个最佳的 policy，就是说多个 policy 可以取得相同的最佳价值。\n\n- 高冷的面试官：能不能手写一下第n步的值函数更新公式呀？另外，当n越来越大时，值函数的期望和方差分别变大还是变小呢？\n\n  答：$n$越大，方差越大，期望偏差越小。值函数的更新公式? 话不多说，公式如下：\n  $$\n  Q\\left(S, A\\right) \\leftarrow Q\\left(S, A\\right)+\\alpha\\left[\\sum_{i=1}^{n} \\gamma^{i-1} R_{t+i}+\\gamma^{n} \\max _{a}   Q\\left(S\',a\\right)-Q\\left(S, A\\right)\\right]\n  $$','2021-12-10 12:43:49','2021-12-19 19:23:23'),
	(38,3,'第三章 表格型方法','## Tabular Methods\n\n本章我们通过最简单的`表格型的方法(tabular methods)`来讲解如何使用 value-based 方法去求解强化学习。\n\n## MDP\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.1.png)\n\n**强化学习的三个重要的要素：状态、动作和奖励。**强化学习智能体跟环境是一步一步交互的，就是我先观察一下状态，然后再输入动作。再观察一下状态，再输出动作，拿到这些 reward 。它是一个跟时间相关的序列决策的问题。\n\n举个例子，在 $t-1$ 时刻，我看到了熊对我招手，那我下意识的可能输出的动作就是赶紧跑路。熊看到了有人跑了，可能就觉得发现猎物，开始发动攻击。而在 $t$ 时刻的话，我如果选择装死的动作，可能熊咬了咬我，摔了几下就发现就觉得挺无趣的，可能会走开。这个时候，我再跑路的话可能就跑路成功了，就是这样子的一个序列决策的过程。\n\n当然在输出每一个动作之前，你可以选择不同的动作。比如说在 $t$ 时刻，我选择跑路的时候，熊已经追上来了，如果说 $t$ 时刻，我没有选择装死，而我是选择跑路的话，这个时候熊已经追上了，那这个时候，其实我有两种情况转移到不同的状态去，就我有一定的概率可以逃跑成功，也有很大的概率我会逃跑失败。那我们就用状态转移概率 $p\\left[s_{t+1}, r_{t} \\mid s_{t}, a_{t}\\right]$ 来表述说在 $s_t$ 的状态选择了 $a_t$ 的动作的时候，转移到 $s_{t+1}$ ，而且拿到  $r_t$ 的概率是多少。\n\n这样子的一个状态转移概率是具有`马尔可夫性质`的(系统下一时刻的状态仅由当前时刻的状态决定，不依赖于以往任何状态)。因为这个状态转移概率，它是下一时刻的状态是取决于当前的状态，它和之前的 $s_{t-1}$ 和 $s_{t-2}$  都没有什么关系。然后再加上这个过程也取决于智能体跟环境交互的这个 $a_t$ ，所以有一个决策的一个过程在里面。我们就称这样的一个过程为`马尔可夫决策过程(Markov Decision Process, MDP)`。\n\nMDP 就是序列决策这样一个经典的表达方式。MDP 也是强化学习里面一个非常基本的学习框架。状态、动作、状态转移概率和奖励 $(S,A,P,R)$，这四个合集就构成了强化学习 MDP 的四元组，后面也可能会再加个衰减因子构成五元组。\n\n### Model-based\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.2.png)\n\n\n\n\n如上图所示，我们把这些可能的动作和可能的状态转移的关系画成一个树状图。它们之间的关系就是从 $s_t$ 到 $a_t$ ，再到 $s_{t+1}$ ，再到 $a_{t+1}$，再到 $s_{t+2}$ 这样子的一个过程。\n\n我们去跟环境交互，只能走完整的一条通路。这里面产生了一系列的一个决策的过程，就是我们跟环境交互产生了一个经验。**我们会使用 `概率函数(probability function)`和 `奖励函数(reward function)`来去描述环境。**概率函数就是状态转移的概率，概率函数实际上反映的是环境的一个随机性。\n\n当我们知道概率函数和奖励函数时，我们就说这个 MDP 是已知的，可以通过 policy iteration 和 value iteration 来找最佳的策略。\n\n比如，在熊发怒的情况下，我如果选择装死，假设熊看到人装死就一定会走的话，我们就称在这里面的状态转移概率就是 100%。但如果说在熊发怒的情况下，我选择跑路而导致可能跑成功以及跑失败，出现这两种情况。那我们就可以用概率去表达一下说转移到其中一种情况的概率大概 10%，另外一种情况的概率大概是 90% 会跑失败。\n\n**如果知道这些状态转移概率和奖励函数的话，我们就说这个环境是已知的，因为我们是用这两个函数去描述环境的。**如果是已知的话，我们其实可以用动态规划去计算说，如果要逃脱熊，那么能够逃脱熊概率最大的最优策略是什么。很多强化学习的经典算法都是 model-free 的，就是环境是未知的。\n\n### Model-free\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.3.png)\n因为现实世界中人类第一次遇到熊之前，我们根本不知道能不能跑得过熊，所以刚刚那个 10%、90% 的概率也就是虚构出来的概率。熊到底在什么时候会往什么方向去转变的话，我们经常是不知道的。\n\n**我们是处在一个未知的环境里的，也就是这一系列的决策的概率函数和奖励函数是未知的，这就是 model-based 跟 model-free 的一个最大的区别。**\n\n强化学习就是可以用来解决用完全未知的和随机的环境。强化学习要像人类一样去学习，人类学习的话就是一条路一条路地去尝试一下，先走一条路，看看结果到底是什么。多试几次，只要能活命的。我们可以慢慢地了解哪个状态会更好，\n\n* 我们用价值函数 $V(s)$ 来代表这个状态是好的还是坏的。\n* 用 Q 函数来判断说在什么状态下做什么动作能够拿到最大奖励，用 Q 函数来表示这个状态-动作值。\n\n### Model-based vs. Model-free\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_1.png)\n\n* Policy iteration 和 value iteration 都需要得到环境的转移和奖励函数，所以在这个过程中，agent 没有跟环境进行交互。\n* 在很多实际的问题中，MDP 的模型有可能是未知的，也有可能模型太大了，不能进行迭代的计算。比如 Atari 游戏、围棋、控制直升飞机、股票交易等问题，这些问题的状态转移太复杂了。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_2.png)\n\n* 在这种情况下，我们使用 model-free 强化学习的方法来解。 \n* Model-free 没有获取环境的状态转移和奖励函数，我们让 agent 跟环境进行交互，采集到很多的轨迹数据，agent 从轨迹中获取信息来改进策略，从而获得更多的奖励。\n\n## Q-table\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.4.png)\n\n接下来介绍下 Q 函数。在多次尝试和熊打交道之后，人类就可以对熊的不同的状态去做出判断，我们可以用状态动作价值来表达说在某个状态下，为什么动作 1 会比动作 2 好，因为动作 1 的价值比动作 2 要高，这个价值就叫 `Q 函数`。\n\n**如果 `Q 表格`是一张已经训练好的表格的话，那这一张表格就像是一本生活手册。**我们就知道在熊发怒的时候，装死的价值会高一点。在熊离开的时候，我们可能偷偷逃跑的会比较容易获救。\n\n这张表格里面 Q 函数的意义就是我选择了这个动作之后，最后面能不能成功，就是我需要去计算在这个状态下，我选择了这个动作，后续能够一共拿到多少总收益。如果可以预估未来的总收益的大小，我们当然知道在当前的这个状态下选择哪个动作，价值更高。我选择某个动作是因为我未来可以拿到的那个价值会更高一点。所以强化学习的目标导向性很强，环境给出的奖励是一个非常重要的反馈，它就是根据环境的奖励来去做选择。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.5.png)Q: 为什么可以用未来的总收益来评价当前这个动作是好是坏?\n\nA: 举个例子，假设一辆车在路上，当前是红灯，我们直接走的收益就很低，因为违反交通规则，这就是当前的单步收益。可是如果我们这是一辆救护车，我们正在运送病人，把病人快速送达医院的收益非常的高，而且越快你的收益越大。在这种情况下，我们很可能应该要闯红灯，因为未来的远期收益太高了。这也是为什么强化学习需要去学习远期的收益，因为在现实世界中奖励往往是延迟的。所以我们一般会从当前状态开始，把后续有可能会收到所有收益加起来计算当前动作的 Q 的价值，让 Q 的价值可以真正地代表当前这个状态下，动作的真正的价值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.6.png)\n\n但有的时候把目光放得太长远不好，因为如果事情很快就结束的话，你考虑到最后一步的收益无可厚非。如果是一个持续的没有尽头的任务，即`持续式任务(Continuing Task)`，你把未来的收益全部相加，作为当前的状态价值就很不合理。\n\n股票的例子就很典型了，我们要关注的是累积的收益。可是如果说十年之后才有一次大涨大跌，你显然不会把十年后的收益也作为当前动作的考虑因素。那我们会怎么办呢，有句俗话说得好，对远一点的东西，我们就当做近视，就不需要看得太清楚，我们可以引入这个衰减因子 $\\gamma$ 来去计算这个未来总收益，$\\gamma \\in [0,1]$，越往后 $\\gamma^n$ 就会越小，也就是说越后面的收益对当前价值的影响就会越小。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.7.png)\n\n\n举个例子来看看计算出来的是什么效果。这是一个悬崖问题，这个问题是需要智能体从出发点 S 出发，到达目的地 G，同时避免掉进悬崖(cliff)，掉进悬崖的话就会有 -100 分的惩罚，但游戏不会结束，它会被直接拖回起点，游戏继续。为了到达目的地，我们可以沿着蓝线和红线走。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.8.png)\n\n在这个环境当中，我们怎么去计算状态动作价值(未来的总收益)。\n\n* 如果 $\\gamma = 0$， 假设我走一条路，并从这个状态出发，在这里选择是向上，这里选择向右。如果 $\\gamma = 0$，用这个公式去计算的话，它相当于考虑的就是一个单步的收益。我们可以认为它是一个目光短浅的计算的方法。\n\n* 如果 $\\gamma = 1$，那就等于是说把后续所有的收益都全部加起来。在这里悬崖问题，你每走一步都会拿到一个 -1 分的 reward，只有到了终点之后，它才会停止。如果 $\\gamma =1 $ 的话，我们用这个公式去计算，就这里是 -1。然后这里的话，未来的总收益就是 $-1+-1=-2$ 。 \n\n* 如果 $\\gamma = 0.6$，就是目光没有放得那么的长远，计算出来是这个样子的。利用 $G_{t}=R_{t+1}+\\gamma G_{t+1}$ 这个公式从后往前推。\n\n$$\n\\begin{array}{l}\nG_{7}=R+\\gamma G_{8}=-1+0.6 *(-2.176)=-2.3056 \\approx-2.3 \\\\\nG_{8}=R+\\gamma G_{9}=-1+0.6 *(-1.96)=-2.176 \\approx-2.18 \\\\\nG_{9}=R+\\gamma G_{10}=-1+0.6 *(-1.6)=-1.96 \\\\\nG_{10}=R+\\gamma G_{11}=-1+0.6 *(-1)=-1.6 \\\\\nG_{12}=R+\\gamma G_{13}=-1+0.6 * 0=-1 \\\\\nG_{13}=0\n\\end{array}\n$$\n\n\n这里的计算是我们选择了一条路，计算出这条路径上每一个状态动作的价值。我们可以看一下右下角这个图，如果说我走的不是红色的路，而是蓝色的路，那我算出来的 Q 值可能是这样。那我们就知道，当小乌龟在 -12 这个点的时候，往右边走是 -11，往上走是 -15，它自然就知道往右走的价值更大，小乌龟就会往右走。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.9.png)\n类似于上图，最后我们要求解的就是一张 Q 表格，\n\n* 它的行数是所有的状态数量，一般可以用坐标来表示表示格子的状态，也可以用 1、2、3、4、5、6、7 来表示不同的位置。\n* Q 表格的列表示上下左右四个动作。\n\n最开始这张 Q 表格会全部初始化为零，然后 agent 会不断地去和环境交互得到不同的轨迹，当交互的次数足够多的时候，我们就可以估算出每一个状态下，每个行动的平均总收益去更新这个 Q  表格。怎么去更新 Q 表格就是接下来要引入的强化概念。\n\n**`强化`就是我们可以用下一个状态的价值来更新当前状态的价值，其实就是强化学习里面 bootstrapping 的概念。**在强化学习里面，你可以每走一步更新一下 Q 表格，然后用下一个状态的 Q 值来更新这个状态的 Q 值，这种单步更新的方法叫做`时序差分`。\n\n## Model-free Prediction\n\n在没法获取 MDP 的模型情况下，我们可以通过以下两种方法来估计某个给定策略的价值：\n\n* Monte Carlo policy evaluation\n* Temporal Difference(TD) learning\n\n### Monte-Carlo Policy Evaluation\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_1.png)\n\n* `蒙特卡罗(Monte-Carlo，MC)`方法是基于采样的方法，我们让 agent 跟环境进行交互，就会得到很多轨迹。每个轨迹都有对应的 return：\n\n$$\nG_{t}=R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots\n$$\n\n* 我们把每个轨迹的 return 进行平均，就可以知道某一个策略下面对应状态的价值。\n\n* MC 是用 `经验平均回报(empirical mean return)` 的方法来估计。\n\n* MC 方法不需要 MDP 的转移函数和奖励函数，并且不需要像动态规划那样用 bootstrapping  的方法。\n\n* MC 的局限性：只能用在有终止的 MDP 。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_2.png)\n\n* 上图是 MC 算法的概括。\n* 为了得到评估 $v(s)$，我们进行了如下的步骤：\n  * 在每个回合中，如果在时间步 t 状态 s 被访问了，那么\n    * 状态 s 的访问数 $N(s)$ 增加 1，\n    * 状态 s 的总的回报 $S(s)$  增加 $G_t$。\n  * 状态 s 的价值可以通过 return 的平均来估计，即 $v(s)=S(s)/N(s)$。\n\n* 根据大数定律，只要我们得到足够多的轨迹，就可以趋近这个策略对应的价值函数。\n\n假设现在有样本 $x_1,x_2,\\cdots$，我们可以把经验均值(empirical mean)转换成 `增量均值(incremental mean)` 的形式，如下式所示：\n$$\n\\begin{aligned}\n\\mu_{t} &=\\frac{1}{t} \\sum_{j=1}^{t} x_{j} \\\\\n&=\\frac{1}{t}\\left(x_{t}+\\sum_{j=1}^{t-1} x_{j}\\right) \\\\\n&=\\frac{1}{t}\\left(x_{t}+(t-1) \\mu_{t-1}\\right) \\\\\n&=\\frac{1}{t}\\left(x_{t}+t \\mu_{t-1}-\\mu_{t-1}\\right) \\\\\n&=\\mu_{t-1}+\\frac{1}{t}\\left(x_{t}-\\mu_{t-1}\\right) \n\\end{aligned}\n$$\n通过这种转换，我们就可以把上一时刻的平均值跟现在时刻的平均值建立联系，即：\n$$\n\\mu_t = \\mu_{t-1}+\\frac{1}{t}(x_t-\\mu_{t-1})\n$$\n其中：\n\n* $x_t- \\mu_{t-1}$ 是残差\n* $\\frac{1}{t}$ 类似于学习率(learning rate)\n\n当我们得到 $x_t$，就可以用上一时刻的值来更新现在的值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_3.png)\n\n我们可以把 Monte-Carlo 更新的方法写成 incremental MC 的方法：\n\n* 我们采集数据，得到一个新的轨迹。\n* 对于这个轨迹，我们采用增量的方法进行更新，如下式所示：\n\n$$\n\\begin{array}{l}\nN\\left(S_{t}\\right) \\leftarrow N\\left(S_{t}\\right)+1 \\\\\nv\\left(S_{t}\\right) \\leftarrow v\\left(S_{t}\\right)+\\frac{1}{N\\left(S_{t}\\right)}\\left(G_{t}-v\\left(S_{t}\\right)\\right)\n\\end{array}\n$$\n\n* 我们可以直接把 $\\frac{1}{N(S_t)}$ 变成 $\\alpha$ (学习率)，$\\alpha$ 代表着更新的速率有多快，我们可以进行设置。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_4.png)\n\n**我们再来看一下 DP 和 MC 方法的差异。**\n\n* 动态规划也是常用的估计价值函数的方法。在动态规划里面，我们使用了 bootstrapping 的思想。bootstrapping 的意思就是我们基于之前估计的量来估计一个量。\n\n* DP 就是用 Bellman expectation backup，就是通过上一时刻的值 $v_{i-1}(s\')$ 来更新当前时刻 $v_i(s)$ 这个值，不停迭代，最后可以收敛。Bellman expectation backup 就有两层加和，内部加和和外部加和，算了两次 expectation，得到了一个更新。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_5.png)\n\nMC 是通过 empirical mean return （实际得到的收益）来更新它，对应树上面蓝色的轨迹，我们得到是一个实际的轨迹，实际的轨迹上的状态已经是决定的，采取的行为都是决定的。MC 得到的是一条轨迹，这条轨迹表现出来就是这个蓝色的从起始到最后终止状态的轨迹。现在只是更新这个轨迹上的所有状态，跟这个轨迹没有关系的状态都没有更新。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/MC_6.png)\n\n* MC 可以在不知道环境的情况下 work，而 DP 是 model-based。\n* MC 只需要更新一条轨迹的状态，而 DP 则是需要更新所有的状态。状态数量很多的时候（比如一百万个，两百万个），DP 这样去迭代的话，速度是非常慢的。这也是 sample-based 的方法 MC 相对于 DP 的优势。\n\n### Temporal Difference\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.10.png)\n\n为了让大家更好地理解`时序差分(Temporal Difference,TD)`这种更新方法，这边给出它的物理意义。我们先理解一下巴普洛夫的条件反射实验，这个实验讲的是小狗会对盆里面的食物无条件产生刺激，分泌唾液。一开始小狗对于铃声这种中性刺激是没有反应的，可是我们把这个铃声和食物结合起来，每次先给它响一下铃，再给它喂食物，多次重复之后，当铃声响起的时候，小狗也会开始流口水。盆里的肉可以认为是强化学习里面那个延迟的 reward，声音的刺激可以认为是有 reward 的那个状态之前的一个状态。多次重复实验之后，最后的这个 reward 会强化小狗对于这个声音的条件反射，它会让小狗知道这个声音代表着有食物，这个声音对于小狗来说也就有了价值，它听到这个声音也会流口水。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.11.png)\n\n巴普洛夫效应揭示的是中性刺激(铃声)跟无条件刺激(食物)紧紧挨着反复出现的时候，中性刺激也可以引起无条件刺激引起的唾液分泌，然后形成条件刺激。\n\n**这种中性刺激跟无条件刺激在时间上面的结合，我们就称之为强化。** 强化的次数越多，条件反射就会越巩固。小狗本来不觉得铃声有价值的，经过强化之后，小狗就会慢慢地意识到铃声也是有价值的，它可能带来食物。更重要是一种条件反射巩固之后，我们再用另外一种新的刺激和条件反射去结合，还可以形成第二级条件反射，同样地还可以形成第三级条件反射。\n\n在人的身上是可以建立多级的条件反射的，举个例子，比如说一般我们遇到熊都是这样一个顺序：看到树上有熊爪，然后看到熊之后，突然熊发怒，扑过来了。经历这个过程之后，我们可能最开始看到熊才会瑟瑟发抖，后面就是看到树上有熊爪就已经有害怕的感觉了。也就说在不断的重复试验之后，下一个状态的价值，它是可以不断地去强化影响上一个状态的价值的。\n\n为了让大家更加直观感受下一个状态影响上一个状态(状态价值迭代)，我们推荐这个网站：[Temporal Difference Learning Gridworld Demo](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.13.png \':size=500\')\n\n* 我们先初始化一下，然后开始时序差分的更新过程。\n* 在训练的过程中，这个小黄球在不断地试错，在探索当中会先迅速地发现有奖励的地方。最开始的时候，只是这些有奖励的格子才有价值。当不断地重复走这些路线的时候，这些有价值的格子可以去慢慢地影响它附近的格子的价值。\n* 反复训练之后，这些有奖励的格子周围的格子的状态就会慢慢地被强化。强化就是当它收敛到最后一个最优的状态了，这些价值最终收敛到一个最优的情况之后，那个小黄球就会自动地知道，就是我一直往价值高的地方走，就能够走到能够拿到奖励的地方。\n\n**下面开始正式介绍 TD 方法。**\n\n* TD 是介于 MC 和 DP 之间的方法。\n* TD 是 model-free 的，不需要 MDP 的转移矩阵和奖励函数。\n* TD 可以从**不完整的** episode 中学习，结合了 bootstrapping 的思想。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/TD_2.png)\n\n* 上图是 TD 算法的框架。\n\n* 目的：对于某个给定的策略，在线(online)地算出它的价值函数，即一步一步地(step-by-step)算。\n\n* 最简单的算法是 `TD(0)`，每往前走一步，就做一步 bootstrapping，用得到的估计回报(estimated return)来更新上一时刻的值。 \n\n* 估计回报 $R_{t+1}+\\gamma v(S_{t+1})$ 被称为 `TD target`，TD target 是带衰减的未来收益的总和。TD target 由两部分组成：\n  * 走了某一步后得到的实际奖励：$R_{t+1}$， \n  * 我们利用了 bootstrapping 的方法，通过之前的估计来估计 $v(S_{t+1})$  ，然后加了一个折扣系数，即 $\\gamma v(S_{t+1})$，具体过程如下式所示：\n  \n  $$\n  \\begin{aligned}\n  v(s)&=\\mathbb{E}\\left[G_{t} \\mid s_{t}=s\\right] \\\\ \n  &=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots \\mid s_{t}=s\\right]  \\\\\n  &=\\mathbb{E}\\left[R_{t+1}|s_t=s\\right] +\\gamma \\mathbb{E}\\left[R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\ldots \\mid s_{t}=s\\right]\\\\\n  &=R(s)+\\gamma \\mathbb{E}[G_{t+1}|s_t=s] \\\\\n  &=R(s)+\\gamma \\mathbb{E}[v(s_{t+1})|s_t=s]\\\\\n  \\end{aligned}\n  $$\n  \n* TD目标是估计有两个原因：它对期望值进行采样，并且使用当前估计 V 而不是真实 $v_{\\pi}$。\n\n* `TD error(误差)` $\\delta=R_{t+1}+\\gamma v(S_{t+1})-v(S_t)$。\n\n* 可以类比于 Incremental Monte-Carlo 的方法，写出如下的更新方法：\n\n$$\nv\\left(S_{t}\\right) \\leftarrow v\\left(S_{t}\\right)+\\alpha\\left(R_{t+1}+\\gamma v\\left(S_{t+1}\\right)-v\\left(S_{t}\\right)\\right)\n$$\n\n> 上式体现了强化这个概念。\n\n* 我们对比下 MC 和 TD：\n  * 在 MC 里面 $G_{i,t}$ 是实际得到的值（可以看成 target），因为它已经把一条轨迹跑完了，可以算每个状态实际的 return。\n  * TD 没有等轨迹结束，往前走了一步，就可以更新价值函数。 \n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/TD_3.png)\n\n* TD 只执行了一步，状态的值就更新。\n* MC 全部走完了之后，到了终止状态之后，再更新它的值。\n\n接下来，进一步比较下 TD 和 MC。\n\n* TD 可以在线学习(online learning)，每走一步就可以更新，效率高。\n* MC 必须等游戏结束才可以学习。\n\n* TD 可以从不完整序列上进行学习。\n* MC 只能从完整的序列上进行学习。\n\n* TD 可以在连续的环境下（没有终止）进行学习。\n* MC 只能在有终止的情况下学习。\n\n* TD 利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。\n* MC 没有假设环境具有马尔可夫性质，利用采样的价值来估计某一个状态的价值，在不是马尔可夫的环境下更加有效。\n\n**举个例子来解释 TD 和 MC 的区别，**\n\n* TD 是指在不清楚马尔可夫状态转移概率的情况下，以采样的方式得到不完整的状态序列，估计某状态在该状态序列完整后可能得到的收益，并通过不断地采样持续更新价值。\n* MC 则需要经历完整的状态序列后，再来更新状态的真实价值。\n\n例如，你想获得开车去公司的时间，每天上班开车的经历就是一次采样。假设今天在路口 A 遇到了堵车，\n\n* TD 会在路口 A 就开始更新预计到达路口 B、路口 C $\\cdots \\cdots$，以及到达公司的时间；\n* 而 MC 并不会立即更新时间，而是在到达公司后，再修改到达每个路口和公司的时间。\n\n**TD 能够在知道结果之前就开始学习，相比 MC，其更快速、灵活。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/TD_5.png)\n\n* 我们可以把 TD 进行进一步的推广。之前是只往前走一步，即 one-step TD，TD(0)。\n\n* 我们可以调整步数，变成 `n-step TD`。比如 `TD(2)`，即往前走两步，然后利用两步得到的 return，使用 bootstrapping 来更新状态的价值。\n\n* 这样就可以通过 step 来调整这个算法需要多少的实际奖励和 bootstrapping。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/TD_6.png)\n\n* 通过调整步数，可以进行一个 MC 和 TD 之间的 trade-off，如果 $n=\\infty$， 即整个游戏结束过后，再进行更新，TD 就变成了 MC。\n* n-step 的 TD target 如下式所示：\n\n$$\nG_{t}^{n}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1} R_{t+n}+\\gamma^{n} v\\left(S_{t+n}\\right)\n$$\n\n* 得到 TD target 之后，我们用增量学习(incremental learning)的方法来更新状态的价值：\n\n$$\nv\\left(S_{t}\\right) \\leftarrow v\\left(S_{t}\\right)+\\alpha\\left(G_{t}^{n}-v\\left(S_{t}\\right)\\right)\n$$\n\n### Bootstrapping and Sampling for DP,MC and TD\n\n* Bootstrapping：更新时使用了估计：\n  * MC 没用 bootstrapping，因为它是根据实际的 return 来更新。\n  * DP 用了 bootstrapping。\n  * TD 用了 bootstrapping。\n\n* Sampling：更新时通过采样得到一个期望：\n  * MC 是纯 sampling 的方法。\n  * DP 没有用 sampling，它是直接用 Bellman expectation equation 来更新状态价值的。\n  * TD 用了 sampling。TD target 由两部分组成，一部分是 sampling，一部分是 bootstrapping。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/comparison_2.png)\n\nDP 是直接算 expectation，把它所有相关的状态都进行加和。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/comparison_3.png)\n\nMC 在当前状态下，采一个支路，在一个path 上进行更新，更新这个 path 上的所有状态。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/comparison_4.png)\n\nTD 是从当前状态开始，往前走了一步，关注的是非常局部的步骤。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/comparison_5.png)\n\n* 如果 TD 需要更广度的 update，就变成了 DP（因为 DP 是把所有状态都考虑进去来进行更新）。\n* 如果 TD 需要更深度的 update，就变成了 MC。\n* 右下角是穷举的方法（exhaustive search），穷举的方法既需要很深度的信息，又需要很广度的信息。\n\n## Model-free Control\n\nQ: 当我们不知道 MDP 模型情况下，如何优化价值函数，得到最佳的策略？\n\nA: 我们可以把 policy iteration 进行一个广义的推广，使它能够兼容 MC 和 TD 的方法，即 `Generalized Policy Iteration(GPI) with MC and TD`。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_1.png)\n\nPolicy iteration 由两个步骤组成：\n\n1. 根据给定的当前的 policy $\\pi$ 来估计价值函数；\n2. 得到估计的价值函数后，通过 greedy 的方法来改进它的算法。\n\n这两个步骤是一个互相迭代的过程。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_2.png)\n\n得到一个价值函数过后，我们并不知道它的奖励函数和状态转移，所以就没法估计它的 Q 函数。所以这里有一个问题：当我们不知道奖励函数和状态转移时，如何进行策略的优化。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_3.png)\n\n针对上述情况，我们引入了广义的 policy iteration 的方法。\n\n我们对 policy evaluation 部分进行修改：用 MC 的方法代替 DP 的方法去估计 Q 函数。\n\n当得到 Q 函数后，就可以通过 greedy 的方法去改进它。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_4.png)\n\n上图是用 MC 估计 Q 函数的算法。\n\n* 假设每一个 episode 都有一个 `exploring start`，exploring start 保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地去估计。\n* 算法通过 MC 的方法产生了很多的轨迹，每个轨迹都可以算出它的价值。然后，我们可以通过 average 的方法去估计 Q 函数。Q 函数可以看成一个 Q-table，通过采样的方法把表格的每个单元的值都填上，然后我们使用 policy improvement 来选取更好的策略。\n* 算法核心：如何用 MC 方法来填 Q-table。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_5.png)\n\n为了确保 MC 方法能够有足够的探索，我们使用了 $\\varepsilon$-greedy exploration。\n\n$\\varepsilon\\text{-greedy}$ 的意思是说，我们有 $1-\\varepsilon$ 的概率会按照 Q-function 来决定 action，通常 $\\varepsilon$ 就设一个很小的值， $1-\\varepsilon$ 可能是 90%，也就是 90% 的概率会按照 Q-function 来决定 action，但是你有 10% 的机率是随机的。通常在实现上 $\\varepsilon$ 会随着时间递减。在最开始的时候。因为还不知道那个 action 是比较好的，所以你会花比较大的力气在做 exploration。接下来随着训练的次数越来越多。已经比较确定说哪一个 Q 是比较好的。你就会减少你的 exploration，你会把 $\\varepsilon$ 的值变小，主要根据 Q-function 来决定你的 action，比较少做 random，这是 $\\varepsilon\\text{-greedy}$。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_6.png)\n\n当我们使用 MC 和 $\\varepsilon$-greedy 探索这个形式的时候，我们可以确保价值函数是单调的，改进的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_7.png)上图是带 $\\varepsilon$-greedy 探索的 MC 算法的伪代码。\n\n与 MC 相比，TD 有如下几个优势：\n\n* 低方差。\n* 能够在线学习。\n* 能够从不完整的序列学习。\n\n所以我们可以把 TD 也放到 control loop 里面去估计 Q-table，再采取这个 $\\varepsilon$-greedy policy improvement。这样就可以在 episode 没结束的时候来更新已经采集到的状态价值。  \n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/bias_variance.png \':size=450\')\n\n>* **偏差(bias)：**描述的是预测值（估计值）的期望与真实值之间的差距。偏差越大，越偏离真实数据，如上图第二行所示。\n>* **方差(variance)：**描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，如上图右列所示。\n\n### Sarsa: On-policy TD Control\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/model_free_control_9.png)\n\nTD 是给定了一个策略，然后我们去估计它的价值函数。接着我们要考虑怎么用 TD 这个框架来估计 Q-function。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.14.png)Sarsa 所作出的改变很简单，就是将原本我们 TD 更新 V 的过程，变成了更新 Q，如下式所示：\n\n$$\nQ\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right)-Q\\left(S_{t}, A_{t}\\right)\\right]\n$$\n这个公式就是说可以拿下一步的 Q 值 $Q(S_{t+_1},A_{t+1})$ 来更新我这一步的 Q 值 $Q(S_t,A_t)$ 。\n\nSarsa 是直接估计 Q-table，得到 Q-table 后，就可以更新策略。\n\n为了理解这个公式，如上图所示，我们先把 $R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right.)$ 当作是一个目标值，就是 $Q(S_t,A_t)$ 想要去逼近的一个目标值。$R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right.)$ 就是 TD target。\n\n我们想要计算的就是 $Q(S_t,A_t)$ 。因为最开始 Q 值都是随机初始化或者是初始化为零，它需要不断地去逼近它理想中真实的 Q 值(TD target)，$R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right)-Q\\left(S_{t}, A_{t}\\right)$ 就是 TD 误差。\n\n也就是说，我们拿 $Q(S_t,A_t)$ 来逼近 $G_t$，那 $Q(S_{t+1},A_{t+1})$ 其实就是近似 $G_{t+1}$。我就可以用  $Q(S_{t+1},A_{t+1})$ 近似 $G_{t+1}$，然后把  $R_{t+1}+Q(S_{t+1},A_{t+1})$  当成目标值。\n\n$Q(S_t,A_t)$  就是要逼近这个目标值，我们用软更新的方式来逼近。软更新的方式就是每次我只更新一点点，$\\alpha$ 类似于学习率。最终的话，Q 值都是可以慢慢地逼近到真实的 target 值。这样我们的更新公式只需要用到当前时刻的 $S_{t},A_t$，还有拿到的 $R_{t+1}, S_{t+1}，A_{t+1}$ 。\n\n**该算法由于每次更新值函数需要知道当前的状态(state)、当前的动作(action)、奖励(reward)、下一步的状态(state)、下一步的动作(action)，即 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 这几个值 ，由此得名 `Sarsa` 算法**。它走了一步之后，拿到了 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$  之后，就可以做一次更新。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.15.png)\n\n我们直接看这个框框里面的更新公式， 和之前的公式是一样的。$S\'$ 就是 $S_{t+1}$ 。我们就是拿下一步的 Q 值 $Q(S\',A\')$ 来更新这一步的 Q 值 $Q(S,A)$，不断地强化每一个 Q。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/n-step_sarsa.png)Sarsa 属于单步更新法，也就是说每执行一个动作，就会更新一次价值和策略。如果不进行单步更新，而是采取 $n$ 步更新或者回合更新，即在执行 $n$ 步之后再来更新价值和策略，这样就得到了 `n 步 Sarsa(n-step Sarsa)`。\n\n比如 2-step Sarsa，就是执行两步后再来更新 Q 的值。\n\n具体来说，对于 Sarsa，在 $t$ 时刻其价值的计算公式为\n$$\nq_{t}=R_{t+1}+\\gamma Q\\left(S_{t+1}, A_{t+1}\\right)\n$$\n而对于 $n$ 步 Sarsa，它的 $n$ 步 Q 收获为\n$$\nq_{t}^{(n)}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1} R_{t+n}+\\gamma^{n} Q\\left(S_{t+n}, A_{t+n}\\right)\n$$\n\n如果给 $q_t^{(n)}$ 加上衰减因子 $\\lambda$ 并进行求和，即可得到 Sarsa($\\lambda$) 的 Q 收获：\n$$\nq_{t}^{\\lambda}=(1-\\lambda) \\sum_{n=1}^{\\infty} \\lambda^{n-1} q_{t}^{(n)}\n$$\n因此，$n$ 步 Sarsa($\\lambda$)的更新策略可以表示为\n$$\nQ\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left(q_{t}^{\\lambda}-Q\\left(S_{t}, A_{t}\\right)\\right)\n$$\n总的来说，Sarsa 和 Sarsa($\\lambda$) 的差别主要体现在价值的更新上。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.16.png)\n\n我们看看用代码去怎么去实现。了解单步更新的一个基本公式之后，代码实现就很简单了。右边是环境，左边是 agent 。我们每次跟环境交互一次之后呢，就可以 learn 一下，向环境输出 action，然后从环境当中拿到 state 和 reward。Agent 主要实现两个方法：\n\n* 一个就是根据 Q 表格去选择动作，输出 action。\n* 另外一个就是拿到 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$  这几个值去更新我们的 Q 表格。\n\n### Q-learning: Off-policy TD Control\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.17.png)\n\nSarsa 是一种 on-policy 策略。Sarsa 优化的是它实际执行的策略，它直接拿下一步会执行的 action 来去优化 Q 表格，所以 on-policy 在学习的过程中，只存在一种策略，它用一种策略去做 action 的选取，也用一种策略去做优化。所以 Sarsa 知道它下一步的动作有可能会跑到悬崖那边去，所以它就会在优化它自己的策略的时候，会尽可能的离悬崖远一点。这样子就会保证说，它下一步哪怕是有随机动作，它也还是在安全区域内。\n\n而 off-policy 在学习的过程中，有两种不同的策略:\n\n* 第一个策略是我们需要去学习的策略，即`target policy(目标策略)`，一般用 $\\pi$ 来表示，Target policy 就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。\n* 另外一个策略是探索环境的策略，即`behavior policy(行为策略)`，一般用 $\\mu$ 来表示。$\\mu$ 可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据喂给 target policy 去学习。而且喂给目标策略的数据中并不需要 $A_{t+1}$ ，而 Sarsa 是要有 $A_{t+1}$ 的。Behavior policy 像是一个战士，可以在环境里面探索所有的动作、轨迹和经验，然后把这些经验交给目标策略去学习。比如目标策略优化的时候，Q-learning 不会管你下一步去往哪里探索，它就只选收益最大的策略。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/off_policy_learning.png)\n\n再举个例子，如上图所示，比如环境是一个波涛汹涌的大海，但 learning policy 很胆小，没法直接跟环境去学习，所以我们有了 exploratory policy，exploratory policy 是一个不畏风浪的海盗，他非常激进，可以在环境中探索。他有很多经验，可以把这些经验写成稿子，然后喂给这个 learning policy。Learning policy 可以通过这个稿子来进行学习。\n\n在 off-policy learning 的过程中，我们这些轨迹都是 behavior policy 跟环境交互产生的，产生这些轨迹后，我们使用这些轨迹来更新 target policy $\\pi$。\n\n**Off-policy learning 有很多好处：**\n\n* 我们可以利用 exploratory policy 来学到一个最佳的策略，学习效率高；\n* 可以让我们学习其他 agent 的行为，模仿学习，学习人或者其他 agent 产生的轨迹；\n* 重用老的策略产生的轨迹。探索过程需要很多计算资源，这样的话，可以节省资源。\n\nQ-learning 有两种 policy：behavior policy 和 target policy。\n\nTarget policy $\\pi$ 直接在 Q-table 上取 greedy，就取它下一步能得到的所有状态，如下式所示：\n$$\n\\pi\\left(S_{t+1}\\right)=\\underset{a^{\\prime}}{\\arg \\max}~ Q\\left(S_{t+1}, a^{\\prime}\\right)\n$$\nBehavior policy $\\mu$ 可以是一个随机的 policy，但我们采取 $\\varepsilon\\text{-greedy}$，让 behavior policy 不至于是完全随机的，它是基于 Q-table 逐渐改进的。\n\n我们可以构造 Q-learning target，Q-learning 的 next action 都是通过 arg max 操作来选出来的，于是我们可以代入 arg max 操作，可以得到下式：\n$$\n\\begin{aligned}\nR_{t+1}+\\gamma Q\\left(S_{t+1}, A^{\\prime}\\right) &=R_{t+1}+\\gamma Q\\left(S_{t+1},\\arg \\max ~Q\\left(S_{t+1}, a^{\\prime}\\right)\\right) \\\\\n&=R_{t+1}+\\gamma \\max _{a^{\\prime}} Q\\left(S_{t+1}, a^{\\prime}\\right)\n\\end{aligned}\n$$\n接着我们可以把 Q-learning 更新写成增量学习的形式，TD target 就变成 max 的值，即\n$$\nQ\\left(S_{t}, A_{t}\\right) \\leftarrow Q\\left(S_{t}, A_{t}\\right)+\\alpha\\left[R_{t+1}+\\gamma \\max _{a} Q\\left(S_{t+1}, a\\right)-Q\\left(S_{t}, A_{t}\\right)\\right]\n$$\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.18.png)\n\n **我们再通过对比的方式来进一步理解 `Q-learning`。Q-learning 是 off-policy 的时序差分学习方法，Sarsa 是 on-policy 的时序差分学习方法。**\n\n* Sarsa 在更新 Q 表格的时候，它用到的 A\' 。我要获取下一个 Q 值的时候，A\' 是下一个 step 一定会执行的 action。这个 action 有可能是 $\\varepsilon$-greedy 方法采样出来的值，也有可能是 max Q 对应的 action，也有可能是随机动作，但这是它实际执行的那个动作。\n* 但是 Q-learning 在更新 Q 表格的时候，它用到这个的 Q 值 $Q(S\',a)$ 对应的那个 action ，它不一定是下一个 step 会执行的实际的 action，因为你下一个实际会执行的那个 action 可能会探索。\n* Q-learning 默认的 next action 不是通过 behavior policy 来选取的，Q-learning 直接看 Q-table，取它的 max 的这个值，它是默认 A\' 为最优策略选的动作，所以 Q-learning 在学习的时候，不需要传入 A\'，即 $A_{t+1}$  的值。\n\n> 事实上，Q-learning 算法被提出的时间更早，Sarsa 算法是 Q-learning 算法的改进。\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.19.png)\n\n**Sarsa 和 Q-learning 的更新公式都是一样的，区别只在 target 计算的这一部分，**\n\n* Sarsa 是 $R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})$  ；\n* Q-learning 是 $R_{t+1}+\\gamma  \\underset{a}{\\max} Q\\left(S_{t+1}, a\\right)$ 。\n\nSarsa 是用自己的策略产生了 S,A,R,S\',A\' 这一条轨迹。然后拿着 $Q(S_{t+1},A_{t+1})$ 去更新原本的 Q 值 $Q(S_t,A_t)$。 \n\n但是 Q-learning 并不需要知道我实际上选择哪一个 action ，它默认下一个动作就是 Q 最大的那个动作。Q-learning 知道实际上 behavior policy 可能会有 10% 的概率去选择别的动作，但 Q-learning 并不担心受到探索的影响，它默认了就按照最优的策略来去优化目标策略，所以它可以更大胆地去寻找最优的路径，它会表现得比 Sarsa 大胆非常多。\n\n对 Q-learning 进行逐步地拆解的话，跟 Sarsa 唯一一点不一样就是并不需要提前知道 $A_2$ ，我就能更新 $Q(S_1,A_1)$ 。在训练一个 episode 这个流程图当中，Q-learning 在 learn 之前它也不需要去拿到 next action $A\'$，它只需要前面四个 $ (S,A,R,S\')$ ，这跟 Sarsa 很不一样。 \n## On-policy vs. Off-policy\n\n**总结一下 on-policy 和 off-policy 的区别。**\n\n* Sarsa 是一个典型的 on-policy 策略，它只用了一个 policy $\\pi$，它不仅使用策略 $\\pi$ 学习，还使用策略 $\\pi$ 与环境交互产生经验。如果 policy 采用 $\\varepsilon$-greedy 算法的话，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点胆小。它在解决悬崖问题的时候，会尽可能地离悬崖边上远远的，确保说哪怕自己不小心探索了一点，也还是在安全区域内。此外，因为采用的是 $\\varepsilon$-greedy 算法，策略会不断改变($\\varepsilon$ 会不断变小)，所以策略不稳定。\n* Q-learning 是一个典型的 off-policy 的策略，它有两种策略：target policy 和 behavior policy。它分离了目标策略跟行为策略。Q-learning 就可以大胆地用 behavior policy 去探索得到的经验轨迹来去优化目标策略，从而更有可能去探索到最优的策略。Behavior policy 可以采用 $\\varepsilon$-greedy 算法，但 target policy 采用的是 greedy 算法，直接根据 behavior policy 采集到的数据来采用最优策略，所以 Q-learning 不需要兼顾探索。\n* 比较 Q-learning 和 Sarsa 的更新公式可以发现，Sarsa 并没有选取最大值的 max 操作，因此，\n  * Q-learning 是一个非常激进的算法，希望每一步都获得最大的利益；\n  * 而 Sarsa 则相对非常保守，会选择一条相对安全的迭代路线。\n\n\n\n## Summary\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/3.21.png)\n\n总结如上图所示。\n\n## References\n\n* [百度强化学习](https://aistudio.baidu.com/aistudio/education/lessonvideo/460292)\n\n* [强化学习基础 David Silver 笔记](https://zhuanlan.zhihu.com/c_135909947)\n* [Intro to Reinforcement Learning (强化学习纲要）](https://github.com/zhoubolei/introRL)\n* [Reinforcement Learning: An Introduction (second edition)](https://book.douban.com/subject/30323890/)\n* [百面深度学习](https://book.douban.com/subject/35043939/)\n* [神经网络与深度学习](https://nndl.github.io/)\n* [机器学习](https://book.douban.com/subject/26708119//)\n* [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)','2021-12-10 12:43:49','2021-12-19 19:24:17'),
	(39,3,'第三章 习题','## 1 Keywords\r\n\r\n- **P函数和R函数：** P函数反应的是状态转移的概率，即反应的环境的随机性，R函数就是Reward function。但是我们通常处于一个未知的环境（即P函数和R函数是未知的）。\r\n- **Q表格型表示方法：** 表示形式是一种表格形式，其中横坐标为 action（agent）的行为，纵坐标是环境的state，其对应着每一个时刻agent和环境的情况，并通过对应的reward反馈去做选择。一般情况下，Q表格是一个已经训练好的表格，不过，我们也可以每进行一步，就更新一下Q表格，然后用下一个状态的Q值来更新这个状态的Q值（即时序差分方法）。\r\n- **时序差分（Temporal Difference）：** 一种Q函数（Q值）的更新方式，也就是可以拿下一步的 Q 值 $Q(S_{t+_1},A_{t+1})$ 来更新我这一步的 Q 值 $Q(S_t,A_t)$ 。完整的计算公式如下：$Q(S_t,A_t) \\larr Q(S_t,A_t) + \\alpha [R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$\r\n- **SARSA算法：** 一种更新前一时刻状态的单步更新的强化学习算法，也是一种on-policy策略。该算法由于每次更新值函数需要知道前一步的状态(state)，前一步的动作(action)、奖励(reward)、当前状态(state)、将要执行的动作(action)，即 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 这几个值，所以被称为SARSA算法。agent每进行一次循环，都会用 $(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$ 对于前一步的Q值（函数）进行一次更新。\r\n\r\n## 2 Questions\r\n\r\n- 构成强化学习MDP的四元组有哪些变量？\r\n\r\n  答：状态、动作、状态转移概率和奖励，分别对应（S，A，P，R），后面有可能会加上个衰减因子构成五元组。\r\n\r\n- 基于以上的描述所构成的强化学习的“学习”流程。\r\n\r\n  答：强化学习要像人类一样去学习了，人类学习的话就是一条路一条路的去尝试一下，先走一条路，我看看结果到底是什么。多试几次，只要能一直走下去的，我们其实可以慢慢的了解哪个状态会更好。我们用价值函数 $V(s)$ 来代表这个状态是好的还是坏的。然后用这个 Q 函数来判断说在什么状态下做什么动作能够拿到最大奖励，我们用 Q 函数来表示这个状态-动作值。\r\n\r\n- 基于SARSA算法的agent的学习过程。\r\n\r\n  答：我们现在有环境，有agent。每交互一次以后，我们的agent会向环境输出action，接着环境会反馈给agent当前时刻的state和reward。那么agent此时会实现两个方法：\r\n  \r\n  1.使用已经训练好的Q表格，对应环境反馈的state和reward选取对应的action进行输出。\r\n  \r\n  2.我们已经拥有了$(S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1})$  这几个值，并直接使用 $A_{t+1}$ 去更新我们的Q表格。\r\n\r\n- Q-learning和Sarsa的区别？\r\n\r\n  答：Sarsa算法是Q-learning算法的改进。（这句话出自「神经网络与深度学习」的第 342 页）（可参考SARSA「on-line q-learning using connectionist systems」的 abstract 部分）\r\n\r\n  1. 首先，Q-learning 是 off-policy 的时序差分学习方法，Sarsa 是 on-policy 的时序差分学习方法。\r\n\r\n  2. 其次，Sarsa 在更新 Q 表格的时候，它用到的 A\' 。我要获取下一个 Q 值的时候，A\' 是下一个 step 一定会执行的 action 。这个 action 有可能是 $\\varepsilon$-greddy 方法 sample 出来的值，也有可能是 max Q 对应的 action，也有可能是随机动作。但是就是它实实在在执行了的那个动作。\r\n\r\n  3. 但是 Q-learning 在更新 Q 表格的时候，它用到这个的 Q 值 $Q(S\',a\')$ 对应的那个 action ，它不一定是下一个 step 会执行的实际的 action，因为你下一个实际会执行的那个 action 可能会探索。Q-learning 默认的 action 不是通过 behavior policy 来选取的，它是默认 A\' 为最优策略选的动作，所以 Q-learning 在学习的时候，不需要传入 A\'，即 $a_{t+1}$  的值。\r\n\r\n  4. 更新公式的对比（区别只在target计算这一部分）：\r\n\r\n     - Sarsa的公式： $R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})$ ；\r\n     - Q-learning的公式：$R_{t+1}+\\gamma  \\underset{a}{\\max} Q\\left(S_{t+1}, a\\right)$\r\n\r\n     Sarsa 实际上都是用自己的策略产生了 S,A,R,S\',A\' 这一条轨迹。然后拿着 $Q(S_{t+1},A_{t+1})$ 去更新原本的 Q 值 $Q(S_t,A_t)$。 但是 Q-learning 并不需要知道，我实际上选择哪一个 action ，它默认下一个动作就是 Q 最大的那个动作。所以基于此，Sarsa的action通常会更加“保守”、“胆小”，而对应的Q-Learning的action会更加“莽撞”、“激进”。\r\n\r\n- On-policy和 off-policy 的区别？\r\n\r\n  答：\r\n\r\n  1. Sarsa 就是一个典型的 on-policy 策略，它只用一个 $\\pi$ ，为了兼顾探索和利用，所以它训练的时候会显得有点胆小怕事。它在解决悬崖问题的时候，会尽可能地离悬崖边上远远的，确保说哪怕自己不小心探索了一点了，也还是在安全区域内不不至于跳进悬崖。\r\n  2. Q-learning 是一个比较典型的 off-policy 的策略，它有目标策略 target policy，一般用 $\\pi$ 来表示。然后还有行为策略 behavior policy，用 $\\mu$ 来表示。它分离了目标策略跟行为策略。Q-learning 就可以大胆地用 behavior policy 去探索得到的经验轨迹来去优化我的目标策略。这样子我更有可能去探索到最优的策略。\r\n  3. 比较 Q-learning 和 Sarsa 的更新公式可以发现，Sarsa 并没有选取最大值的 max 操作。因此，Q-learning 是一个非常激进的算法，希望每一步都获得最大的利益；而 Sarsa 则相对非常保守，会选择一条相对安全的迭代路线。\r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：同学，你能否简述on-policy和off-policy的区别？\r\n\r\n  答： off-policy和on-policy的根本区别在于生成样本的policy和参数更新时的policy是否相同。对于on-policy，行为策略和要优化的策略是一个策略，更新了策略后，就用该策略的最新版本对于数据进行采样；对于off-policy，使用任意的一个行为策略来对于数据进行采样，并利用其更新目标策略。如果举例来说，Q-learning在计算下一状态的预期收益时使用了max操作，直接选择最优动作，而当前policy并不一定能选择到最优的action，因此这里生成样本的policy和学习时的policy不同，所以Q-learning为off-policy算法；相对应的SARAS则是基于当前的policy直接执行一次动作选择，然后用这个样本更新当前的policy，因此生成样本的policy和学习时的policy相同，所以SARAS算法为on-policy算法。\r\n\r\n- 高冷的面试官：小同学，能否讲一下Q-Learning，最好可以写出其 $Q(s,a)$ 的更新公式。另外，它是on-policy还是off-policy，为什么？\r\n\r\n  答： Q-learning是通过计算最优动作值函数来求策略的一种时序差分的学习方法，其更新公式为：\r\n  $$\r\n  Q(s, a) \\larr Q(s, a) + \\alpha [r(s,a) + \\gamma \\max_{a\'} Q(s\', a\') - Q(s, a)]\r\n  $$\r\n  其是off-policy的，由于是Q更新使用了下一个时刻的最大值，所以我们只关心哪个动作使得 $Q(s_{t+1}, a)$ 取得最大值，而实际到底采取了哪个动作（行为策略），并不关心。这表明优化策略并没有用到行为策略的数据，所以说它是 off-policy 的。\r\n\r\n- 高冷的面试官：小朋友，能否讲一下SARSA，最好可以写出其Q(s,a)的更新公式。另外，它是on-policy还是off-policy，为什么？\r\n\r\n  答：SARSA可以算是Q-learning的改进（这句话出自「神经网络与深度学习」的第 342 页）（可参考SARSA「on-line q-learning using connectionist systems」的 abstract 部分），其更新公式为：\r\n  $$\r\n  Q(s, a) \\larr Q(s, a) + \\alpha [r(s,a) + \\gamma  Q(s\', a\') - Q(s, a)]\r\n  $$\r\n  其为on-policy的，SARSA必须执行两次动作得到 $(s,a,r,s\',a\') $才可以更新一次；而且 $a\'$ 是在特定策略 $\\pi$ 的指导下执行的动作，因此估计出来的 $Q(s,a)$ 是在该策略 $\\pi$ 之下的Q-value，样本生成用的 $\\pi$ 和估计的 $\\pi$ 是同一个，因此是on-policy。\r\n\r\n- 高冷的面试官：请问value-based和policy-based的区别是什么？\r\n\r\n  答：\r\n\r\n  1. 生成policy上的差异：前者确定，后者随机。Value-Base中的 action-value估计值最终会收敛到对应的true values（通常是不同的有限数，可以转化为0到1之间的概率），因此通常会获得一个确定的策略（deterministic policy）；而Policy-Based不会收敛到一个确定性的值，另外他们会趋向于生成optimal stochastic policy。如果optimal policy是deterministic的，那么optimal action对应的性能函数将远大于suboptimal actions对应的性能函数，性能函数的大小代表了概率的大小。\r\n  2. 动作空间是否连续，前者离散，后者连续。Value-Base，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优action，会在这附近徘徊，过小的离散间距会使得action的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度；而Policy-Based适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过Gaussian distribution （正态分布）选择action。\r\n  3. value-based，例如Q-learning，是通过求解最优值函数间接的求解最优策略；policy-based，例如REINFORCE，Monte-Carlo Policy Gradient，等方法直接将策略参数化，通过策略搜索，策略梯度或者进化方法来更新策略的参数以最大化回报。基于值函数的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举和离策略时会有收敛性问题。策略梯度具有良好的收敛性证明。\r\n  4. 补充：对于值迭代和策略迭代：策略迭代。它有两个循环，一个是在策略估计的时候，为了求当前策略的值函数需要迭代很多次。另外一个是外面的大循环，就是策略评估，策略提升这个循环。值迭代算法则是一步到位，直接估计最优值函数，因此没有策略提升环节。\r\n\r\n- 高冷的面试官：请简述以下时序差分(Temporal Difference，TD)算法。\r\n\r\n  答：TD算法是使用广义策略迭代来更新Q函数的方法，核心使用了自举（bootstrapping），即值函数的更新使用了下一个状态的值函数来估计当前状态的值。也就是使用下一步的 $Q$ 值 $Q(S_{t+1},A_{t+1})$ 来更新我这一步的 Q 值 $Q(S_t,A_t) $。完整的计算公式如下：\r\n  $$\r\n  Q(S_t,A_t) \\larr Q(S_t,A_t) + \\alpha [R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})]\r\n  $$\r\n  \r\n- 高冷的面试官：请问蒙特卡洛方法（Monte Carlo Algorithm，MC）和时序差分(Temporal Difference，TD)算法是无偏估计吗？另外谁的方法更大呢？为什么呢？\r\n\r\n  答：蒙特卡洛方法（MC）是无偏估计，时序差分（TD）是有偏估计；MC的方差较大，TD的方差较小，原因在于TD中使用了自举（bootstrapping）的方法，实现了基于平滑的效果，导致估计的值函数的方差更小。\r\n  \r\n- 高冷的面试官：能否简单说下动态规划、蒙特卡洛和时序差分的异同点？\r\n\r\n  答：\r\n\r\n  - 相同点：都用于进行值函数的描述与更新，并且所有方法都是基于对未来事件的展望来计算一个回溯值。\r\n\r\n  - 不同点：蒙特卡洛和TD算法隶属于model-free，而动态规划属于model-based；TD算法和蒙特卡洛的方法，因为都是基于model-free的方法，因而对于后续状态的获知也都是基于试验的方法；TD算法和动态规划的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的值函数的更新。\r\n\r\n    另外，TD算法不需要等到实验结束后才能进行当前状态的值函数的计算与更新，而蒙特卡洛的方法需要试验交互，产生一整条的马尔科夫链并直到最终状态才能进行更新。TD算法和动态规划的策略评估不同之处为model-free和model-based 这一点，动态规划可以凭借已知转移概率就能推断出来后续的状态情况，而TD只能借助试验才能知道。\r\n\r\n    蒙特卡洛方法和TD方法的不同在于，蒙特卡洛方法进行完整的采样来获取了长期的回报值，因而在价值估计上会有着更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于毕竟基于试验的采样得到，和真实的分布还是有差距，不充足的交互导致的较大方差。而TD算法与其相反，因为只考虑了前一步的回报值 其他都是基于之前的估计值，因而相对来说，其估计值具有偏差大方差小的特点。\r\n\r\n  - 三者的联系：对于$TD(\\lambda)$方法，如果 $ \\lambda = 0$ ，那么此时等价于TD，即只考虑下一个状态；如果$ \\lambda = 1$，等价于MC，即考虑 $T-1$ 个后续状态即到整个episode序列结束。\r\n','2021-12-10 12:43:49','2021-12-19 19:24:44'),
	(40,3,'项目一 使用Q-learning解决悬崖寻路问题','## 使用Q-learning解决悬崖寻路问题\n\n强化学习在运动规划方面也有很大的应用前景，具体包括路径规划与决策，群体派单等等，本次项目就将单体运动规划抽象并简化，让大家初步认识到强化学习在这方面的应用。在运动规划方面，其实已有很多适用于强化学习的仿真环境，小到迷宫，大到贴近真实的自动驾驶环境[CARLA](http://carla.org/)，对这块感兴趣的童鞋可以再多搜集一点。本项目采用gym开发的```CliffWalking-v0```环境，在上面实现一个简单的Q-learning入门demo。\n\n## CliffWalking-v0环境简介\n\n首先对该环境做一个简介，该环境中文名称叫悬崖寻路问题（CliffWalking），是指在一个4 x 12的网格中，智能体以网格的左下角位置为起点，以网格的下角位置为终点，目标是移动智能体到达终点位置，智能体每次可以在上、下、左、右这4个方向中移动一步，每移动一步会得到-1单位的奖励。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/cliffwalking_1.png)\n\n如图，红色部分表示悬崖，数字代表智能体能够观测到的位置信息，即observation，总共会有0-47等48个不同的值，智能体再移动中会有以下限制：\n\n* 智能体不能移出网格，如果智能体想执行某个动作移出网格，那么这一步智能体不会移动，但是这个操作依然会得到-1单位的奖励\n\n* 如果智能体“掉入悬崖” ，会立即回到起点位置，并得到-100单位的奖励\n\n* 当智能体移动到终点时，该回合结束，该回合总奖励为各步奖励之和\n\n实际的仿真界面如下：\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/cliffwalking_2.png)\n\n**由于从起点到终点最少需要13步，每步得到-1的reward，因此最佳训练算法下，每个episode下reward总和应该为-13**。所以我们的目标也是要通过RL训练出一个模型，使得该模型能在测试中一个episode的reward能够接近于-13左右。\n\n## RL基本训练接口\n\n以下是强化学习算法的基本接口，也就是一个完整的上层训练模式，首先是初始化环境和智能体，然后每个episode中，首先agent选择action给到环境，然后环境反馈出下一个状态和reward，然后agent开始更新或者学习，如此多个episode之后agent开始收敛并保存模型。其中可以通过可视化reward随每个episode的变化来查看训练的效果。另外由于强化学习的不稳定性，在收敛的状态下也可能会有起伏的情况，此时可以使用滑动平均的reward让曲线更加平滑便于分析。\n\n```python\n  \'\'\'初始化环境\'\'\'  \nenv = gym.make(\"CliffWalking-v0\")  # 0 up, 1 right, 2 down, 3 left\nenv = CliffWalkingWapper(env)\nagent = QLearning(\n    state_dim=env.observation_space.n,\n    action_dim=env.action_space.n,\n    learning_rate=cfg.policy_lr,\n    gamma=cfg.gamma,\nrewards = []  \nma_rewards = [] # moving average reward\nfor i_ep in range(cfg.train_eps): # train_eps: 训练的最大episodes数\n    ep_reward = 0  # 记录每个episode的reward\n    state = env.reset()  # 重置环境, 重新开一局（即开始新的一个episode）\n    while True:\n        action = agent.choose_action(state)  # 根据算法选择一个动作\n        next_state, reward, done, _ = env.step(action)  # 与环境进行一次动作交互\n        agent.update(state, action, reward, next_state, done)  # Q-learning算法更新\n        state = next_state  # 存储上一个观察值\n        ep_reward += reward\n        if done:\n            break\n    rewards.append(ep_reward)\n    if ma_rewards:\n        ma_rewards.append(ma_rewards[-1]*0.9+ep_reward*0.1)\n    else:\n        ma_rewards.append(ep_reward)\n    print(\"Episode:{}/{}: reward:{:.1f}\".format(i_ep+1, cfg.train_eps,ep_reward))\n```\n\n## 任务要求\n\n基于以上的目标，本次任务即使训练并绘制reward以及滑动平均后的reward随episode的变化曲线图并记录超参数写成报告，示例如下：\n\n![rewards](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/rewards.png)\n\n![moving_average_rewards](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/moving_average_rewards.png)\n\n## 主要代码清单\n\n**main.py** 或 **task_train.py**：保存强化学习基本接口，以及相应的超参数\n\n**agent.py**: 保存算法模型，主要包含choose_action(预测动作)和update两个函数，有时会多一个predict_action函数，此时choose_action使用了epsilon-greedy策略便于训练的探索，而测试时用predict_action单纯贪心地选择网络的值输出动作\n\n**model.py**：保存神经网络，比如全连接网络等等，对于一些算法，分为Actor和Critic两个类\n\n**memory.py**：保存replay buffer，根据算法的不同，replay buffer功能有所不同，因此会改写\n\n**plot.py**：保存相关绘制函数\n\n[参考代码](https://github.com/datawhalechina/easy-rl/tree/master/codes/QLearning)\n\n## 备注\n\n* 注意 $\\varepsilon$-greedy 策略的使用，以及相应的参数$\\varepsilon$如何衰减\n* 训练模型和测试模型的时候选择动作有一些不同，训练时采取 $\\varepsilon$-greedy策略，而测试时直接选取Q值最大对应的动作，所以算法在动作选择的时候会包括sample(训练时的动作采样)和predict(测试时的动作选择)\n\n* Q值最大对应的动作可能不止一个，此时可以随机选择一个输出结果','2021-12-10 12:43:49','2021-12-19 19:25:30'),
	(41,3,'第四章 策略梯度','##  Policy Gradient\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.1.png)\n\n在强化学习中有 3 个组成部分：`演员(actor)`、`环境(environment)` 和 `奖励函数(reward function)`。\n\n让机器玩视频游戏时，\n\n* 演员做的事情就是去操控游戏的摇杆， 比如说向左、向右、开火等操作；\n* 环境就是游戏的主机， 负责控制游戏的画面，负责控制怪物要怎么移动， 你现在要看到什么画面等等；\n* 奖励函数就是当你做什么事情，发生什么状况的时候，你可以得到多少分数， 比如说杀一只怪兽得到 20 分等等。\n\n同样的概念用在围棋上也是一样的，\n\n* 演员就是 Alpha Go，它要决定下哪一个位置；\n* 环境就是对手；\n* 奖励函数就是按照围棋的规则， 赢就是得一分，输就是负一分。\n\n在强化学习里面，环境跟奖励函数不是你可以控制的，环境跟奖励函数是在开始学习之前，就已经事先给定的。你唯一能做的事情是调整演员里面的策略(policy)，使得演员可以得到最大的奖励。演员里面会有一个策略，这个策略决定了演员的行为。给定一个外界的输入，策略会输出演员现在应该要执行的行为。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.2.png)\n\n* 策略一般写成 $\\pi$。假设你是用深度学习的技术来做强化学习的话，**策略就是一个网络**。网络里面就有一堆参数，我们用 $\\theta$ 来代表 $\\pi$ 的参数。\n\n* **网络的输入就是现在机器看到的东西**，如果让机器打电玩的话，机器看到的东西就是游戏的画面。机器看到什么东西，会影响你现在训练到底好不好训练。举例来说，在玩游戏的时候， 也许你觉得游戏的画面前后是相关的，也许你觉得你应该让你的策略，看从游戏初始到现在这个时间点，所有画面的总和。你可能会觉得你要用到 RNN 来处理它，不过这样子会比较难处理。要让你的机器，你的策略看到什么样的画面，这个是你自己决定的。让你知道说给机器看到什么样的游戏画面，可能是比较有效的。\n* **输出的就是机器要采取什么样的行为。**\n\n* 上图就是具体的例子，\n  * 策略就是一个网络；\n  * 输入 就是游戏的画面，它通常是由像素(pixels)所组成的；\n  * 输出就是看看说有哪些选项是你可以去执行的，输出层就有几个神经元。\n  * 假设你现在可以做的行为有 3 个，输出层就是有 3 个神经元。每个神经元对应到一个可以采取的行为。\n  * 输入一个东西后，网络就会给每一个可以采取的行为一个分数。你可以把这个分数当作是概率。演员就是看这个概率的分布，根据这个概率的分布来决定它要采取的行为。比如说 70% 会向左走，20% 向右走，10% 开火等等。概率分布不同，演员采取的行为就会不一样。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.3.png)\n**接下来用一个例子来说明演员是怎么样跟环境互动的。**\n\n首先演员会看到一个游戏画面，我们用 $s_1$ 来表示游戏初始的画面。接下来演员看到这个游戏的初始画面以后，根据它内部的网络，根据它内部的策略来决定一个动作。假设它现在决定的动作 是向右，它决定完动作 以后，它就会得到一个奖励，代表它采取这个动作以后得到的分数。\n\n我们把一开始的初始画面记作 $s_1$， 把第一次执行的动作记作 $a_1$，把第一次执行动作完以后得到的奖励记作 $r_1$。不同的书会有不同的定义，有人会觉得说这边应该要叫做 $r_2$，这个都可以，你自己看得懂就好。演员决定一个行为以后，就会看到一个新的游戏画面，这边是 $s_2$。然后把这个 $s_2$ 输入给演员，这个演员决定要开火，然后它可能杀了一只怪，就得到五分。这个过程就反复地持续下去，直到今天走到某一个时间点执行某一个动作，得到奖励之后，这个环境决定这个游戏结束了。比如说，如果在这个游戏里面，你是控制绿色的船去杀怪，如果你被杀死的话，游戏就结束，或是你把所有的怪都清空，游戏就结束了。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.4.png)\n\n* 一场游戏叫做一个 `回合(episode)` 或者 `试验(trial)`。\n* 把这场游戏里面所有得到的奖励都加起来，就是 `总奖励(total reward)`，我们称其为`回报(return)`，用 R 来表示它。\n* 演员要想办法去最大化它可以得到的奖励。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.5.png)\n首先，`环境` 是一个`函数`，游戏的主机也可以把它看作是一个函数，虽然它不一定是神经网络，可能是基于规则的(rule-based)规则，但你可以把它看作是一个函数。这个函数一开始就先吐出一个状态，也就是游戏的画面，接下来你的演员看到这个游戏画面 $s_1$ 以后，它吐出 $a_1$，然后环境把 $a_1$ 当作它的输入，然后它再吐出 $s_2$，吐出新的游戏画面。演员看到新的游戏画面，再采取新的行为 $a_2$，然后 环境再看到 $a_2$，再吐出 $s_3$。这个过程会一直持续下去，直到环境觉得说应该要停止为止。\n\n在一场游戏里面，我们把环境输出的 $s$ 跟演员输出的行为 $a$，把 $s$ 跟 $a$ 全部串起来， 叫做一个 `Trajectory(轨迹)`，如下式所示。\n$$\n\\text { Trajectory } \\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t}\\right\\}\n$$\n\n你可以计算每一个轨迹发生的概率。假设现在演员的参数已经被给定了话，就是 $\\theta$。根据 $\\theta$，你其实可以计算某一个轨迹发生的概率，你可以计算某一个回合里面发生这样子状况的概率。\n\n$$\n\\begin{aligned}\np_{\\theta}(\\tau)\n&=p\\left(s_{1}\\right) p_{\\theta}\\left(a_{1} | s_{1}\\right) p\\left(s_{2} | s_{1}, a_{1}\\right) p_{\\theta}\\left(a_{2} | s_{2}\\right) p\\left(s_{3} | s_{2}, a_{2}\\right) \\cdots \\\\\n&=p\\left(s_{1}\\right) \\prod_{t=1}^{T} p_{\\theta}\\left(a_{t} | s_{t}\\right) p\\left(s_{t+1} | s_{t}, a_{t}\\right)\n\\end{aligned}\n$$\n\n怎么算呢，如上式所示。在假设演员的参数就是 $\\theta$ 的情况下，某一个轨迹 $\\tau$ 的概率就是这样算的，你先算环境输出 $s_1$ 的概率，再计算根据 $s_1$ 执行 $a_1$ 的概率，这是由你策略里面的网络参数 $\\theta$ 所决定的， 它是一个概率，因为你的策略的网络的输出是一个分布，演员是根据这个分布去做采样，决定现在实际上要采取的动作是哪一个。接下来环境根据 $a_1$ 跟 $s_1$ 产生 $s_2$，因为 $s_2$ 跟 $s_1$ 还是有关系的，下一个游戏画面跟前一个游戏画面通常还是有关系的，至少要是连续的， 所以给定前一个游戏画面 $s_1$ 和现在演员采取的行为 $a_1$，就会产生 $s_2$。\n\n这件事情可能是概率，也可能不是概率，这个取决于环境，就是主机它内部设定是怎样。看今天这个主机在决定，要输出什么样的游戏画面的时候，有没有概率。因为如果没有概率的话，这个游戏的每次的行为都一样，你只要找到一条路径就可以过关了，这样感觉是蛮无聊的 。所以游戏里面通常还是有一些概率的，你做同样的行为，给同样的前一个画面， 下次产生的画面不见得是一样的。过程就反复继续下去，你就可以计算一个轨迹 $s_1$,$a_1$, $s_2$ , $a_2$ 出现的概率有多大。\n\n**这个概率取决于两部分：环境的行为和 agent 的行为**， \n\n*  `环境的行为` 。环境的函数内部的参数或内部的规则长什么样子。 $p(s_{t+1}|s_t,a_t)$这一项代表的是环境，环境这一项通常你是无法控制它的，因为那个是人家写好的，你不能控制它。\n*  `agent 的行为`。你能控制的是 $p_\\theta(a_t|s_t)$。给定一个 $s_t$，演员要采取什么样的 $a_t$ 会取决于演员的参数 $\\theta$， 所以这部分是演员可以自己控制的。随着演员的行为不同，每个同样的轨迹， 它就会有不同的出现的概率。\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.6.png)\n\n在强化学习里面，除了环境跟演员以外， 还有`奖励函数(reward function)`。\n\n奖励函数根据在某一个状态采取的某一个动作决定说现在这个行为可以得到多少的分数。 它是一个函数，给它 $s_1$，$a_1$，它告诉你得到 $r_1$。给它 $s_2$ ，$a_2$，它告诉你得到 $r_2$。 把所有的 $r$ 都加起来，我们就得到了 $R(\\tau)$ ，代表某一个轨迹 $\\tau$ 的奖励。\n\n在某一场游戏里面， 某一个回合里面，我们会得到 R。**我们要做的事情就是调整演员内部的参数 $\\theta$， 使得 R 的值越大越好。** 但实际上奖励并不只是一个标量，奖励其实是一个随机变量。R 其实是一个随机变量，因为演员在给定同样的状态会做什么样的行为，这件事情是有随机性的。环境在给定同样的观测要采取什么样的动作，要产生什么样的观测，本身也是有随机性的，所以 R 是一个随机变量。你能够计算的是 R 的期望值。你能够计算的是说，在给定某一组参数 $\\theta$ 的情况下，我们会得到的 $R_{\\theta}$ 的期望值是多少。\n$$\n\\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)\n$$\n这个期望值的算法如上式所示。我们要穷举所有可能的轨迹 $\\tau$， 每一个轨迹 $\\tau$ 都有一个概率。\n\n比如 $\\theta$ 是一个很强的模型，它都不会死。因为 $\\theta$ 很强，所以：\n\n* 如果有一个回合 $\\theta$ 很快就死掉了，因为这种情况很少会发生，所以该回合对应的轨迹 $\\tau$ 的概率就很小；\n* 如果有一个回合 $\\theta$ 都一直没有死，因为这种情况很可能发生，所以该回合对应的轨迹 $\\tau$ 的概率就很大。\n\n你可以根据 $\\theta$ 算出某一个轨迹 $\\tau$ 出现的概率，接下来计算这个 $\\tau$ 的总奖励是多少。总奖励使用这个 $\\tau$ 出现的概率进行加权，对所有的 $\\tau$ 进行求和，就是期望值。给定一个参数，你会得到的期望值。\n$$\n\\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)=E_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)]\n$$\n我们还可以写成上式那样，从 $p_{\\theta}(\\tau)$ 这个分布采样一个轨迹 $\\tau$，然后计算 $R(\\tau)$ 的期望值，就是你的期望的奖励。 我们要做的事情就是最大化期望奖励。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.7.png)\n\n怎么最大化期望奖励呢？我们用的是 `梯度上升(gradient ascent)`，因为要让它越大越好，所以是梯度上升。梯度上升在更新参数的时候要加。要进行梯度上升，我们先要计算期望的奖励(expected reward) $\\bar{R}$ 的梯度。我们对 $\\bar{R}$ 取一个梯度，这里面只有 $p_{\\theta}(\\tau)$ 是跟 $\\theta$ 有关，所以梯度就放在 $p_{\\theta}(\\tau)$ 这个地方。$R(\\tau)$ 这个奖励函数不需要是可微分的(differentiable)，这个不影响我们解接下来的问题。举例来说，如果是在 GAN 里面，$R(\\tau)$ 其实是一个 discriminator，它就算是没有办法微分，也无所谓，你还是可以做接下来的运算。\n\n取梯度之后，我们背一个公式：\n$$\n\\nabla f(x)=f(x)\\nabla \\log f(x)\n$$\n我们可以对 $\\nabla p_{\\theta}(\\tau)$ 使用这个公式，然后会得到 $\\nabla p_{\\theta}(\\tau)=p_{\\theta}(\\tau)  \\nabla \\log p_{\\theta}(\\tau)$，进一步地，我们可以得到下式：\n\n$$\n\\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}=\\nabla \\log p_{\\theta}(\\tau)\n$$\n\n如下式所示，对 $\\tau$ 进行求和，把 $R(\\tau)$  和  $\\log p_{\\theta}(\\tau)$ 这两项使用 $p_{\\theta}(\\tau)$ 进行加权， 既然使用 $p_{\\theta}(\\tau)$ 进行加权 ，它们就可以被写成期望的形式。也就是你从 $p_{\\theta}(\\tau)$ 这个分布里面采样 $\\tau$ 出来， 去计算 $R(\\tau)$ 乘上 $\\nabla\\log p_{\\theta}(\\tau)$，然后把它对所有可能的 $\\tau$ 进行求和，就是这个期望的值(expected value)。\n$$\n\\begin{aligned}\n\\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n&=E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\n\\end{aligned}\n$$\n\n实际上这个期望值没有办法算，所以你是用采样的方式来采样一大堆的 $\\tau$。你采样 $N$ 笔  $\\tau$， 然后你去计算每一笔的这些值，然后把它全部加起来，就可以得到梯度。你就可以去更新参数，你就可以去更新你的 agent，如下式所示：\n$$\n\\begin{aligned}\nE_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n&=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n\\end{aligned}\n$$\n下面给出 $\\nabla \\log p_{\\theta}(\\tau)$ 的具体计算过程，如下式所示。\n$$\n\\begin{aligned}\n\\nabla \\log p_{\\theta}(\\tau) &= \\nabla \\left(\\log p(s_1)+\\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+ \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\right) \\\\\n&= \\nabla \\log p(s_1)+ \\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)+  \\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t) \\\\\n&=\\nabla \\sum_{t=1}^{T}\\log p_{\\theta}(a_t|s_t)\\\\\n&=\\sum_{t=1}^{T} \\nabla\\log p_{\\theta}(a_t|s_t)\n\\end{aligned}\n$$\n\n注意， $p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 来自于环境，$p_\\theta(a_t|s_t)$ 是来自于 agent。$p(s_1)$ 和 $p(s_{t+1}|s_t,a_t)$ 由环境决定，所以与 $\\theta$ 无关，因此 $\\nabla \\log p(s_1)=0$ ，$\\nabla \\sum_{t=1}^{T}\\log p(s_{t+1}|s_t,a_t)=0$。\n\n\n$$\n\\begin{aligned}\n\\nabla \\bar{R}_{\\theta}&=\\sum_{\\tau} R(\\tau) \\nabla p_{\\theta}(\\tau)\\\\&=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\frac{\\nabla p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)} \\\\&=\n\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla \\log p_{\\theta}(\\tau) \\\\\n&=E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\\\\\n&\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n&=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n\\end{aligned}\n$$\n\n我们可以直观地来理解上面这个式子，也就是在你采样到的数据里面， 你采样到在某一个状态 $s_t$ 要执行某一个动作 $a_t$， 这个 $s_t$ 跟 $a_t$ 它是在整个轨迹 $\\tau$ 的里面的某一个状态和动作的对。\n\n*  假设你在 $s_t$ 执行 $a_t$，最后发现 $\\tau$ 的奖励是正的， 那你就要增加这一项的概率，你就要增加在 $s_t$ 执行 $a_t$ 的概率。\n*  反之，在 $s_t$ 执行 $a_t$ 会导致 $\\tau$ 的奖励变成负的， 你就要减少这一项的概率。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.8.png)\n这个怎么实现呢？ 你用梯度上升来更新你的参数，你原来有一个参数 $\\theta$ ，把你的 $\\theta$  加上你的梯度这一项，那当然前面要有个学习率，学习率也是要调整的，你可用 Adam、RMSProp 等方法对其进行调整。\n\n我们可以套下面这个公式来把梯度计算出来:\n$$\n\\nabla \\bar{R}_{\\theta}=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\n$$\n实际上，要套上面这个公式， 首先你要先收集一大堆的 s 跟 a 的对(pair)，你还要知道这些 s 跟 a 在跟环境互动的时候，你会得到多少的奖励。 这些资料怎么收集呢？你要拿你的 agent，它的参数是 $\\theta$，去跟环境做互动， 也就是拿你已经训练好的 agent 先去跟环境玩一下，先去跟那个游戏互动一下， 互动完以后，你就会得到一大堆游戏的纪录，你会记录说，今天先玩了第一场，在第一场游戏里面，我们在状态 $s_1$ 采取动作 $a_1$，在状态$s_2$ 采取动作 $a_2$ 。\n\n玩游戏的时候是有随机性的，所以 agent 本身是有随机性的，在同样状态$s_1$，不是每次都会采取 $a_1$，所以你要记录下来。在状态 $s_1^1$ 采取 $a_1^1$，在状态 $s_2^1$ 采取 $a_2^1$。整场游戏结束以后，得到的分数是 $R(\\tau^1)$。你会采样到另外一笔数据，也就是另外一场游戏。在另外一场游戏里面，你在状态 $s_1^2$ 采取 $a_1^2$，在状态 $s_2^2$ 采取 $a_2^2$，然后你采样到的就是 $\\tau^2$，得到的奖励是 $R(\\tau^2)$。\n\n你就可以把采样到的东西代到这个梯度的式子里面，把梯度算出来。也就是把这边的每一个 s 跟 a 的对拿进来，算一下它的对数概率(log probability)。你计算一下在某一个状态采取某一个动作的对数概率，然后对它取梯度，然后这个梯度前面会乘一个权重，权重就是这场游戏的奖励。 有了这些以后，你就会去更新你的模型。\n\n更新完你的模型以后。你要重新去收集数据，再更新模型。注意，一般  `policy gradient(PG) `采样的数据就只会用一次。你把这些数据采样起来，然后拿去更新参数，这些数据就丢掉了。接着再重新采样数据，才能够去更新参数，等一下我们会解决这个问题。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.9.png)\n\n**接下来讲一些实现细节。**\n\n我们可以把它想成一个分类的问题，在分类里面就是输入一个图像，然后输出决定说是 10 个类里面的哪一个。在做分类时，我们要收集一堆训练数据，要有输入跟输出的对。\n\n在实现的时候，你就把状态当作是分类器的输入。 你就当在做图像分类的问题，只是现在的类不是说图像里面有什么东西，而是说看到这张图像我们要采取什么样的行为，每一个行为就是一个类。比如说第一个类叫做向左，第二个类叫做向右，第三个类叫做开火。\n\n在做分类的问题时，要有输入和正确的输出，要有训练数据。而这些训练数据是从采样的过程来的。假设在采样的过程里面，在某一个状态，你采样到你要采取动作 a， 你就把这个动作 a 当作是你的 ground truth。你在这个状态，你采样到要向左。 本来向左这件事概率不一定是最高， 因为你是采样，它不一定概率最高。假设你采样到向左，在训练的时候，你告诉机器说，调整网络的参数， 如果看到这个状态，你就向左。在一般的分类问题里面，其实你在实现分类的时候，你的目标函数都会写成最小化交叉熵(cross entropy)，其实最小化交叉熵就是最大化对数似然(log likelihood)。\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.10.png)\n\n做分类的时候，目标函数就是最大化或最小化的对象， 因为我们现在是最大化似然(likelihood)，所以其实是最大化， 你要最大化的对象，如下式所示:\n$$\n\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n像这种损失函数，你可在 TensorFlow 里调用现成的函数，它就会自动帮你算，然后你就可以把梯度计算出来。这是一般的分类问题，RL 唯一不同的地方是 loss 前面乘上一个权重：整场游戏得到的总奖励 R，它并不是在状态 s 采取动作 a 的时候得到的奖励，如下式所示：\n$$\n\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right)  \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n你要把你的每一笔训练数据，都使用这个 R 进行加权。然后你用 TensorFlow 或 PyTorch 去帮你算梯度就结束了，跟一般分类差不多。\n\n## Tips\n这边有一些在实现的时候，你也许用得上的 tip。\n### Tip 1: Add a Baseline\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.11.png)\n\n**第一个 tip 是 add 一个 baseline。** 如果给定状态 s 采取动作 a 会给你整场游戏正的奖励，就要增加它的概率。如果状态 s 执行动作 a，整场游戏得到负的奖励，就要减少这一项的概率。\n\n但在很多游戏里面，奖励总是正的，就是说最低都是 0。比如说打乒乓球游戏， 你的分数就是介于 0 到 21 分之间，所以 R 总是正的。假设你直接套用这个式子， 在训练的时候告诉模型说，不管是什么动作你都应该要把它的概率提升。 在理想上，这么做并不一定会有问题。因为虽然说 R 总是正的，但它正的量总是有大有小，你在玩乒乓球那个游戏里面，得到的奖励总是正的，但它是介于 0~21分之间，有时候你采取某些动作可能是得到 0 分，采取某些动作可能是得到 20 分。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.12.png)\n\n假设你在某一个状态有 3 个动作 a/b/c可以执行。根据下式，\n$$\n\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n你要把这 3 项的概率，对数概率都拉高。 但是它们前面权重的 R 是不一样的。 R 是有大有小的，权重小的，它上升的就少，权重多的，它上升的就大一点。 因为这个对数概率是一个概率，所以动作 a、b、c 的对数概率的和要是 0。 所以上升少的，在做完归一化(normalize)以后， 它其实就是下降的，上升的多的，才会上升。\n\n\n ![1](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.13.png)\n\n\n这是一个理想上的状况，但是实际上，我们是在做采样就本来这边应该是一个期望(expectation)，对所有可能的 s 跟 a 的对进行求和。 但你真正在学的时候不可能是这么做的，你只是采样了少量的 s 跟 a 的对而已。 因为我们做的是采样，有一些动作可能从来都没有采样到。在某一个状态，虽然可以执行的动作有 a/b/c，但你可能只采样到动作 b，你可能只采样到动作 c，你没有采样到动作 a。但现在所有动作的奖励都是正的，所以根据这个式子，它的每一项的概率都应该要上升。你会遇到的问题是，因为 a 没有被采样到，其它动作的概率如果都要上升，a 的概率就下降。 所以 a 不一定是一个不好的动作， 它只是没被采样到。但只是因为它没被采样到， 它的概率就会下降，这个显然是有问题的，要怎么解决这个问题呢？你会希望你的奖励不要总是正的。\n\n![1.](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.14.png)\n\n为了解决奖励总是正的这个问题，你可以把奖励减掉一项叫做 b，这项 b 叫做 baseline。你减掉这项 b 以后，就可以让 $R(\\tau^n)-b$ 这一项有正有负。 所以如果得到的总奖励 $R(\\tau^n)$ 大于 b 的话，就让它的概率上升。如果这个总奖励小于 b，就算它是正的，正的很小也是不好的，你就要让这一项的概率下降。 如果$R(\\tau^n)<b$  ， 你就要让这个状态采取这个动作的分数下降 。这个 b 怎么设呢？一个最简单的做法就是：你把 $\\tau^n$ 的值取期望， 算一下 $\\tau^n$ 的平均值，即：\n$$\nb \\approx E[R(\\tau)]\n$$\n这是其中一种做法， 你可以想想看有没有其它的做法。\n\n 所以在实现训练的时候，你会不断地把 $R(\\tau)$ 的分数记录下来 然后你会不断地去计算 $R(\\tau)$ 的平均值， 你会把这个平均值，当作你的 b 来用。 这样就可以让你在训练的时候， $\\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$ 乘上前面这一项， 是有正有负的，这个是第一个 tip。\n\n\n### Tip 2: Assign Suitable Credit\n\n**第二个 tip：给每一个动作合适的分数(credit)。**\n\n如果我们看下面这个式子的话，\n$$\n\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(R\\left(\\tau^{n}\\right)-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n我们原来会做的事情是，在某一个状态，假设你执行了某一个动作 a，它得到的奖励，它前面乘上的这一项 $R(\\tau^n)-b$。\n\n只要在同一个回合里面，在同一场游戏里面， 所有的状态跟动作的对都会使用同样的奖励项(term)进行加权，这件事情显然是不公平的，因为在同一场游戏里面 也许有些动作是好的，有些动作是不好的。 假设整场游戏的结果是好的， 并不代表这个游戏里面每一个行为都是对的。若是整场游戏结果不好， 但不代表游戏里面的所有行为都是错的。所以我们希望可以给每一个不同的动作前面都乘上不同的权重。每一个动作的不同权重， 它反映了每一个动作到底是好还是不好。 \n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.15.png \':size=450\')\n\n举个例子， 假设这个游戏都很短，只有 3~4 个互动， 在 $s_a$ 执行 $a_1$ 得到 5 分。在 $s_b$ 执行 $a_2$ 得到 0 分。在 $s_c$ 执行 $a_3$ 得到 -2 分。 整场游戏下来，你得到 +3 分，那你得到 +3 分 代表在 $s_b$ 执行动作 $a_2$ 是好的吗？并不见得代表 $s_b$ 执行 $a_2$ 是好的。因为这个正的分数，主要来自于在 $s_a$ 执行了 $a_1$，跟在 $s_b$ 执行 $a_2$ 是没有关系的，也许在 $s_b$ 执行 $a_2$ 反而是不好的， 因为它导致你接下来会进入 $s_c$，执行 $a_3$ 被扣分，所以整场游戏得到的结果是好的， 并不代表每一个行为都是对的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.16.png \':size=450\')\n\n如果按照我们刚才的讲法，整场游戏得到的分数是 3 分，那到时候在训练的时候， 每一个状态跟动作的对，都会被乘上 +3。 在理想的状况下，这个问题，如果你采样够多就可以被解决。因为假设你采样够多，在 $s_b$ 执行 $a_2$ 的这件事情，被采样到很多。就某一场游戏，在 $s_b$ 执行 $a_2$，你会得到 +3 分。 但在另外一场游戏，在 $s_b$ 执行 $a_2$，你却得到了 -7 分，为什么会得到 -7 分呢？ 因为在 $s_b$ 执行 $a_2$ 之前， 你在 $s_a$ 执行 $a_2$ 得到 -5 分，-5 分这件事可能也不是在 $s_b$ 执行 $a_2$ 的错，这两件事情，可能是没有关系的，因为它先发生了，这件事才发生，所以它们是没有关系的。\n\n在 $s_b$ 执行 $a_2$ 可能造成的问题只有会在接下来 -2 分，而跟前面的 -5 分没有关系的。但是假设我们今天采样到这项的次数够多，把所有发生这件事情的情况的分数通通都集合起来， 那可能不是一个问题。但现在的问题就是，我们采样的次数是不够多的。在采样的次数不够多的情况下，你要给每一个状态跟动作对合理的分数，你要让大家知道它合理的贡献。怎么给它一个合理的贡献呢？ \n\n一个做法是计算这个对的奖励的时候，不把整场游戏得到的奖励全部加起来，**只计算从这一个动作执行以后所得到的奖励**。因为这场游戏在执行这个动作之前发生的事情是跟执行这个动作是没有关系的， 所以在执行这个动作之前得到多少奖励都不能算是这个动作的功劳。跟这个动作有关的东西， 只有在执行这个动作以后发生的所有的奖励把它加起来，才是这个动作真正的贡献。所以在这个例子里面，在 $s_b$ 执行 $a_2$ 这件事情，也许它真正会导致你得到的分数应该是 -2 分而不是 +3 分，因为前面的 +5 分 并不是执行 $a_2$ 的功劳。实际上执行 $a_2$ 以后，到游戏结束前， 你只有被扣 2 分而已，所以它应该是 -2。那一样的道理，今天执行 $a_2$ 实际上不应该是扣 7 分，因为前面扣 5 分，跟在 $s_b$ 执行 $a_2$ 是没有关系的。在 $s_b$ 执行 $a_2$，只会让你被扣两分而已，所以也许在 $s_b$ 执行 $a_2$， 你真正会导致的结果只有扣两分而已。如果要把它写成式子的话是什么样子呢？如下式所示：\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.17.png)\n\n本来的权重是整场游戏的奖励的总和，现在改成从某个时间 $t$ 开始，假设这个动作是在 $t$ 这个时间点所执行的，从 $t$ 这个时间点一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的。 \n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.18.png)\n接下来再更进一步，我们把未来的奖励做一个折扣(discount)，由此得到的回报被称为 `Discounted Return(折扣回报)`。为什么要把未来的奖励做一个折扣呢？因为虽然在某一个时间点，执行某一个动作，会影响接下来所有的结果，有可能在某一个时间点执行的动作，接下来得到的奖励都是这个动作的功劳。但在比较真实的情况下， 如果时间拖得越长，影响力就越小。 比如说在第二个时间点执行某一个动作， 那我在第三个时间点得到的奖励可能是在第二个时间点执行某个动作的功劳，但是在 100 个时间点之后又得到奖励，那可能就不是在第二个时间点执行某一个动作得到的功劳。 所以我们实际上在做的时候，你会在 R 前面乘上一个 `discount factor`  $\\gamma$， $\\gamma \\in [0,1] $ ，一般会设个 0.9 或 0.99，\n\n* $\\gamma = 0$ : 只关心即时奖励； \n* $\\gamma = 1$ : 未来奖励等同于即时奖励。\n\n\n如果时间点 $t\'$ 越大，它前面就乘上越多次的 $\\gamma$，就代表说现在在某一个状态 $s_t$， 执行某一个动作 $a_t$ 的时候，它真正的分数是在执行这个动作之后所有奖励的总和，而且你还要乘上 $\\gamma$。\n\n举一个例子， 你就想成说，这是游戏的第 1、2、3、4 回合，假设你在游戏的第二回合的某一个 $s_t$ 执行 $a_t$ 得到 +1 分，在 $s_{t+1}$ 执行 $a_{t+1}$ 得到 +3 分，在 $s_{t+2}$ 执行 $a_{t+2}$ 得到 -5 分，然后第二回合结束。$a_t$ 的分数应该是：\n$$\n1+ \\gamma \\times 3+\\gamma^2 \\times-5\n$$\n实际上就是这么实现的，b 可以是取决于状态(state-dependent)的，事实上 b 它通常是一个网络估计出来的，它是一个网络的输出。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.19.png)\n\n把 $R-b$ 这一项合起来，我们统称为` 优势函数(advantage function)`， 用 `A` 来代表优势函数。优势函数取决于 s 和 a，我们就是要计算的是在某一个状态 s 采取某一个动作 a 的时候，优势函数有多大。\n\n在算优势函数时，你要计算 $\\sum_{t^{\\prime}=t}^{T_{n}} r_{t^{\\prime}}^{n}$ ，你需要有一个互动的结果。你需要有一个模型去跟环境做互动，你才知道接下来得到的奖励会有多少。优势函数 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 的上标是 $\\theta$，$\\theta$ 就是代表说是用 $\\theta$ 这个模型跟环境去做互动，然后你才计算出这一项。从时间 t 开始到游戏结束为止，所有 r 的加和减掉 b，这个就叫优势函数。\n\n优势函数的意义就是，假设我们在某一个状态$s_t$ 执行某一个动作 $a_t$，相较于其他可能的动作，它有多好。它在意的不是一个绝对的好，而是相对的好，即`相对优势(relative advantage)`。因为会减掉一个 b，减掉一个 baseline， 所以这个东西是相对的好，不是绝对的好。 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 通常可以是由一个网络估计出来的，这个网络叫做 critic。 \n\n## REINFORCE: Monte Carlo Policy Gradient\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.20.png)\n\nMC 可以理解为算法完成一个回合之后，再拿这个回合的数据来去 learn 一下，做一次更新。因为我们已经拿到了一整个回合的数据的话，也能够拿到每一个步骤的奖励，我们可以很方便地去计算每个步骤的未来总收益，就是我们的期望，就是我们的回报 $G_t$ 。$G_t$ 是我们的未来总收益，$G_t$ 代表是从这个步骤后面，我能拿到的收益之和是多少。$G_1 $是说我从第一步开始，往后能够拿到多少的收益。$G_2$ 是说从第二步开始，往后一共能够拿到多少的收益。\n\n相比 MC 还是一个回合更新一次这样子的方式，TD 就是每个步骤都更新一下。每走一步，我就更新下，这样的更新频率会更高一点。它拿的是 Q-function 来去近似地表示我的未来总收益 $G_t$。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.21.png)\n\n我们介绍下策略梯度最简单的也是最经典的一个算法 `REINFORCE`。REINFORCE 用的是回合更新的方式。它在代码上的处理上是先拿到每个步骤的奖励，然后计算每个步骤的未来总收益 $G_t$ 是多少，然后拿每个 $G_t$ 代入公式，去优化每一个动作的输出。所以编写代码时会有这样一个函数，输入每个步骤拿到的奖励，把这些奖励转成每一个步骤的未来总收益。因为未来总收益是这样计算的：\n$$\n\\begin{aligned}\nG_{t} &=\\sum_{k=t+1}^{T} \\gamma^{k-t-1} r_{k} \\\\\n&=r_{t+1}+\\gamma G_{t+1}\n\\end{aligned}\n$$\n上一个步骤和下一个步骤的未来总收益可以有这样子的一个关系。所以在代码的计算上，我们就是从后往前推，一步一步地往前推，先算 $G_T$，然后往前推，一直算到 $G_1$ 。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.22.png)\n\nREINFORCE 的伪代码主要看最后四行，先产生一个回合的数据，比如 $(s_1,a_1,G_1),(s_2,a_2,G_2),\\cdots,(s_T,a_T,G_T)$。然后针对每个动作来计算梯度。 在代码上计算时，我们要拿到神经网络的输出。神经网络会输出每个动作对应的概率值，然后我们还可以拿到实际的动作，把它转成 one-hot 向量乘一下，我们可以拿到 $\\ln \\pi(A_t|S_t,\\theta)$  。\n\n> 独热编码(one-hot Encoding)通常用于处理类别间不具有大小关系的特征。 例如血型，一共有4个取值（A型、B型、AB型、O型），独热编码会把血型变成一个4维稀疏向量，A型血表示为（1,0,0,0），B型血表示为（0,1,0,0），AB型会表示为（0,0,1,0），O型血表示为（0,0,0,1）。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.23.png)\n\n* 手写数字识别是一个经典的多分类问题，输入是一张手写数字的图片，经过神经网络输出的是各个类别的一个概率。\n* 目的是希望输出的这个概率的分布尽可能地去贴近真实值的概率分布。\n* 因为真实值只有一个数字 9，你用这个 one-hot 向量的形式去给它编码的话，也可以把这个真实值理解为一个概率分布，9 的概率就是1，其他的概率就是 0。\n\n* 神经的网络输出一开始可能会比较平均，通过不断地迭代，训练优化之后，我会希望 9 输出的概率可以远高于其他数字输出的概率。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.24.png)\n\n如上图所示，就是提高 9 对应的概率，降低其他数字对应的概率，让神经网络输出的概率能够更贴近这个真实值的概率分布。我们可以用`交叉熵`来去表示两个概率分布之间的差距。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.25.png)\n\n我们看一下它的优化流程，就是怎么让这个输出去逼近这个真实值。\n\n* 它的优化流程就是将图片作为输入传给神经网络，神经网络会判断这个图片属于哪一类数字，输出所有数字可能的概率，然后再计算这个交叉熵，就是神经网络的输出 $Y_i$ 和真实的标签值 $Y_i\'$ 之间的距离 $-\\sum Y_{i}^{\\prime} \\cdot \\log \\left(Y_{i}\\right)$。\n* 我们希望尽可能地缩小这两个概率分布之间的差距，计算出来的交叉熵可以作为这个损失函数传给神经网络里面的优化器去优化，去自动去做神经网络的参数更新。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.26.png)\n\n* 类似地，policy gradient 预测每一个状态下面应该要输出的这个行动的概率，就是输入状态 $s_t$，然后输出动作的概率，比如 0.02，0.08，0.09。实际上输出给环境的动作是随机选了一个动作，比如说我选了右这个动作，它的 one-hot 向量就是 0，0，1。\n\n* 我们把神经网络的输出和实际动作带入交叉熵的公式就可以求出输出的概率和实际的动作之间的差距。\n* 但这个实际的动作 $a_t$ 只是我们输出的真实的动作，它并不一定是正确的动作，它不能像手写数字识别一样作为一个正确的标签来去指导神经网络朝着正确的方向去更新，所以我们需要乘以一个奖励回报 $G_t$。这个奖励回报相当于是对这个真实动作 的评价。\n  * 如果 $G_t$ 越大，未来总收益越大，那就说明当前输出的这个真实的动作就越好，这个 loss 就越需要重视。\n  * 如果 $G_t$ 越小，那就说明做这个动作 $a_t$ 并没有那么的好，loss 的权重就要小一点，优化力度就小一点。\n* 通过这个和那个手写输入识别的一个对比，我们就知道为什么 loss 会构造成这个样子。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.27.png)\n\n实际上我们在计算这个 loss 的时候，我们要拿到那个 $\\ln \\pi(A_t|S_t,\\theta)$。我就拿实际执行的这个动作，先取个 one-hot 向量，然后再拿到神经网络预测的动作概率，这两个一相乘，我就可以拿到算法里面的那个  $\\ln \\pi(A_t|S_t,\\theta)$。这个就是我们要构造的 loss。因为我们会拿到整个回合的所有的轨迹，所以我们可以对这一条整条轨迹里面的每个动作都去计算一个 loss。把所有的 loss 加起来之后，我们再扔给 adam 的优化器去自动更新参数就好了。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/4.28.png)\n\n上图是 REINFORCE 的流程图。首先我们需要一个 policy model 来输出动作概率，输出动作概率后，我们 `sample()` 函数去得到一个具体的动作，然后跟环境交互过后，我们可以得到一整个回合的数据。拿到回合数据之后，我再去执行一下 `learn()` 函数，在 `learn()` 函数里面，我就可以拿这些数据去构造损失函数，扔给这个优化器去优化，去更新我的 policy model。\n\n## References\n\n* [Intro to Reinforcement Learning (强化学习纲要）](https://github.com/zhoubolei/introRL)\n* [神经网络与深度学习](https://nndl.github.io/)\n* [百面深度学习](https://book.douban.com/subject/35043939/)\n\n','2021-12-10 12:43:49','2021-12-19 19:38:14'),
	(42,3,'第四章 习题','## 1 Keywords\n\n- **policy（策略）：** 每一个actor中会有对应的策略，这个策略决定了actor的行为。具体来说，Policy 就是给一个外界的输入，然后它会输出 actor 现在应该要执行的行为。**一般地，我们将policy写成 $\\pi$ 。**\n- **Return（回报）：** 一个回合（Episode）或者试验（Trial）所得到的所有的reward的总和，也被人们称为Total reward。**一般地，我们用 $R$ 来表示它。**\n- **Trajectory：** 一个试验中我们将environment 输出的 $s$ 跟 actor 输出的行为 $a$，把这个 $s$ 跟 $a$ 全部串起来形成的集合，我们称为Trajectory，即 $\\text { Trajectory } \\tau=\\left\\{s_{1}, a_{1}, s_{2}, a_{2}, \\cdots, s_{t}, a_{t}\\right\\}$。\n- **Reward function：** 根据在某一个 state 采取的某一个 action 决定说现在这个行为可以得到多少的分数，它是一个 function。也就是给一个 $s_1$，$a_1$，它告诉你得到 $r_1$。给它 $s_2$ ，$a_2$，它告诉你得到 $r_2$。 把所有的 $r$ 都加起来，我们就得到了 $R(\\tau)$ ，代表某一个 trajectory $\\tau$ 的 reward。\n- **Expected reward：** $\\bar{R}_{\\theta}=\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau)=E_{\\tau \\sim p_{\\theta}(\\tau)}[R(\\tau)]$。\n- **Reinforce：** 基于策略梯度的强化学习的经典算法，其采用回合更新的模式。\n\n## 2 Questions\n\n- 如果我们想让机器人自己玩video game, 那么强化学习中三个组成（actor、environment、reward function）部分具体分别是什么？\n\n  答：actor 做的事情就是去操控游戏的摇杆， 比如说向左、向右、开火等操作；environment 就是游戏的主机， 负责控制游戏的画面负责控制说，怪物要怎么移动， 你现在要看到什么画面等等；reward function 就是当你做什么事情，发生什么状况的时候，你可以得到多少分数， 比如说杀一只怪兽得到 20 分等等。\n\n- 在一个process中，一个具体的trajectory $s_1$,$a_1$, $s_2$ , $a_2$ 出现的概率取决于什么？\n\n  答：\n\n  1. 一部分是 **environment 的行为**， environment 的 function 它内部的参数或内部的规则长什么样子。 $p(s_{t+1}|s_t,a_t)$这一项代表的是 environment， environment 这一项通常你是无法控制它的，因为那个是人家写好的，或者已经客观存在的。\n\n  2. 另一部分是 **agent 的行为**，你能控制的是 $p_\\theta(a_t|s_t)$。给定一个 $s_t$， actor 要采取什么样的 $a_t$ 会取决于你 actor 的参数 $\\theta$， 所以这部分是 actor 可以自己控制的。随着 actor 的行为不同，每个同样的 trajectory， 它就会有不同的出现的概率。\n\n- 当我们在计算 maximize expected reward时，应该使用什么方法？\n\n  答： **gradient ascent（梯度上升）**，因为要让它越大越好，所以是 gradient ascent。Gradient ascent 在 update 参数的时候要加。要进行 gradient ascent，我们先要计算 expected reward $\\bar{R}$ 的 gradient 。我们对 $\\bar{R}$ 取一个 gradient，这里面只有 $p_{\\theta}(\\tau)$ 是跟 $\\theta$ 有关，所以 gradient 就放在 $p_{\\theta}(\\tau)$ 这个地方。\n\n- 我们应该如何理解梯度策略的公式呢？\n\n  答：\n  $$\n  \\begin{aligned}\n  E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right] &\\approx \\frac{1}{N} \\sum_{n=1}^{N} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(\\tau^{n}\\right) \\\\\n  &=\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n  \\end{aligned}\n  $$\n   $p_{\\theta}(\\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自于 environment，$p_\\theta(a_t|s_t)$ 是来自于 agent。 $p(s_{t+1}|s_t,a_t)$ 由环境决定从而与 $\\theta$ 无关，因此 $\\nabla \\log p(s_{t+1}|s_t,a_t) =0 $。因此 $\\nabla p_{\\theta}(\\tau)=\n  \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$。 公式的具体推导可见我们的教程。\n\n  具体来说：\n\n  *  假设你在 $s_t$ 执行 $a_t$，最后发现 $\\tau$ 的 reward 是正的， 那你就要增加这一项的概率，即增加在 $s_t$ 执行 $a_t$ 的概率。\n  *  反之，在 $s_t$ 执行 $a_t$ 会导致$\\tau$  的 reward 变成负的， 你就要减少这一项的概率。\n\n- 我们可以使用哪些方法来进行gradient ascent的计算？\n\n  答：用 gradient ascent 来 update 参数，对于原来的参数 $\\theta$ ，可以将原始的 $\\theta$  加上更新的 gradient 这一项，再乘以一个 learning rate，learning rate 其实也是要调的，和神经网络一样，我们可以使用 Adam、RMSProp 等优化器对其进行调整。\n\n- 我们进行基于梯度策略的优化时的小技巧有哪些？\n\n  答：\n\n  1. **Add a baseline：**为了防止所有的reward都大于0，从而导致每一个stage和action的变换，会使得每一项的概率都会上升。所以通常为了解决这个问题，我们把reward 减掉一项叫做 b，这项 b 叫做 baseline。你减掉这项 b 以后，就可以让 $R(\\tau^n)-b$ 这一项， 有正有负。 所以如果得到的 total reward $R(\\tau^n)$ 大于 b 的话，就让它的概率上升。如果这个 total reward 小于 b，就算它是正的，正的很小也是不好的，你就要让这一项的概率下降。 如果$R(\\tau^n)<b$  ， 你就要让这个 state 采取这个 action 的分数下降 。这样也符合常理。但是使用baseline会让本来reward很大的“行为”的reward变小，降低更新速率。\n  2. **Assign suitable credit：** 首先第一层，本来的 weight 是整场游戏的 reward 的总和。那现在改成从某个时间 $t$ 开始，假设这个 action 是在 t 这个时间点所执行的，从 $t$ 这个时间点，一直到游戏结束所有 reward 的总和，才真的代表这个 action 是好的还是不好的；接下来我们再进一步，我们把未来的reward做一个discount，这里我们称由此得到的reward的和为**Discounted Return(折扣回报)** 。\n  3. 综合以上两种tip，我们将其统称为**Advantage function**， 用 `A` 来代表 advantage function。Advantage function 是 dependent on s and a，我们就是要计算的是在某一个 state s 采取某一个 action a 的时候，advantage function 有多大。\n  4. Advantage function 的意义就是，假设我们在某一个 state $s_t$ 执行某一个 action $a_t$，相较于其他可能的 action，它有多好。它在意的不是一个绝对的好，而是相对的好，即相对优势(relative advantage)。因为会减掉一个 b，减掉一个 baseline， 所以这个东西是相对的好，不是绝对的好。 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 通常可以是由一个 network estimate 出来的，这个 network 叫做 critic。\n  \n- 对于梯度策略的两种方法，蒙特卡洛（MC）强化学习和时序差分（TD）强化学习两个方法有什么联系和区别？\n\n  答：\n\n  1. **两者的更新频率不同**，蒙特卡洛强化学习方法是**每一个episode更新一次**，即需要经历完整的状态序列后再更新（比如我们的贪吃蛇游戏，贪吃蛇“死了”游戏结束后再更新），而对于时序差分强化学习方法是**每一个step就更新一次** ，（比如我们的贪吃蛇游戏，贪吃蛇每移动一次（或几次）就进行更新）。相对来说，时序差分强化学习方法比蒙特卡洛强化学习方法更新的频率更快。\n  2. 时序差分强化学习能够在知道一个小step后就进行学习，相比于蒙特卡洛强化学习，其更加**快速、灵活**。\n  3. 具体举例来说：假如我们要优化开车去公司的通勤时间。对于此问题，每一次通勤，我们将会到达不同的路口。对于时序差分（TD）强化学习，其会对于每一个经过的路口都会计算时间，例如在路口 A 就开始更新预计到达路口 B、路口 C $\\cdots \\cdots$, 以及到达公司的时间；而对于蒙特卡洛（MC）强化学习，其不会每经过一个路口就更新时间，而是到达最终的目的地后，再修改每一个路口和公司对应的时间。\n\n- 请详细描述REINFORCE的计算过程。\n\n  答：首先我们需要根据一个确定好的policy model来输出每一个可能的action的概率，对于所有的action的概率，我们使用sample方法（或者是随机的方法）去选择一个action与环境进行交互，同时环境就会给我们反馈一整个episode数据。对于此episode数据输入到learn函数中，并根据episode数据进行loss function的构造，通过adam等优化器的优化，再来更新我们的policy model。\n\n\n## 3 Something About Interview\n\n- 高冷的面试官：同学来吧，给我手工推导一下策略梯度公式的计算过程。\n\n  答：首先我们目的是最大化reward函数，即调整 $\\theta$ ，使得期望回报最大，可以用公式表示如下\n  $$\n  J(\\theta)=E_{\\tau \\sim p_{\\theta(\\mathcal{T})}}[\\sum_tr(s_t,a_t)]\n  $$\n  对于上面的式子， $\\tau$ 表示从从开始到结束的一条完整路径。通常，对于最大化问题，我们可以使用梯度上升算法来找到最大值，即\n  $$\n  \\theta^* = \\theta + \\alpha\\nabla J({\\theta})\n  $$\n  所以我们仅仅需要计算（更新）$\\nabla J({\\theta})$  ，也就是计算回报函数 $J({\\theta})$ 关于 $\\theta$ 的梯度，也就是策略梯度，计算方法如下：\n  $$\\begin{aligned}\n  \\nabla_{\\theta}J(\\theta) &= \\int {\\nabla}_{\\theta}p_{\\theta}(\\tau)r(\\tau)d_{\\tau} \\\\\n  &= \\int p_{\\theta}{\\nabla}_{\\theta}logp_{\\theta}(\\tau)r(\\tau)d_{\\tau} \\\\\n  &= E_{\\tau \\sim p_{\\theta}(\\tau)}[{\\nabla}_{\\theta}logp_{\\theta}(\\tau)r(\\tau)]\n  \\end{aligned}$$\n  接着我们继续讲上式展开，对于 $p_{\\theta}(\\tau)$ ，即 $p_{\\theta}(\\tau|{\\theta})$ :\n  $$\n  p_{\\theta}(\\tau|{\\theta}) = p(s_1)\\prod_{t=1}^T \\pi_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)\n  $$\n  取对数后为：\n  $$\n  logp_{\\theta}(\\tau|{\\theta}) = logp(s_1)+\\sum_{t=1}^T log\\pi_{\\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)\n  $$\n  继续求导：\n  $$\n  \\nabla logp_{\\theta}(\\tau|{\\theta}) = \\sum_{t=1}^T \\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)\n  $$\n  带入第三个式子，可以将其化简为：\n  $$\\begin{aligned}\n  \\nabla_{\\theta}J(\\theta) &= E_{\\tau \\sim p_{\\theta}(\\tau)}[{\\nabla}_{\\theta}logp_{\\theta}(\\tau)r(\\tau)] \\\\\n  &= E_{\\tau \\sim p_{\\theta}}[(\\nabla_{\\theta}log\\pi_{\\theta}(a_t|s_t))(\\sum_{t=1}^Tr(s_t,a_t))] \\\\ \n  &= \\frac{1}{N}\\sum_{i=1}^N[(\\sum_{t=1}^T\\nabla_{\\theta}log \\pi_{\\theta}(a_{i,t}|s_{i,t}))(\\sum_{t=1}^Nr(s_{i,t},a_{i,t}))]\n  \\end{aligned}$$\n  \n- 高冷的面试官：可以说一下你了解到的基于梯度策略的优化时的小技巧吗？\n\n  答：\n\n  1. **Add a baseline：**为了防止所有的reward都大于0，从而导致每一个stage和action的变换，会使得每一项的概率都会上升。所以通常为了解决这个问题，我们把reward 减掉一项叫做 b，这项 b 叫做 baseline。你减掉这项 b 以后，就可以让 $R(\\tau^n)-b$ 这一项， 有正有负。 所以如果得到的 total reward $R(\\tau^n)$ 大于 b 的话，就让它的概率上升。如果这个 total reward 小于 b，就算它是正的，正的很小也是不好的，你就要让这一项的概率下降。 如果$R(\\tau^n)<b$  ， 你就要让这个 state 采取这个 action 的分数下降 。这样也符合常理。但是使用baseline会让本来reward很大的“行为”的reward变小，降低更新速率。\n  2. **Assign suitable credit：** 首先第一层，本来的 weight 是整场游戏的 reward 的总和。那现在改成从某个时间 $t$ 开始，假设这个 action 是在 t 这个时间点所执行的，从 $t$ 这个时间点，一直到游戏结束所有 reward 的总和，才真的代表这个 action 是好的还是不好的；接下来我们再进一步，我们把未来的reward做一个discount，这里我们称由此得到的reward的和为**Discounted Return(折扣回报)** 。\n  3. 综合以上两种tip，我们将其统称为**Advantage function**， 用 `A` 来代表 advantage function。Advantage function 是 dependent on s and a，我们就是要计算的是在某一个 state s 采取某一个 action a 的时候，advantage function 有多大。\n\n\n\n','2021-12-10 12:43:49','2021-12-19 19:38:31'),
	(43,3,'第五章 近端策略优化（PPO）算法','## From On-policy to Off-policy\n在讲 PPO 之前，我们先回顾下 on-policy 和 off-policy 这两种训练方法的区别。\n在强化学习里面，我们要学习的就是一个 agent。\n\n* 如果要学习的 agent 跟和环境互动的 agent 是同一个的话， 这个叫做`on-policy(同策略)`。 \n* 如果要学习的 agent 跟和环境互动的 agent 不是同一个的话， 那这个叫做`off-policy(异策略)`。\n\n比较拟人化的讲法是如果要学习的那个 agent，一边跟环境互动，一边做学习这个叫 on-policy。 如果它在旁边看别人玩，通过看别人玩来学习的话，这个叫做 off-policy。\n\n为什么我们会想要考虑 off-policy ？让我们来想想 policy gradient。Policy gradient 是 on-policy 的做法，因为在做 policy gradient 时，我们需要有一个 agent、一个 policy 和一个 actor。这个 actor 先去跟环境互动去搜集资料，搜集很多的 $\\tau$，根据它搜集到的资料按照 policy gradient 的式子去更新 policy 的参数。所以 policy gradient 是一个 on-policy 的算法。\n\n`近端策略优化(Proximal Policy Optimization，简称 PPO)` 是 policy gradient 的一个变形，它是现在 OpenAI 默认的强化学习算法。\n$$\n\\nabla \\bar{R}_{\\theta}=E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla \\log p_{\\theta}(\\tau)\\right]\n$$\n\n问题是上面这个更新的式子中的 $E_{\\tau \\sim p_{\\theta}(\\tau)}$  应该是你现在的 policy $\\pi_{\\theta}$ 所采样出来的轨迹 $\\tau$ 做期望(expectation)。一旦更新了参数，从 $\\theta$ 变成 $\\theta\'$ ，$p_\\theta(\\tau)$这个概率就不对了，之前采样出来的数据就变的不能用了。所以 policy gradient 是一个会花很多时间来采样数据的算法，大多数时间都在采样数据，agent 去跟环境做互动以后，接下来就要更新参数。你只能更新参数一次。接下来你就要重新再去收集数据， 然后才能再次更新参数。\n\n这显然是非常花时间的，所以我们想要从 on-policy 变成 off-policy。 这样做就可以用另外一个 policy， 另外一个 actor $\\theta\'$  去跟环境做互动($\\theta\'$ 被固定了)。用 $\\theta\'$ 收集到的数据去训练 $\\theta$。假设我们可以用 $\\theta\'$ 收集到的数据去训练 $\\theta$，意味着说我们可以把 $\\theta\'$ 收集到的数据用非常多次，我们可以执行梯度上升(gradient ascent)好几次，我们可以更新参数好几次， 都只要用同一笔数据就好了。因为假设 $\\theta$ 有能力学习另外一个 actor $\\theta\'$ 所采样出来的数据的话， 那 $\\theta\'$  就只要采样一次，也许采样多一点的数据， 让 $\\theta$ 去更新很多次，这样就会比较有效率。\n\n### Importance Sampling\n\n具体怎么做呢？这边就需要介绍 `重要性采样(Importance Sampling，IS)` 的概念。\n\n> 对于ー个随机变量，通常用概率密度函数来刻画该变量的概率分布特性。具体来说，给定随机变量的一个取值，可以根据概率密度函数来计算该值对应的概率（密度）。反过来，也可以根据概率密度函数提供的概率分布信息来生成随机变量的一个取值，这就是采样。因此，从某种意义上来说，采样是概率密度函数的逆向应用。与根据概率密度函数计算样本点对应的概率值不同，采样过程往往没有那么直接，通常需要根据待采样分布的具体特点来选择合适的采样策略。\n\n假设你有一个函数 $f(x)$，你要计算从 p 这个分布采样 $x$，再把 $x$ 带到 $f$ 里面，得到 $f(x)$。你要该怎么计算这个 $f(x)$ 的期望值？假设你不能对 p 这个分布做积分的话，那你可以从 p 这个分布去采样一些数据 $x^i$。把 $x^i$ 代到 $f(x)$ 里面，然后取它的平均值，就可以近似 $f(x)$ 的期望值。\n\n现在有另外一个问题，我们没有办法从 p 这个分布里面采样数据。假设我们不能从 p 采样数据，只能从另外一个分布 q 去采样数据，q  可以是任何分布。我们不能够从 p 去采样数据，但可以从 q 去采样 $x$。我们从 q 去采样 $x^i$ 的话就不能直接套下面的式子：\n$$\nE_{x \\sim p}[f(x)] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x^i)\n$$\n因为上式是假设你的 $x$ 都是从 p 采样出来的。\n\n所以做一个修正，修正是这样子的。期望值 $E_{x \\sim p}[f(x)]$ 其实就是 $\\int f(x) p(x) dx$，我们对其做如下的变换：\n$$\n\\int f(x) p(x) d x=\\int f(x) \\frac{p(x)}{q(x)} q(x) d x=E_{x \\sim q}[f(x){\\frac{p(x)}{q(x)}}]\n$$\n我们就可以写成对 q 里面所采样出来的 $x$ 取期望值。我们从 q 里面采样 $x$，然后再去计算 $f(x) \\frac{p(x)}{q(x)}$，再去取期望值。所以就算我们不能从 p 里面去采样数据，只要能够从 q 里面去采样数据，然后代入上式，你就可以计算从 p 这个分布采样 $x$ 代入 $f$ 以后所算出来的期望值。\n\n这边是从 q 做采样，所以从 q 里采样出来的每一笔数据，你需要乘上一个`重要性权重(importance weight)` $\\frac{p(x)}{q(x)}$ 来修正这两个分布的差异。$q(x)$ 可以是任何分布，唯一的限制情况就是 $q(x)$ 的概率是 0 的时候，$p(x)$ 的概率不为 0，这样会没有定义。假设  $q(x)$ 的概率是 0 的时候，$p(x)$ 的概率也都是 0 的话，那这样 $p(x)$ 除以 $q(x)$是有定义的。所以这个时候你就可以使用重要性采样这个技巧。你就可以从 p 做采样换成从 q 做采样。\n\n**重要性采样有一些问题。**虽然理论上你可以把 p 换成任何的 q。但是在实现上，p 和 q 不能差太多。差太多的话，会有一些问题。什么样的问题呢？\n$$\nE_{x \\sim p}[f(x)]=E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]\n$$\n虽然上式成立（上式左边是 $f(x)$ 的期望值，它的分布是 p，上式右边是 $f(x) \\frac{p(x)}{q(x)}$ 的期望值，它的分布是 q），但如果不是算期望值，而是算方差的话，$\\operatorname{Var}_{x \\sim p}[f(x)]$ 和 $\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 是不一样的。两个随机变量的平均值一样，并不代表它的方差一样。\n\n我们可以代一下方差的公式 $\\operatorname{Var}[X]=E\\left[X^{2}\\right]-(E[X])^{2}$，然后得到下式：\n$$\n\\operatorname{Var}_{x \\sim p}[f(x)]=E_{x \\sim p}\\left[f(x)^{2}\\right]-\\left(E_{x \\sim p}[f(x)]\\right)^{2}\n$$\n\n$$\n\\begin{aligned}\n\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right] &=E_{x \\sim q}\\left[\\left(f(x) \\frac{p(x)}{q(x)}\\right)^{2}\\right]-\\left(E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]\\right)^{2} \\\\\n&=E_{x \\sim p}\\left[f(x)^{2} \\frac{p(x)}{q(x)}\\right]-\\left(E_{x \\sim p}[f(x)]\\right)^{2}\n\\end{aligned}\n$$\n\n$\\operatorname{Var}_{x \\sim p}[f(x)]$ 和 $\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 的差别在第一项是不同的， $\\operatorname{Var}_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 的第一项多乘了$\\frac{p(x)}{q(x)}$，如果 $\\frac{p(x)}{q(x)}$ 差距很大的话，$f(x)\\frac{p(x)}{q(x)}$ 的方差就会很大。所以理论上它们的期望值一样，也就是说，你只要对 p 这个分布采样够多次，q 这个分布采样够多，你得到的结果会是一样的。但是如果你采样的次数不够多，因为它们的方差差距是很大的，所以你就有可能得到非常大的差别。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.4.png)\n\n举个例子，当 $p(x)$ 和 $q(x)$ 差距很大的时候，会发生什么样的问题。\n\n假设蓝线是 $p(x)$  的分布，绿线是 $q(x)$  的分布，红线是 $f(x)$。如果我们要计算 $f(x)$的期望值，从 $p(x)$  这个分布做采样的话，那显然 $E_{x \\sim p}[f(x)]$ 是负的，因为左边那块区域 $p(x)$ 的概率很高，所以要采样的话，都会采样到这个地方，而 $f(x)$ 在这个区域是负的， 所以理论上这一项算出来会是负。\n\n接下来我们改成从 $q(x)$ 这边做采样，因为 $q(x)$ 在右边这边的概率比较高，所以如果你采样的点不够的话，那你可能都只采样到右侧。如果你都只采样到右侧的话，你会发现说，算 $E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$这一项，搞不好还应该是正的。你这边采样到这些点，然后你去计算它们的 $f(x) \\frac{p(x)}{q(x)}$ 都是正的。你采样到这些点都是正的。 你取期望值以后也都是正的，这是因为你采样的次数不够多。假设你采样次数很少，你只能采样到右边这边。左边虽然概率很低，但也不是没有可能被采样到。假设你今天好不容易采样到左边的点，因为左边的点，$p(x)$ 和 $q(x)$ 是差很多的， 这边 $p(x)$ 很大，$q(x)$ 很小。今天 $f(x)$ 好不容易终于采样到一个负的，这个负的就会被乘上一个非常大的权重，这样就可以平衡掉刚才那边一直采样到正的值的情况。最终你算出这一项的期望值，终究还是负的。但前提是你要采样够多次，这件事情才会发生。**但有可能采样次数不够多，$E_{x \\sim p}[f(x)]$ 跟 $E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 就有可能有很大的差距。这就是重要性采样的问题。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.5.png)\n\n现在要做的事情就是把重要性采样用在 off-policy 的情况，把 on-policy 训练的算法改成 off-policy 训练的算法。\n\n怎么改呢，之前我们是拿 $\\theta$ 这个 policy 去跟环境做互动，采样出轨迹 $\\tau$，然后计算 $R(\\tau) \\nabla \\log p_{\\theta}(\\tau)$。现在我们不用 $\\theta$ 去跟环境做互动，假设有另外一个 policy  $\\theta\'$，它就是另外一个 actor。它的工作是去做示范(demonstration)。$\\theta\'$ 的工作是要去示范给 $\\theta$ 看。它去跟环境做互动，告诉 $\\theta$ 说，它跟环境做互动会发生什么事，借此来训练 $\\theta$。我们要训练的是 $\\theta$ ，$\\theta\'$  只是负责做示范，跟环境做互动。\n\n我们现在的 $\\tau$ 是从 $\\theta\'$ 采样出来的，是拿 $\\theta\'$ 去跟环境做互动。所以采样出来的 $\\tau$ 是从 $\\theta\'$ 采样出来的，这两个分布不一样。但没有关系，假设你本来是从 p 做采样，但你发现你不能从 p 做采样，所以我们不拿 $\\theta$ 去跟环境做互动。你可以把 p 换 q，然后在后面补上一个重要性权重。现在的状况就是一样，把 $\\theta$ 换成 $\\theta\'$ 后，要补上一个重要性权重 $\\frac{p_{\\theta}(\\tau)}{p_{\\theta^{\\prime}}(\\tau)}$。这个重要性权重就是某一个轨迹 $\\tau$ 用 $\\theta$ 算出来的概率除以这个轨迹 $\\tau$ 用 $\\theta\'$ 算出来的概率。这一项是很重要的，因为你要学习的是 actor $\\theta$ 和 $\\theta\'$ 是不太一样的，$\\theta\'$ 会见到的情形跟 $\\theta$ 见到的情形不见得是一样的，所以中间要做一个修正的项。\n\nQ: 现在的数据是从 $\\theta\'$ 采样出来的，从 $\\theta$ 换成 $\\theta\'$ 有什么好处？\n\nA: 因为现在跟环境做互动是 $\\theta\'$ 而不是 $\\theta$。所以采样出来的东西跟 $\\theta$ 本身是没有关系的。所以你就可以让 $\\theta\'$ 做互动采样一大堆的数据，$\\theta$ 可以更新参数很多次，一直到 $\\theta$ 训练到一定的程度，更新很多次以后，$\\theta\'$ 再重新去做采样，这就是 on-policy 换成 off-policy 的妙用。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.6.png)\n\n实际在做 policy gradient 的时候，我们并不是给整个轨迹 $\\tau$ 都一样的分数，而是每一个状态-动作的对(pair)会分开来计算。实际上更新梯度的时候，如下式所示。\n$$\n=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta}}\\left[A^{\\theta}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]\n$$\n\n我们用 $\\theta$ 这个 actor 去采样出 $s_t$ 跟 $a_t$，采样出状态跟动作的对，我们会计算这个状态跟动作对的 advantage $A^{\\theta}\\left(s_{t}, a_{t}\\right)$， 就是它有多好。\n\n$A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 就是累积奖励减掉 bias，这一项就是估测出来的。它要估测的是，在状态 $s_t$ 采取动作 $a_t$ 是好的还是不好的。接下来后面会乘上 $\\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)$，也就是说如果 $A^{\\theta}\\left(s_{t}, a_{t}\\right)$ 是正的，就要增加概率， 如果是负的，就要减少概率。\n\n我们通过重要性采样把 on-policy 变成 off-policy，从 $\\theta$ 变成 $\\theta\'$。所以现在 $s_t$、$a_t$ 是 $\\theta\'$ 跟环境互动以后所采样到的数据。 但是拿来训练要调整参数是模型 $\\theta$。因为 $\\theta\'$  跟 $\\theta$ 是不同的模型，所以你要做一个修正的项。这项修正的项，就是用重要性采样的技术，把 $s_t$、$a_t$ 用 $\\theta$ 采样出来的概率除掉 $s_t$、$a_t$  用 $\\theta\'$  采样出来的概率。\n\n$$\n=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(s_{t}, a_{t}\\right)}{p_{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)} A^{\\theta}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]\n$$\n\n$A^{\\theta}(s_t,a_t)$ 有一个上标 $\\theta$，$\\theta$  代表说这个是 actor $\\theta$ 跟环境互动的时候所计算出来的 A。但是实际上从 $\\theta$ 换到 $\\theta\'$  的时候，$A^{\\theta}(s_t,a_t)$ 应该改成 $A^{\\theta\'}(s_t,a_t)$，为什么？A 这一项是想要估测说现在在某一个状态采取某一个动作，接下来会得到累积奖励的值减掉 baseline 。你怎么估 A 这一项，你就会看在状态 $s_t$，采取动作 $a_t$，接下来会得到的奖励的总和，再减掉 baseline。之前是 $\\theta$ 在跟环境做互动，所以你观察到的是 $\\theta$ 可以得到的奖励。但现在是 $\\theta\'$  在跟环境做互动，所以你得到的这个 advantage， 其实是根据 $\\theta\'$  所估计出来的 advantage。但我们现在先不要管那么多，我们就假设这两项可能是差不多的。\n\n接下来，我们可以拆解 $p_{\\theta}\\left(s_{t}, a_{t}\\right)$ 和 $p_{\\theta\'}\\left(s_{t}, a_{t}\\right)$，即\n$$\n\\begin{aligned}\np_{\\theta}\\left(s_{t}, a_{t}\\right)&=p_{\\theta}\\left(a_{t}|s_{t}\\right) p_{\\theta}(s_t) \\\\\np_{\\theta\'}\\left(s_{t}, a_{t}\\right)&=p_{\\theta\'}\\left(a_{t}|s_{t}\\right) p_{\\theta\'}(s_t) \n\\end{aligned}\n$$\n于是我们得到下式：\n$$\n=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} \\frac{p_{\\theta}\\left(s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]\n$$\n\n\n这边需要做一件事情是，假设模型是 $\\theta$ 的时候，你看到 $s_t$ 的概率，跟模型是 $\\theta\'$  的时候，你看到 $s_t$ 的概率是差不多的，即 $p_{\\theta}(s_t)=p_{\\theta\'}(s_t)$。因为它们是一样的，所以你可以把它删掉，即\n$$\n=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} | s_{t}^{n}\\right)\\right]  \\tag{1}\n$$\n\nQ: 为什么可以假设 $p_{\\theta}(s_t)$ 和 $p_{\\theta\'}(s_t)$ 是差不多的？\n\nA: 因为你会看到什么状态往往跟你会采取什么样的动作是没有太大的关系的。比如说你玩不同的 Atari 的游戏，其实你看到的游戏画面都是差不多的，所以也许不同的 $\\theta$  对 $s_t$ 是没有影响的。但更直觉的理由就是 $p_{\\theta}(s_t)$ 很难算，想想看这项要怎么算，这一项你还要说我有一个参数 $\\theta$，然后拿 $\\theta$ 去跟环境做互动，算 $s_t$ 出现的概率，这个你很难算。尤其如果输入是图片的话， 同样的 $s_t$ 根本就不会出现第二次。你根本没有办法估这一项， 所以干脆就无视这个问题。\n\n但是 $p_{\\theta}(a_t|s_t)$很好算。你手上有 $\\theta$ 这个参数，它就是个网络。你就把 $s_t$ 带进去，$s_t$ 就是游戏画面，你把游戏画面带进去，它就会告诉你某一个状态的 $a_t$ 概率是多少。我们有个 policy 的网络，把 $s_t$ 带进去，它会告诉我们每一个 $a_t$ 的概率是多少。所以 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$ 这一项，你只要知道 $\\theta$ 和 $\\theta\'$ 的参数就可以算。\n\n现在我们得到一个新的目标函数。\n\n$$\nJ^{\\theta^{\\prime}}(\\theta)=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right]\n$$\n\n\n式(1)是梯度，其实我们可以从梯度去反推原来的目标函数，我们可以用如下的公式来反推目标函数：\n\n$$\n\\nabla f(x)=f(x) \\nabla \\log f(x)\n$$\n\n要注意一点，对 $\\theta$ 求梯度时，$p_{\\theta^{\\prime}}(a_{t} | s_{t})$ 和 $A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)$ 都是常数。\n\n\n所以实际上，当我们使用重要性采样的时候，要去优化的那一个目标函数就长这样子，我们把它写作 $J^{\\theta^{\\prime}}(\\theta)$。为什么写成 $J^{\\theta^{\\prime}}(\\theta)$ 呢，这个括号里面那个 $\\theta$ 代表我们要去优化的那个参数。$\\theta\'$  是说我们拿 $\\theta\'$  去做示范，就是现在真正在跟环境互动的是 $\\theta\'$。因为 $\\theta$ 不跟环境做互动，是 $\\theta\'$ 在跟环境互动。\n\n然后你用 $\\theta\'$  去跟环境做互动，采样出 $s_t$、$a_t$ 以后，你要去计算 $s_t$ 跟 $a_t$ 的 advantage，然后你再去把它乘上 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$。$\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)}$ 是好算的，$A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)$ 可以从这个采样的结果里面去估测出来的，所以 $J^{\\theta^{\\prime}}(\\theta)$ 是可以算的。实际上在更新参数的时候，就是按照式(1) 来更新参数。\n\n## PPO\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.7.png)\n\n我们可以通过重要性采样把 on-policy 换成 off-policy，但重要性采样有一个问题：如果 $p_{\\theta}\\left(a_{t} | s_{t}\\right)$ 跟 $p_{\\theta\'}\\left(a_{t} | s_{t}\\right)$ 这两个分布差太多的话，重要性采样的结果就会不好。怎么避免它差太多呢？这个就是 `Proximal Policy Optimization (PPO) ` 在做的事情。**注意，由于在 PPO 中 $\\theta\'$ 是 $\\theta_{\\text{old}}$，即 behavior policy 也是 $\\theta$，所以 PPO 是 on-policy 的算法**。 \n\nPPO 实际上做的事情就是这样，在 off-policy 的方法里要优化的是 $J^{\\theta^{\\prime}}(\\theta)$。但是这个目标函数又牵涉到重要性采样。在做重要性采样的时候，$p_{\\theta}\\left(a_{t} | s_{t}\\right)$ 不能跟 $p_{\\theta\'}\\left(a_{t} | s_{t}\\right)$差太多。你做示范的模型不能够跟真正的模型差太多，差太多的话，重要性采样的结果就会不好。我们在训练的时候，多加一个约束(constrain)。这个约束是 $\\theta$  跟 $\\theta\'$ 输出的动作的 KL 散度(KL divergence)，简单来说，这一项的意思就是要衡量说 $\\theta$ 跟 $\\theta\'$ 有多像。\n\n然后我们希望在训练的过程中，学习出来的 $\\theta$ 跟 $\\theta\'$  越像越好。因为如果 $\\theta$ 跟 $\\theta\'$ 不像的话，最后的结果就会不好。所以在 PPO 里面有两个式子，一方面是优化本来要优化的东西，但再加一个约束。这个约束就好像正则化(regularization) 的项(term) 一样，在做机器学习的时候不是有 L1/L2 的正则化。这一项也很像正则化，这样正则化做的事情就是希望最后学习出来的 $\\theta$ 不要跟 $\\theta\'$ 太不一样。\n\nPPO 有一个前身叫做`信任区域策略优化(Trust Region Policy Optimization，TRPO)`，TRPO 的式子如下式所示：\n\n$$\n\\begin{aligned}\nJ_{T R P O}^{\\theta^{\\prime}}(\\theta)=E_{\\left(s_{t}, a_{t}\\right) \\sim \\pi_{\\theta^{\\prime}}}\\left[\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{\\prime}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{\\prime}}\\left(s_{t}, a_{t}\\right)\\right] \\\\ \\\\\n \\mathrm{KL}\\left(\\theta, \\theta^{\\prime}\\right)<\\delta \n\\end{aligned}\n$$\n\n它与 PPO 不一样的地方是约束摆的位置不一样，PPO 是直接把约束放到你要优化的那个式子里面，然后你就可以用梯度上升的方法去最大化这个式子。但 TRPO 是把 KL 散度当作约束，它希望 $\\theta$ 跟 $\\theta\'$ 的 KL 散度小于一个 $\\delta$。如果你使用的是基于梯度的优化时，有约束是很难处理的。\n\nTRPO 是很难处理的，因为它把 KL 散度约束当做一个额外的约束，没有放目标(objective)里面，所以它很难算，所以一般就用 PPO 而不是 TRPO。看文献上的结果是，PPO 跟 TRPO 性能差不多，但 PPO 在实现上比 TRPO 容易的多。\n\nQ: KL 散度到底指的是什么？\n\nA: \n\n这边我是直接把 KL 散度当做一个函数，输入是 $\\theta$ 跟 $\\theta\'$，但我的意思并不是说把 $\\theta$ 或 $\\theta\'$  当做一个分布，算这两个分布之间的距离。所谓的 $\\theta$ 跟 $\\theta\'$  的距离并不是参数上的距离，而是行为(behavior)上的距离。\n\n假设你有两个 actor，它们的参数分别为 $\\theta$ 和 $\\theta\'$，所谓参数上的距离就是你算这两组参数有多像。这里讲的不是参数上的距离， 而是它们行为上的距离。你先代进去一个状态 s，它会对这个动作的空间输出一个分布。假设你有 3 个动作，3 个可能的动作就输出 3 个值。今天所指的距离是行为距离(behavior distance)，也就是说，给定同样的状态，输出动作之间的差距。这两个动作的分布都是一个概率分布，所以就可以计算这两个概率分布的 KL 散度。把不同的状态输出的这两个分布的 KL 散度平均起来才是我这边所指的两个 actor 间的 KL 散度。\n\nQ: 为什么不直接算 $\\theta$ 和 $\\theta\'$ 之间的距离？算这个距离的话，甚至不要用 KL 散度算，L1 跟 L2 的范数(norm)也可以保证 $\\theta$ 跟 $\\theta\'$ 很接近。\n\nA: 在做强化学习的时候，之所以我们考虑的不是参数上的距离，而是动作上的距离，是因为很有可能对 actor 来说，参数的变化跟动作的变化不一定是完全一致的。有时候你参数小小变了一下，它可能输出的行为就差很多。或者是参数变很多，但输出的行为可能没什么改变。**所以我们真正在意的是这个 actor 的行为上的差距，而不是它们参数上的差距。**所以在做 PPO 的时候，所谓的 KL 散度并不是参数的距离，而是动作的距离。\n\n### PPO-Penalty\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.8.png)\n\n**PPO 算法有两个主要的变种：PPO-Penalty 和 PPO-Clip。**\n\n我们来看一下 `PPO1` 的算法，即 `PPO-Penalty`。它先初始化一个 policy 的参数 $\\theta^0$。然后在每一个迭代里面，你要用参数 $\\theta^k$，$\\theta^k$ 就是你在前一个训练的迭代得到的 actor 的参数，你用 $\\theta^k$ 去跟环境做互动，采样到一大堆状态-动作的对。\n\n然后你根据 $\\theta^k$ 互动的结果，估测一下 $A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right)$。然后你就使用 PPO 的优化的公式。但跟原来的 policy gradient 不一样，原来的 policy gradient 只能更新一次参数，更新完以后，你就要重新采样数据。但是现在不用，你拿 $\\theta^k$ 去跟环境做互动，采样到这组数据以后，你可以让 $\\theta$ 更新很多次，想办法去最大化目标函数。这边 $\\theta$ 更新很多次没有关系，因为我们已经有做重要性采样，所以这些经验，这些状态-动作的对是从 $\\theta^k$ 采样出来的没有关系。$\\theta$ 可以更新很多次，它跟 $\\theta^k$ 变得不太一样也没有关系，你还是可以照样训练 $\\theta$。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.9.png)\n\n在 PPO 的论文里面还有一个 `adaptive KL divergence`。这边会遇到一个问题就是 $\\beta$  要设多少，它就跟正则化一样。正则化前面也要乘一个权重，所以这个 KL 散度前面也要乘一个权重，但 $\\beta$  要设多少呢？所以有个动态调整 $\\beta$ 的方法。\n\n* 在这个方法里面，你先设一个你可以接受的 KL 散度的最大值。假设优化完这个式子以后，你发现 KL 散度的项太大，那就代表说后面这个惩罚的项没有发挥作用，那就把 $\\beta$ 调大。\n* 另外，你设一个 KL 散度的最小值。如果优化完上面这个式子以后，你发现 KL 散度比最小值还要小，那代表后面这一项的效果太强了，你怕他只弄后面这一项，那 $\\theta$ 跟 $\\theta^k$ 都一样，这不是你要的，所以你要减少 $\\beta$。\n\n所以 $\\beta$ 是可以动态调整的。这个叫做 `adaptive KL penalty`。\n\n### PPO-Clip\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.10.png)\n\n如果你觉得算 KL 散度很复杂，有一个`PPO2`，PPO2 即 `PPO-Clip`。PPO2 要去最大化的目标函数如下式所示，它的式子里面就没有 KL 散度 。\n$$\n\\begin{aligned}\nJ_{P P O 2}^{\\theta^{k}}(\\theta) \\approx \\sum_{\\left(s_{t}, a_{t}\\right)} \\min &\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)} A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right),\\right.\\\\\n&\\left.\\operatorname{clip}\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A^{\\theta^{k}}\\left(s_{t}, a_{t}\\right)\\right)\n\\end{aligned}\n$$\n这个式子看起来有点复杂，但实际实现就很简单。我们来看一下这个式子到底是什么意思。\n\n* Min 这个操作符(operator)做的事情是第一项跟第二项里面选比较小的那个。\n* 第二项前面有个 clip 函数，clip 函数的意思是说，\n  * 在括号里面有三项，如果第一项小于第二项的话，那就输出 $1-\\varepsilon$ 。\n  * 第一项如果大于第三项的话，那就输出 $1+\\varepsilon$。 \n* $\\varepsilon$ 是一个超参数，你要 tune 的，你可以设成 0.1 或 设 0.2 。\n\n假设这边设 0.2 的话，如下式所示\n$$\n\\operatorname{clip}\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}, 0.8, 1.2\\right)\n$$\n\n如果 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 算出来小于 0.8，那就当作 0.8。如果算出来大于 1.2，那就当作1.2。\n\n我们先看看下面这项这个算出来到底是什么东西：\n$$\n\\operatorname{clip}\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right)\n$$\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.11.png \':size=450\')\n\n上图的横轴是 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$，纵轴是 clip 函数的输出。\n\n* 如果 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 大于$1+\\varepsilon$，输出就是 $1+\\varepsilon$。\n* 如果小于 $1-\\varepsilon$， 它输出就是 $1-\\varepsilon$。\n* 如果介于 $1+\\varepsilon$ 跟 $1-\\varepsilon$ 之间， 就是输入等于输出。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.12.png \':size=450\')\n\n*  $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 是绿色的线；\n* $\\operatorname{clip}\\left(\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right)$ 是蓝色的线；\n* 在绿色的线跟蓝色的线中间，我们要取一个最小的。假设前面乘上的这个项 A，它是大于 0 的话，取最小的结果，就是红色的这一条线。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.13.png \':size=450\')\n\n如果 A 小于 0 的话，取最小的以后，就得到红色的这一条线。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.14.png \':size=500\')\n\n虽然这个式子看起来有点复杂，实现起来是蛮简单的，**因为这个式子想要做的事情就是希望 $p_{\\theta}(a_{t} | s_{t})$ 跟 $p_{\\theta^k}(a_{t} | s_{t})$，也就是你拿来做示范的模型跟你实际上学习的模型，在优化以后不要差距太大。**\n\n**怎么让它做到不要差距太大呢？**\n\n* 如果 A > 0，也就是某一个状态-动作的对是好的，那我们希望增加这个状态-动作对的概率。也就是说，我们想要让  $p_{\\theta}(a_{t} | s_{t})$ 越大越好，但它跟 $p_{\\theta^k}(a_{t} | s_{t})$ 的比值不可以超过 $1+\\varepsilon$。如果超过 $1+\\varepsilon$  的话，就没有 benefit 了。红色的线就是我们的目标函数，我们希望目标越大越好，我们希望 $p_{\\theta}(a_{t} | s_{t})$ 越大越好。但是 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 只要大过 $1+\\varepsilon$，就没有 benefit 了。所以今天在训练的时候，当 $p_{\\theta}(a_{t} | s_{t})$ 被训练到 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}>1+\\varepsilon$ 时，它就会停止。假设 $p_{\\theta}(a_{t} | s_{t})$  比 $p_{\\theta^k}(a_{t} | s_{t})$ 还要小，并且这个 advantage 是正的。因为这个动作是好的，我们当然希望这个动作被采取的概率越大越好，我们希望 $p_{\\theta}(a_{t} | s_{t})$ 越大越好。所以假设 $p_{\\theta}(a_{t} | s_{t})$ 还比 $p_{\\theta^k}(a_{t} | s_{t})$  小，那就尽量把它挪大，但只要大到 $1+\\varepsilon$ 就好。\n* 如果 A < 0，也就是某一个状态-动作对是不好的，我们希望把 $p_{\\theta}(a_{t} | s_{t})$ 减小。如果 $p_{\\theta}(a_{t} | s_{t})$ 比 $p_{\\theta^k}(a_{t} | s_{t})$  还大，那你就尽量把它压小，压到 $\\frac{p_{\\theta}\\left(a_{t} | s_{t}\\right)}{p_{\\theta^{k}}\\left(a_{t} | s_{t}\\right)}$ 是 $1-\\epsilon$ 的时候就停了，就不要再压得更小。\n\n这样的好处就是，你不会让 $p_{\\theta}(a_{t} | s_{t})$ 跟 $p_{\\theta^k}(a_{t} | s_{t})$ 差距太大。要实现这个东西，很简单。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/5.15.png)\n上图是 PPO 跟其它方法的比较。Actor-Critic 和 A2C+Trust Region 方法是基于 actor-critic 的方法。PPO 是紫色线的方法，这边每张图就是某一个 RL 的任务，你会发现说在多数的情况(cases)里面，PPO 都是不错的，不是最好的，就是第二好的。\n\n## References\n\n* [OpenAI Spinning Up ](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#)\n* [百面机器学习](https://book.douban.com/subject/30285146/)\n\n\n\n','2021-12-10 12:43:49','2021-12-19 19:38:44'),
	(44,3,'第五章 习题','## 1 Keywords\r\n\r\n- **on-policy(同策略)：** 要learn的agent和环境互动的agent是同一个时，对应的policy。\r\n- **off-policy(异策略)：** 要learn的agent和环境互动的agent不是同一个时，对应的policy。\r\n- **important sampling（重要性采样）：** 使用另外一种数据分布，来逼近所求分布的一种方法，在强化学习中通常和蒙特卡罗方法结合使用，公式如下：$\\int f(x) p(x) d x=\\int f(x) \\frac{p(x)}{q(x)} q(x) d x=E_{x \\sim q}[f(x){\\frac{p(x)}{q(x)}}]=E_{x \\sim p}[f(x)]$  我们在已知 $q$ 的分布后，可以使用上述公式计算出从 $p$ 这个distribution sample x 代入 $f$ 以后所算出来的期望值。\r\n- **Proximal Policy Optimization (PPO)：** 避免在使用important sampling时由于在 $\\theta$ 下的 $p_{\\theta}\\left(a_{t} | s_{t}\\right)$ 跟 在  $\\theta \'$  下的 $p_{\\theta\'}\\left(a_{t} | s_{t}\\right)$ 差太多，导致important sampling结果偏差较大而采取的算法。具体来说就是在training的过程中增加一个constrain，这个constrain对应着 $\\theta$  跟 $\\theta\'$  output 的 action 的 KL divergence，来衡量 $\\theta$  与 $\\theta\'$ 的相似程度。\r\n\r\n## 2 Questions\r\n\r\n- 基于on-policy的policy gradient有什么可改进之处？或者说其效率较低的原因在于？\r\n\r\n  答：\r\n\r\n  - 经典policy gradient的大部分时间花在sample data处，即当我们的agent与环境做了交互后，我们就要进行policy model的更新。但是对于一个回合我们仅能更新policy model一次，更新完后我们就要花时间去重新collect data，然后才能再次进行如上的更新。\r\n\r\n  - 所以我们的可以自然而然地想到，使用off-policy方法使用另一个不同的policy和actor，与环境进行互动并用collect data进行原先的policy的更新。这样等价于使用同一组data，在同一个回合，我们对于整个的policy model更新了多次，这样会更加有效率。\r\n\r\n- 使用important sampling时需要注意的问题有哪些。\r\n\r\n  答：我们可以在important sampling中将 $p$ 替换为任意的 $q$，但是本质上需要要求两者的分布不能差的太多，即使我们补偿了不同数据分布的权重 $\\frac{p(x)}{q(x)}$ 。 $E_{x \\sim p}[f(x)]=E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right]$ 当我们对于两者的采样次数都比较多时，最终的结果时一样的，没有影响的。但是通常我们不会取理想的数量的sample data，所以如果两者的分布相差较大，最后结果的variance差距（平方级）将会很大。\r\n\r\n- 基于off-policy的importance sampling中的 data 是从 $\\theta\'$ sample 出来的，从 $\\theta$ 换成 $\\theta\'$ 有什么优势？\r\n\r\n  答：使用off-policy的importance sampling后，我们不用 $\\theta$ 去跟环境做互动，假设有另外一个 policy  $\\theta\'$，它就是另外一个actor。它的工作是他要去做demonstration，$\\theta\'$ 的工作是要去示范给 $\\theta$ 看。它去跟环境做互动，告诉 $\\theta$ 说，它跟环境做互动会发生什么事。然后，借此来训练$\\theta$。我们要训练的是 $\\theta$ ，$\\theta\'$  只是负责做 demo，负责跟环境做互动，所以 sample 出来的东西跟 $\\theta$ 本身是没有关系的。所以你就可以让 $\\theta\'$ 做互动 sample 一大堆的data，$\\theta$ 可以update 参数很多次。然后一直到 $\\theta$  train 到一定的程度，update 很多次以后，$\\theta\'$ 再重新去做 sample，这就是 on-policy 换成 off-policy 的妙用。\r\n\r\n- 在本节中PPO中的KL divergence指的是什么？\r\n\r\n  答：本质来说，KL divergence是一个function，其度量的是两个action （对应的参数分别为$\\theta$ 和 $\\theta\'$ ）间的行为上的差距，而不是参数上的差距。这里行为上的差距（behavior distance）可以理解为在相同的state的情况下，输出的action的差距（他们的概率分布上的差距），这里的概率分布即为KL divergence。 \r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：请问什么是重要性采样呀？\r\n\r\n  答：使用另外一种数据分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式是：\r\n  $$\\begin{aligned}\r\n  \\int f(x) p(x) d x &= \\int f(x) \\frac{p(x)}{q(x)} q(x) d x \\\\\r\n  &= E_{x \\sim q}[f(x){\\frac{p(x)}{q(x)}}] \\\\\r\n  &= E_{x \\sim p}[f(x)]\r\n  \\end{aligned}$$\r\n   我们在已知 $q$ 的分布后，可以使用上述公式计算出从 $p$ 分布的期望值。也就可以使用 $q$ 来对于 $p$ 进行采样了，即为重要性采样。\r\n\r\n- 高冷的面试官：请问on-policy跟off-policy的区别是什么？\r\n\r\n  答：用一句话概括两者的区别，生成样本的policy（value-funciton）和网络参数更新时的policy（value-funciton）是否相同。具体来说，on-policy：生成样本的policy（value function）跟网络更新参数时使用的policy（value function）相同。SARAS算法就是on-policy的，基于当前的policy直接执行一次action，然后用这个样本更新当前的policy，因此生成样本的policy和学习时的policy相同，算法为on-policy算法。该方法会遭遇探索-利用的矛盾，仅利用目前已知的最优选择，可能学不到最优解，收敛到局部最优，而加入探索又降低了学习效率。epsilon-greedy 算法是这种矛盾下的折衷。优点是直接了当，速度快，劣势是不一定找到最优策略。off-policy：生成样本的policy（value function）跟网络更新参数时使用的policy（value function）不同。例如，Q-learning在计算下一状态的预期收益时使用了max操作，直接选择最优动作，而当前policy并不一定能选择到最优动作，因此这里生成样本的policy和学习时的policy不同，即为off-policy算法。\r\n\r\n- 高冷的面试官：请简述下PPO算法。其与TRPO算法有何关系呢?\r\n\r\n  答：PPO算法的提出：旨在借鉴TRPO算法，使用一阶优化，在采样效率、算法表现，以及实现和调试的复杂度之间取得了新的平衡。这是因为PPO会在每一次迭代中尝试计算新的策略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。具体来说，在避免使用important sampling时由于在 $\\theta$ 下的 $p_{\\theta}\\left(a_{t} | s_{t}\\right)$ 跟 在 $\\theta\'$ 下的 $ p_{\\theta\'}\\left(a_{t} | s_{t}\\right) $ 差太多，导致important sampling结果偏差较大而采取的算法。\r\n','2021-12-10 12:43:49','2021-12-19 19:39:05'),
	(45,3,'第六章 DQN基本概念','## DQN\n传统的强化学习算法会使用表格的形式存储状态值函数 $V(s)$ 或状态动作值函数 $Q(s,a)$，但是这样的方法存在很大的局限性。例如：现实中的强化学习任务所面临的状态空间往往是连续的，存在无穷多个状态，在这种情况下，就不能再使用表格对值函数进行存储。值函数近似利用函数直接拟合状态值函数或状态动作值函数，减少了对存储空间的要求，有效地解决了这个问题。\n\n为了在连续的状态和动作空间中计算值函数 $Q^{\\pi}(s,a)$，我们可以用一个函数 $Q_{\\phi}(\\boldsymbol{s},\\boldsymbol{a})$ 来表示近似计算，称为`价值函数近似(Value Function Approximation)`。\n$$\nQ_{\\phi}(\\boldsymbol{s}, \\boldsymbol{a}) \\approx Q^{\\pi}(s, a)\n$$\n\n其中\n* $\\boldsymbol{s},\\boldsymbol{a}$ 分别是状态 $s$ 和动作 $a$ 的向量表示，\n* 函数 $Q_{\\phi}(\\boldsymbol{s}, \\boldsymbol{a})$ 通常是一个参数为 $\\phi$ 的函数，比如`神经网络`，输出为一个实数，称为`Q 网络(Q-network)`。\n\n## State Value Function\n\n**Q-learning 是 `value-based` 的方法。在 value-based 的方法里面，我们学习的不是策略，我们要学习的是一个 `critic(评论家)`。**评论家要做的事情是评价现在的行为有多好或是有多不好。假设有一个演员(actor) $\\pi$ ，评论家就是来评价这个演员的策略 $\\pi$  好还是不好，即 `Policy Evaluation(策略评估)`。\n\n> 注：「李宏毅深度强化学习」课程提到的 Q-learning，其实是 DQN(Deep Q-network)。\n>\n> DQN 是指基于深度学习的 Q-learning 算法，主要结合了`价值函数近似(Value Function Approximation)`与神经网络技术，并采用了目标网络和经历回放的方法进行网络的训练。\n>\n> 在 Q-learning 中，我们使用表格来存储每个状态 s 下采取动作 a 获得的奖励，即状态-动作值函数 $Q(s,a)$。然而，这种方法在状态量巨大甚至是连续的任务中，会遇到维度灾难问题，往往是不可行的。因此，DQN 采用了价值函数近似的表示方法。\n\n举例来说，有一种评论家叫做 `state value function(状态价值函数)`。状态价值函数的意思就是说，假设演员叫做 $\\pi$，拿 $\\pi$  跟环境去做互动。假设 $\\pi$  看到了某一个状态 s，如果在玩 Atari 游戏的话，状态 s 是某一个画面，看到某一个画面的时候，接下来一直玩到游戏结束，期望的累积奖励有多大。所以 $V^{\\pi}$ 是一个函数，这个函数输入一个状态，然后它会输出一个标量( scalar)。这个标量代表说，$\\pi$ 这个演员看到状态 s 的时候，接下来预期到游戏结束的时候，它可以得到多大的值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.1.png \':size=550\')\n\n举个例子，假设你是玩 space invader 的话，\n\n* 左边这个状态 s，这个游戏画面，$V^{\\pi}(s)$  也许会很大，因为还有很多的怪可以杀， 所以你会得到很大的分数。一直到游戏结束的时候，你仍然有很多的分数可以吃。\n* 右边这种情况得到的 $V^{\\pi}(s)$ 可能就很小，因为剩下的怪也不多了，并且红色的防护罩已经消失了，所以可能很快就会死掉。所以接下来得到预期的奖励，就不会太大。\n\n这边需要强调的一个点是说，评论家都是绑一个演员的，评论家没有办法去凭空去评价一个状态的好坏，它所评价的东西是在给定某一个状态的时候， 假设接下来互动的演员是 $\\pi$，那我会得到多少奖励。因为就算是给同样的状态，你接下来的 $\\pi$ 不一样，你得到的奖励也是不一样的。\n\n举例来说，在左边的情况，假设是一个正常的 $\\pi$，它可以杀很多怪，那假设它是一个很弱的 $\\pi$，它就站在原地不动，然后马上就被射死了，那你得到的 $V^\\pi(s)$ 还是很小。所以评论家的输出值取决于状态和演员。所以评论家其实都要绑一个演员，它是在衡量某一个演员的好坏，而不是衡量一个状态的好坏。这边要强调一下，评论家的输出是跟演员有关的，状态的价值其实取决于你的演员，当演员变的时候，状态价值函数的输出其实也是会跟着改变的。\n\n### State Value Function Estimation\n\n**怎么衡量这个状态价值函数 $V^{\\pi}(s)$ 呢？**有两种不同的做法：MC-based 的方法和 TD-based 的方法。\n\n` Monte-Carlo(MC)-based`的方法就是让演员去跟环境做互动，要看演员好不好，我们就让演员去跟环境做互动，给评论家看。然后，评论家就统计说，\n\n* 演员如果看到状态 $s_a$，接下来的累积奖励会有多大。\n* 如果它看到状态 $s_b$，接下来的累积奖励会有多大。\n\n但是实际上，我们不可能把所有的状态通通都扫过。如果是玩 Atari 游戏的话，状态是图像，你没有办法把所有的状态通通扫过。所以实际上 $V^{\\pi}(s)$ 是一个网络。对一个网络来说，就算输入状态是从来都没有看过的，它也可以想办法估测一个值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.2.png \':size=350\')\n\n怎么训练这个网络呢？因为如果在状态 $s_a$，接下来的累积奖励就是 $G_a$。也就是说，对这个价值函数来说，如果输入是状态 $s_a$，正确的输出应该是 $G_a$。如果输入状态 $s_b$，正确的输出应该是值 $G_b$。**所以在训练的时候， 它就是一个 `回归问题(regression problem)`。**网络的输出就是一个值，你希望在输入 $s_a$ 的时候，输出的值跟 $G_a$ 越近越好，输入 $s_b$ 的时候，输出的值跟 $G_b$ 越近越好。接下来把网络训练下去，就结束了。这是 MC-based 的方法。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.3.png \':size=550\')\n\n**第二个方法是`Temporal-difference(时序差分)` 的方法， `即 TD-based ` 的方法。**\n\n在 MC-based 的方法中，每次我们都要算累积奖励，也就是从某一个状态 $s_a$ 一直玩到游戏结束的时候，得到的所有奖励的总和。所以你要使用 MC-based 的方法，你必须至少把这个游戏玩到结束。但有些游戏非常长，你要玩到游戏结束才能够更新网络，花的时间太长了，因此我们会采用 TD-based 的方法。\n\nTD-based 的方法不需要把游戏玩到底，只要在游戏的某一个情况，某一个状态 $s_t$ 的时候，采取动作 $a_t$ 得到奖励$r_t$ ，跳到状态 $s_{t+1}$，就可以使用 TD 的方法。\n\n怎么使用 TD 的方法呢？这边是基于以下这个式子：\n$$\nV^{\\pi}\\left(s_{t}\\right)=V^{\\pi}\\left(s_{t+1}\\right)+r_{t}\n$$\n\n假设我们现在用的是某一个策略 $\\pi$，在状态 $s_t$，它会采取动作 $a_t$，给我们奖励 $r_t$ ，接下来进入 $s_{t+1}$ 。状态 $s_{t+1}$ 的值跟状态 $s_t$ 的值，它们的中间差了一项 $r_t$。因为你把 $s_{t+1}$ 得到的值加上得到的奖励 $r_t$ 就会等于 $s_t$ 得到的值。有了这个式子以后，你在训练的时候，你并不是直接去估测 V，而是希望你得到的结果 V 可以满足这个式子。\n\n也就是说我们会是这样训练的，我们把 $s_t$ 丢到网络里面，因为 $s_t$ 丢到网络里面会得到 $V^{\\pi}(s_t)$，把 $s_{t+1}$ 丢到你的值网络里面会得到 $V^{\\pi}(s_{t+1})$，这个式子告诉我们，$V^{\\pi}(s_t)$ 减 $V^{\\pi}(s_{t+1})$ 的值应该是 $r_t$。然后希望它们两个相减的 loss 跟 $r_t$ 越接近，训练下去，更新 V 的参数，你就可以把 V 函数学习出来。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.4.png \':size=500\')\n\n**MC 跟 TD 有什么样的差别呢？**\n\n**MC 最大的问题就是方差很大。**因为我们在玩游戏的时候，它本身是有随机性的。所以你可以把 $G_a$ 看成一个随机变量。因为你每次同样走到 $s_a$ 的时候，最后你得到的 $G_a$ 其实是不一样的。你看到同样的状态 $s_a$，最后玩到游戏结束的时候，因为游戏本身是有随机性的，玩游戏的模型搞不好也有随机性，所以你每次得到的 $G_a$ 是不一样的，每一次得到 $G_a$ 的差别其实会很大。为什么它会很大呢？因为 $G_a$ 其实是很多个不同的步骤的奖励的和。假设你每一个步骤都会得到一个奖励，$G_a$ 是从状态 $s_a$ 开始，一直玩到游戏结束，每一个步骤的奖励的和。\n\n举例来说，通过下面式子，我们知道 $G_a$ 的方差相较于某一个状态的奖励，它会是比较大的。\n\n$$\n\\operatorname{Var}[k X]=k^{2} \\operatorname{Var}[X]\n$$\n> Var 是指 variance。 \n\n如果用 TD 的话，你是要去最小化这样的一个式子：\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.5.png \':size=550\')\n\n在这中间会有随机性的是 r。因为计算你在 $s_t$ 采取同一个动作，你得到的奖励也不一定是一样的，所以 r 是一个随机变量。但这个随机变量的方差会比 $G_a$ 还要小，因为 $G_a$ 是很多 r 合起来，这边只是某一个 r  而已。$G_a$ 的方差会比较大，r  的方差会比较小。但是这边你会遇到的**一个问题是你这个 V 不一定估得准**。假设你的这个 V 估得是不准的，那你使用这个式子学习出来的结果，其实也会是不准的。所以 MC 跟 TD 各有优劣。**今天其实 TD 的方法是比较常见的，MC 的方法其实是比较少用的。**\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.6.png \':size=550\')\n\n**上图是讲 TD 跟 MC 的差异。**假设有某一个评论家，它去观察某一个策略 $\\pi$  跟环境互动的 8 个 episode 的结果。有一个演员 $\\pi$  跟环境互动了8 次，得到了8 次玩游戏的结果。接下来这个评论家去估测状态的值。\n\n**我们先计算 $s_b$ 的值。** 状态 $s_b$ 在 8 场游戏里面都有经历过，其中有 6 场得到奖励 1，有 2 场得到奖励 0。所以如果你是要算期望值的话，就算看到状态 $s_b$ 以后得到的奖励，一直到游戏结束的时候得到的累积奖励期望值是 3/4，计算过程如下式所示：\n$$\n\\frac{6 \\times 1 + 2 \\times 0}{8}=\\frac{6}{8}=\\frac{3}{4}\n$$\n**但 $s_a$ 期望的奖励到底应该是多少呢？**这边其实有两个可能的答案：一个是 0，一个是 3/4。为什么有两个可能的答案呢？这取决于你用 MC 还是TD。用 MC 跟用 TD 算出来的结果是不一样的。\n\n假如用 MC 的话，你会发现这个 $s_a$ 就出现一次，看到 $s_a$ 这个状态，接下来累积奖励就是 0，所以 $s_a$ 期望奖励就是 0。\n\n但 TD 在计算的时候，它要更新下面这个式子：\n$$\nV^{\\pi}\\left(s_{a}\\right)=V^{\\pi}\\left(s_{b}\\right)+r\n$$\n\n因为我们在状态 $s_a$ 得到奖励 r=0 以后，跳到状态 $s_b$。所以状态 $s_b$ 的奖励会等于状态 $s_b$ 的奖励加上在状态 $s_a$ 跳到状态 $s_b$ 的时候可能得到的奖励 r。而这个得到的奖励 r 的值是 0，$s_b$ 期望奖励是 3/4，那 $s_a$ 的奖励应该是 3/4。\n\n用 MC 跟 TD 估出来的结果很有可能是不一样的。就算评论家观察到一样的训练数据，它最后估出来的结果也不一定是一样的。为什么会这样呢？你可能问说，哪一个结果比较对呢？其实就都对。\n\n因为在第一个轨迹， $s_a$ 得到奖励 0 以后，再跳到 $s_b$ 也得到奖励 0。这边有两个可能。\n\n* 一个可能是： $s_a$ 是一个标志性的状态，只要看到 $s_a$ 以后，$s_b$ 就会拿不到奖励，$s_a$ 可能影响了 $s_b$。如果是用 MC 的算法的话，它会把 $s_a$ 影响 $s_b$ 这件事考虑进去。所以看到 $s_a$ 以后，接下来 $s_b$ 就得不到奖励，$s_b$ 期望的奖励是 0。\n\n* 另一个可能是：看到 $s_a$ 以后，$s_b$ 的奖励是 0 这件事只是一个巧合，并不是 $s_a$ 所造成，而是因为说 $s_b$ 有时候就是会得到奖励 0，这只是单纯运气的问题。其实平常 $s_b$ 会得到奖励期望值是 3/4，跟 $s_a$ 是完全没有关系的。所以假设 $s_a$ 之后会跳到 $s_b$，那其实得到的奖励按照 TD 来算应该是 3/4。\n\n**所以不同的方法考虑了不同的假设，运算结果不同。**\n\n## State-action Value Function(Q-function)\n\n还有另外一种评论家叫做 `Q-function`。它又叫做`state-action value function(状态-动作价值函数)`。\n\n* 状态价值函数的输入是一个状态，它是根据状态去计算出，看到这个状态以后的期望的累积奖励( expected accumulated reward)是多少。\n* 状态-动作价值函数的输入是一个状态、动作对，它的意思是说，在某一个状态采取某一个动作，假设我们都使用演员 $\\pi$ ，得到的累积奖励的期望值有多大。\n\nQ-function 有一个需要注意的问题是，这个演员 $\\pi$，在看到状态 s 的时候，它采取的动作不一定是 a。Q-function 假设在状态 s 强制采取动作 a。不管你现在考虑的这个演员 $\\pi$， 它会不会采取动作 a，这不重要。在状态 s 强制采取动作 a。接下来都用演员 $\\pi$ 继续玩下去，就只有在状态 s，我们才强制一定要采取动作 a，接下来就进入自动模式，让演员 $\\pi$ 继续玩下去，得到的期望奖励才是 $Q^{\\pi}(s,a)$ 。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.7.png \':size=550\')\n\nQ-function 有两种写法：\n\n* 输入是状态跟动作，输出就是一个标量；\n* 输入是一个状态，输出就是好几个值。\n\n假设动作是离散的，动作就只有 3 个可能：往左往右或是开火。那这个 Q-function 输出的 3 个值就分别代表 a 是向左的时候的 Q 值，a 是向右的时候的 Q 值，还有 a 是开火的时候的 Q 值。\n\n要注意的事情是，上图右边的函数只有离散动作才能够使用。如果动作是无法穷举的，你只能够用上图左边这个式子，不能够用右边这个式子。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.8.png \':size=550\')\n\n上图是文献上的结果，你去估计 Q-function 的话，看到的结果可能如上图所示。假设我们有 3 个动作：原地不动、向上、向下。\n\n* 假设是在第一个状态，不管是采取哪个动作，最后到游戏结束的时候，得到的期望奖励其实都差不多。因为球在这个地方，就算是你向下，接下来你应该还可以急救。所以不管采取哪个动作，都差不了太多。\n\n* 假设在第二个状态，这个乒乓球它已经反弹到很接近边缘的地方，这个时候你采取向上，你才能得到正的奖励，才接的到球。如果你是站在原地不动或向下的话，接下来你都会错过这个球。你得到的奖励就会是负的。\n\n* 假设在第三个状态，球很近了，所以就要向上。\n\n* 假设在第四个状态，球被反弹回去，这时候采取哪个动作就都没有差了。\n\n这是状态-动作价值的一个例子。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.9.png \':size=550\')\n\n虽然表面上我们学习一个 Q-function，它只能拿来评估某一个演员$\\pi$ 的好坏，但只要有了这个 Q-function，我们就可以做强化学习。有了这个 Q-function，我们就可以决定要采取哪一个动作，我们就可以进行`策略改进(Policy Improvement)`。\n\n它的大原则是这样，假设你有一个初始的演员，也许一开始很烂，随机的也没有关系。初始的演员叫做 $\\pi$，这个 $\\pi$ 跟环境互动，会收集数据。接下来你学习一个 $\\pi$ 这个演员的 Q 值，你去衡量一下 $\\pi$ 在某一个状态强制采取某一个动作，接下来用 $\\pi$ 这个策略 会得到的期望奖励，用 TD 或 MC 都是可以的。你学习出一个 Q-function 以后，就保证你可以找到一个新的策略 $\\pi\'$ ，policy $\\pi\'$ 一定会比原来的策略 $\\pi$ 还要好。那等一下会定义说，什么叫做好。所以假设你有一个 Q-function 和某一个策略 $\\pi$，你根据策略 $\\pi$ 学习出策略 $\\pi$ 的 Q-function，接下来保证你可以找到一个新的策略  $\\pi\'$ ，它一定会比 $\\pi$ 还要好，然后你用 $\\pi\'$ 取代 $\\pi$，再去找它的 Q-function，得到新的以后，再去找一个更好的策略。**这样一直循环下去，policy 就会越来越好。** \n\n首先要定义的是什么叫做比较好？我们说 $\\pi\'$ 一定会比 $\\pi$ 还要好，这边好是说，对所有可能的状态 s 而言，$V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s)$。也就是说我们走到同一个状态 s 的时候，如果拿 $\\pi$ 继续跟环境互动下去，我们得到的奖励一定会小于等于用 $\\pi\'$ 跟环境互动下去得到的奖励。所以不管在哪一个状态，你用 $\\pi\'$ 去做交互，得到的期望奖励一定会比较大。所以 $\\pi\'$ 是比 $\\pi$  还要好的一个策略。\n\n有了 Q-function 以后，怎么找这个 $\\pi\'$ 呢？如果你根据以下的这个式子去决定你的动作，\n$$\n\\pi^{\\prime}(s)=\\arg \\max _{a} Q^{\\pi}(s, a)\n$$\n\n根据上式去决定你的动作的步骤叫做 $\\pi\'$ 的话，那 $\\pi\'$ 一定会比 $\\pi$ 还要好。假设你已经学习出 $\\pi$ 的 Q-function，今天在某一个状态 s，你把所有可能的动作 a 都一一带入这个 Q-function，看看哪一个 a 可以让 Q-function 的值最大，那这个动作就是 $\\pi\'$ 会采取的动作。\n\n这边要注意一下，给定这个状态 s，你的策略 $\\pi$  并不一定会采取动作a，我们是给定某一个状态 s 强制采取动作 a，用 $\\pi$  继续互动下去得到的期望奖励，这个才是 Q-function 的定义。所以在状态 s 里面不一定会采取动作 a。用 $\\pi\'$ 在状态 s 采取动作 a 跟 $\\pi$ 采取的动作是不一定会一样的，$\\pi\'$ 所采取的动作会让它得到比较大的奖励。\n\n* 所以这个 $\\pi\'$ 是用 Q-function 推出来的，没有另外一个网络决定 $\\pi\'$ 怎么交互，有 Q-function 就可以找出 $\\pi\'$。\n* 但是这边有另外一个问题就是，在这边要解一个 arg max 的问题，所以 a 如果是连续的就会有问题。如果是离散的，a 只有 3  个选项，一个一个带进去， 看谁的 Q 最大，没有问题。但如果 a 是连续的，要解 arg max 问题，你就会有问题。\n\n**接下来讲一下为什么用 $Q^{\\pi}(s,a)$ 决定出来的 $\\pi\'$ 一定会比 $\\pi$ 好。**\n\n假设有一个策略叫做 $\\pi\'$，它是由 $Q^{\\pi}$ 决定的。我们要证对所有的状态 s 而言，$V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s)$。\n\n怎么证呢？我们先把 $V^{\\pi}(s)$ 写出来：\n$$\nV^{\\pi}(s)=Q^{\\pi}(s, \\pi(s))\n$$\n假设在状态 s follow $\\pi$ 这个演员，它会采取的动作就是 $\\pi(s)$，那你算出来的 $Q^{\\pi}(s, \\pi(s))$ 会等于 $V^{\\pi}(s)$。一般而言，$Q^{\\pi}(s, \\pi(s))$ 不一定等于 $V^{\\pi}(s)$ ，因为动作不一定是 $\\pi(s)$。但如果这个动作是 $\\pi(s)$ 的话，$Q^{\\pi}(s, \\pi(s))$ 是等于 $V^{\\pi}(s)$ 的。\n\n\n$Q^{\\pi}(s, \\pi(s))$ 还满足如下的关系：\n$$\nQ^{\\pi}(s, \\pi(s)) \\le \\max _{a} Q^{\\pi}(s, a)\n$$\n\n因为 a 是所有动作里面可以让 Q 最大的那个动作，所以今天这一项一定会比它大。这一项就是 $Q^{\\pi}(s, a)$，$a$ 就是 $\\pi\'(s)$。因为 $\\pi\'(s)$ 输出的 $a$ 就是可以让 $Q^\\pi(s,a)$ 最大的那一个，所以我们得到了下面的式子：\n$$\n\\max _{a} Q^{\\pi}(s, a)=Q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)\n$$\n\n于是：\n$$\nV^{\\pi}(s) \\leq Q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)\n$$\n也就是说某一个状态，如果按照策略 $\\pi$ 一直做下去，你得到的奖励一定会小于等于，在这个状态 s 你故意不按照 $\\pi$ 所给你指示的方向，而是按照 $\\pi\'$ 的方向走一步，但只有第一步是按照 $\\pi\'$ 的方向走，只有在状态 s 这个地方，你才按照 $\\pi\'$ 的指示走，接下来你就按照 $\\pi$ 的指示走。虽然只有一步之差， 但是从上面这个式子可知，虽然只有一步之差，但你得到的奖励一定会比完全 follow $\\pi$ 得到的奖励还要大。\n\n接下来要证下面的式子：\n$$\nQ^{\\pi}\\left(s, \\pi^{\\prime}(s) \\right) \\le V^{\\pi\'}(s)\n$$\n\n也就是说，只有一步之差，你会得到比较大的奖励。**但假设每步都是不一样的，每步都是 follow $\\pi\'$ 而不是 $\\pi$ 的话，那你得到的奖励一定会更大。**如果你要用数学式把它写出来的话，你可以写成 $Q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)$ ，它的意思就是说，我们在状态 $s_t$ 采取动作 $a_t$，得到奖励 $r_{t}$，然后跳到状态 $s_{t+1}$，即如下式所示：\n\n$$\nQ^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)=E\\left[r_t+V^{\\pi}\\left(s_{t+1}\\right) \\mid s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right]\n$$\n> 在文献上有时也会说：在状态 $s_t$ 采取动作 $a_t$ 得到奖励 $r_{t+1}$， 有人会写成 $r_t$，但意思其实都是一样的。\n\n在状态 $s$ 按照 $\\pi\'$ 采取某一个动作 $a_t$ ，得到奖励 $r_{t}$，然后跳到状态 $s_{t+1}$，$V^{\\pi}\\left(s_{t+1}\\right)$ 是状态 $s_{t+1}$ 根据 $\\pi$ 这个演员所估出来的值。因为在同样的状态采取同样的动作，你得到的奖励和会跳到的状态不一定一样， 所以这边需要取一个期望值。\n\n因为 $V^{\\pi}(s) \\leq Q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)$，也就是 $V^{\\pi}(s_{t+1}) \\leq Q^{\\pi}\\left(s_{t+1}, \\pi^{\\prime}(s_{t+1})\\right)$，所以我们得到下式：\n$$\n\\begin{array}{l}\nE\\left[r_{t}+V^{\\pi}\\left(s_{t+1}\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right] \\\\\n\\leq E\\left[r_{t}+Q^{\\pi}\\left(s_{t+1}, \\pi^{\\prime}\\left(s_{t+1}\\right)\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right]\n\\end{array}\n$$\n\n因为 $Q^{\\pi}\\left(s_{t+1}, \\pi^{\\prime}\\left(s_{t+1}\\right)\\right) = r_{t+1}+V^{\\pi}\\left(s_{t+2}\\right)$，所以我们得到下式：\n$$\n\\begin{array}{l}\nE\\left[r_{t}+Q^{\\pi}\\left(s_{t+1}, \\pi^{\\prime}\\left(s_{t+1}\\right)\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right] \\\\\n=E\\left[r_{t}+r_{t+1}+V^{\\pi}\\left(s_{t+2}\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right]\n\\end{array}\n$$\n\n然后你再代入 $V^{\\pi}(s) \\leq Q^{\\pi}\\left(s, \\pi^{\\prime}(s)\\right)$，一直算到回合结束，即：\n$$\n\\begin{aligned}\nV^{\\pi}(s) &\\le Q^{\\pi}(s,\\pi\'(s)) \\\\\n&= E\\left[r_{t}+Q^{\\pi}\\left(s_{t+1}, \\pi^{\\prime}\\left(s_{t+1}\\right)\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right] \\\\\n&=E\\left[r_{t}+r_{t+1}+V^{\\pi}\\left(s_{t+2}\\right) |s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right]  \\\\\n& \\le E\\left[r_{t}+r_{t+1}+Q^{\\pi}\\left(s_{t+2},\\pi\'(s_{t+2}\\right) | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right] \\\\\n& = E\\left[r_{t}+r_{t+1}+r_{t+2}+V^{\\pi}\\left(s_{t+3}\\right) |s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right] \\\\\n& \\le \\cdots\\\\\n& \\le E\\left[r_{t}+r_{t+1}+r_{t+2}+\\cdots | s_{t}=s, a_{t}=\\pi^{\\prime}\\left(s_{t}\\right)\\right]  \\\\\n& = V^{\\pi\'}(s)\n\\end{aligned}\n$$\n\n\n因此：\n$$\nV^{\\pi}(s)\\le V^{\\pi\'}(s)\n$$\n\n**从这边我们可以知道，你可以估计某一个策略的 Q-function，接下来你就可以找到另外一个策略 $\\pi\'$ 比原来的策略还要更好。**\n\n## Target Network\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.12.png \':size=550\')\n\n接下来讲一下在 DQN 里一定会用到的 tip。第一个是 `目标网络(target network)`，什么意思呢？我们在学习 Q-function 的时候，也会用到 TD 的概念。那怎么用 TD？你现在收集到一个数据， 是说在状态 $s_t$，你采取动作 $a_t$ 以后，你得到奖励 $r_t$ ，然后跳到状态 $s_{t+1}$。然后根据这个 Q-function，你会知道说\n$$\n\\mathrm{Q}^{\\pi}\\left(s_{t}, a_{t}\\right) \n=r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)\n$$\n\n所以在学习的时候，你会说我们有 Q-function，输入 $s_t$, $a_t$ 得到的值，跟输入 $s_{t+1}$, $\\pi (s_{t+1})$ 得到的值中间，我们希望它差了一个 $r_t$， 这跟刚才讲的 TD 的概念是一样的。\n\n但是实际上这样的一个输入并不好学习，因为假设这是一个回归问题，$\\mathrm{Q}^{\\pi}\\left(s_{t}, a_{t}\\right) $ 是网络的输出，$r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 是目标，你会发现目标是会动的。当然你要实现这样的训练，其实也没有问题，就是你在做反向传播的时候， $Q^{\\pi}$ 的参数会被更新，你会把两个更新的结果加在一起。因为它们是同一个模型 $Q^{\\pi}$， 所以两个更新的结果会加在一起。但这样会导致训练变得不太稳定，因为假设你把 $\\mathrm{Q}^{\\pi}\\left(s_{t}, a_{t}\\right) $ 当作你模型的输出，$r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 当作目标的话，你要去拟合的目标是一直在变的，这种一直在变的目标的训练是不太好训练的。\n\n所以你会把其中一个 Q 网络，通常是你会把右边这个 Q 网络固定住。也就是说你在训练的时候，你只更新左边的 Q 网络的参数，而右边的 Q 网络的参数会被固定住。因为右边的 Q 网络负责产生目标，所以叫 `目标网络`。因为目标网络是固定的，所以你现在得到的目标 $r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 的值也是固定的。因为目标网络是固定的，我们只调左边网络的参数，它就变成是一个回归问题。我们希望模型的输出的值跟目标越接近越好，你会最小化它的均方误差(mean square error)。\n\n在实现的时候，你会把左边的 Q 网络更新好几次以后，再去用更新过的 Q 网络替换这个目标网络。但它们两个不要一起动，它们两个一起动的话，结果会很容易坏掉。\n\n一开始这两个网络是一样的，然后在训练的时候，你会把右边的 Q 网络固定住。你在做梯度下降的时候，只调左边这个网络的参数，那你可能更新 100 次以后才把这个参数复制到右边的网络去，把它盖过去。把它盖过去以后，你这个目标值就变了。就好像说你本来在做一个回归问题，那你训练 后把这个回归问题的 loss 压下去以后，接下来你把这边的参数把它复制过去以后，你的目标就变掉了，接下来就要重新再训练。\n\n###  Intuition\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.13.png \':size=550\')\n\n我们可以通过猫追老鼠的例子来直观地理解为什么要 fix target network。猫是 `Q estimation`，老鼠是 `Q target`。一开始的话，猫离老鼠很远，所以我们想让这个猫追上老鼠。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.14.png \':size=550\')\n\n因为 Q target 也是跟模型参数相关的，所以每次优化后，Q target 也会动。这就导致一个问题，猫和老鼠都在动。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.15.png \':size=550\')\n\n然后它们就会在优化空间里面到处乱动，就会产生非常奇怪的优化轨迹，这就使得训练过程十分不稳定。所以我们可以固定 Q target，让老鼠动得不是那么频繁，可能让它每 5 步动一次，猫则是每一步都在动。如果老鼠每 5 次动一步的话，猫就有足够的时间来接近老鼠。然后它们之间的距离会随着优化过程越来越小，最后它们就可以拟合，拟合过后就可以得到一个最好的Q 网络。\n\n\n## Exploration\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.16.png \':size=550\')\n\n**第二个 tip 是`探索(Exploration)`。**当我们使用 Q-function 的时候，policy 完全取决于 Q-function。给定某一个状态，你就穷举所有的 a， 看哪个 a 可以让 Q 值最大，它就是采取的动作。这个跟策略梯度不一样，在做策略梯度的时候，输出其实是随机的。我们输出一个动作的分布，根据这个动作的分布去做采样， 所以在策略梯度里面，你每次采取的动作是不一样的，是有随机性的。\n\n像这种 Q-function， 如果你采取的动作总是固定的，会有什么问题呢？你会遇到的问题就是这不是一个好的收集数据的方式。因为假设我们今天真的要估某一个状态，你可以采取动作 $a_{1}$, $a_{2}$, $a_{3}$。你要估测在某一个状态采取某一个动作会得到的 Q 值，你一定要在那一个状态采取过那一个动作，才估得出它的值。如果你没有在那个状态采取过那个动作，你其实估不出那个值的。如果是用深的网络，就你的 Q-function 是一个网络，这种情形可能会没有那么严重。但是一般而言，假设 Q-function 是一个表格，没有看过的 state-action pair，它就是估不出值来。网络也是会有一样的问题，只是没有那么严重。所以今天假设你在某一个状态，动作 $a_{1}$, $a_{2}$, $a_{3}$ 你都没有采取过，那你估出来的 $Q(s,a_{1})$, $Q(s,a_{2})$, $Q(s,a_{3})$ 的值可能都是一样的，就都是一个初始值，比如说 0，即\n$$\n\\begin{array}{l}\nQ(s, a_1)=0 \\\\\nQ(s, a_2)=0 \\\\\nQ(s, a_3)=0\n\\end{array}\n$$\n\n但是假设你在状态 s，你采样过某一个动作 $a_{2}$ ，它得到的值是正的奖励。那 $Q(s, a_2)$ 就会比其他的动作都要好。在采取动作的时候， 就看说谁的 Q 值最大就采取谁，所以之后你永远都只会采样到 $a_{2}$，其他的动作就再也不会被做了，所以就会有问题。就好像说你进去一个餐厅吃饭，其实你都很难选。你今天点了某一个东西以后，假说点了某一样东西， 比如说椒麻鸡，你觉得还可以。接下来你每次去就都会点椒麻鸡，再也不会点别的东西了，那你就不知道说别的东西是不是会比椒麻鸡好吃，这个是一样的问题。\n\n如果你没有好的探索的话，你在训练的时候就会遇到这种问题。举个例子， 假设你用 DQN 来玩`slither.io`。 你会有一个蛇，它在环境里面就走来走去，吃到星星，它就加分。假设这个游戏一开始，它往上走，然后就吃到那个星星，它就得到分数，它就知道说往上走可以得到奖励。接下来它就再也不会采取往上走以外的动作了，所以接下来就会变成每次游戏一开始，它就往上冲，然后就死掉。所以需要有探索的机制，让机器知道说，虽然根据之前采样的结果，$a_2$ 好像是不错的，但你至少偶尔也试一下 $a_{1}$ 跟 $a_{3}$，说不定它们更好。\n\n这个问题其实就是`探索-利用窘境(Exploration-Exploitation dilemma)`问题。\n\n有两个方法解这个问题，一个是 `Epsilon Greedy`。Epsilon Greedy($\\varepsilon\\text{-greedy}$) 的意思是说，我们有 $1-\\varepsilon$ 的概率会按照 Q-function 来决定 动作，通常 $\\varepsilon$ 就设一个很小的值， $1-\\varepsilon$ 可能是 90%，也就是 90% 的概率会按照 Q-function 来决定 动作，但是你有 10% 的机率是随机的。通常在实现上 $\\varepsilon$ 会随着时间递减。在最开始的时候。因为还不知道那个动作是比较好的，所以你会花比较大的力气在做探索。接下来随着训练的次数越来越多。已经比较确定说哪一个 Q 是比较好的。你就会减少你的探索，你会把 $\\varepsilon$ 的值变小，主要根据 Q-function 来决定你的动作，比较少随机决定动作，这是 Epsilon Greedy。\n\n还有一个方法叫做 `Boltzmann Exploration`，这个方法就比较像是策略梯度。在策略梯度里面，网络的输出是一个期望的动作空间上面的一个的概率分布，再根据概率分布去做采样。那其实你也可以根据 Q 值 去定一个概率分布，假设某一个动作的 Q 值越大，代表它越好，我们采取这个动作的机率就越高。但是某一个动作的 Q 值小，不代表我们不能尝试。\n\nQ: 我们有时候也要尝试那些 Q 值比较差的动作，怎么做呢？\n\nA: 因为 Q 值是有正有负的，所以可以它弄成一个概率，你先取指数，再做归一化。然后把 $\\exp(Q(s,a))$ 做归一化的这个概率当作是你在决定动作的时候采样的概率。在实现上，Q 是一个网络，所以你有点难知道， 在一开始的时候网络的输出到底会长怎么样子。假设你一开始没有任何的训练数据，参数是随机的，那给定某一个状态 s，不同的 a 输出的值可能就是差不多的，所以一开始 $Q(s,a)$ 应该会倾向于是均匀的。也就是在一开始的时候，你这个概率分布算出来，它可能是比较均匀的。\n\n## Experience Replay\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.17.png \':size=550\')\n\n**第三个 tip 是 `Experience Replay(经验回放)`。** Experience Replay 会构建一个 `Replay Buffer`，Replay Buffer 又被称为 `Replay Memory`。Replay Buffer 是说现在会有某一个策略$\\pi$ 去跟环境做互动，然后它会去收集数据。我们会把所有的数据放到一个 buffer 里面，buffer 里面就存了很多数据。比如说 buffer 是 5 万，这样它里面可以存 5 万笔资料，每一笔资料就是记得说，我们之前在某一个状态 $s_t$，采取某一个动作 $a_t$，得到了奖励 $r_t$。然后跳到状态 $s_{t+1}$。那你用 $\\pi$ 去跟环境互动很多次，把收集到的资料都放到这个 replay buffer 里面。\n\n这边要注意是 replay buffer 里面的经验可能是来自于不同的策略，你每次拿 $\\pi$ 去跟环境互动的时候，你可能只互动 10000 次，然后接下来就更新你的 $\\pi$ 了。但是这个 buffer 里面可以放 5 万笔资料，所以 5 万笔资料可能是来自于不同的策略。Buffer 只有在它装满的时候，才会把旧的资料丢掉。所以这个 buffer 里面它其实装了很多不同的策略的经验。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.18.png \':size=550\')\n\n有了 buffer 以后，怎么训练 Q 的模型，怎么估 Q-function？你的做法是这样：迭代地去训练这个 Q-function，在每次迭代里面，从这个 buffer 里面随机挑一个 batch 出来，就跟一般的网络训练一样，从那个训练集里面，去挑一个 batch 出来。你去采样一个 batch 出来，里面有一把的经验，根据这把经验去更新你的 Q-function。就跟 TD learning 要有一个目标网络是一样的。你去采样一堆 batch，采样一个 batch 的数据，采样一堆经验，然后再去更新你的 Q-function。\n\n当我们这么做的时候， 它变成了一个 `off-policy` 的做法。因为本来我们的 Q 是要观察 $\\pi$ 的经验，但实际上 replay buffer 里面的这些经验不是通通来自于 $\\pi$，有些是过去其他的 $\\pi$  所遗留下来的经验。因为你不会拿某一个 $\\pi$ 就把整个 buffer 装满，然后拿去测 Q-function，这个 $\\pi$ 只是采样一些数据塞到那个 buffer 里面去，然后接下来就让 Q 去训练。所以 Q 在采样的时候， 它会采样到过去的一些资料。\n\n这么做有两个好处：\n\n* 其实在做强化学习的时候， 往往最花时间的步骤是在跟环境做互动，训练网络反而是比较快的。因为用 GPU 训练其实很快， 真正花时间的往往是在跟环境做互动。用 replay buffer 可以减少跟环境做互动的次数，因为在做训练的时候，你的经验不需要通通来自于某一个策略。一些过去的策略所得到的经验可以放在 buffer 里面被使用很多次，被反复的再利用，这样让采样到经验的利用是比较高效的。\n\n* 在训练网络的时候，其实我们希望一个 batch 里面的数据越多样(diverse)越好。如果 batch 里面的数据都是同样性质的，训练下去是容易坏掉的。如果 batch 里面都是一样的数据，训练的时候，performance 会比较差。我们希望 batch 的数据越多样越好。那如果 buffer 里面的那些经验通通来自于不同的策略，那采样到的一个 batch 里面的数据会是比较多样的。\n\nQ：我们明明是要观察 $\\pi$ 的值，里面混杂了一些不是 $\\pi$ 的经验，这有没有关系？\n\nA：没关系。这并不是因为过去的 $\\pi$ 跟现在的 $\\pi$ 很像， 就算过去的 $\\pi$ 没有很像，其实也是没有关系的。主要的原因是因为， 我们并不是去采样一个轨迹，我们只采样了一笔经验，所以跟是不是 off-policy 这件事是没有关系的。就算是 off-policy，就算是这些经验不是来自于 $\\pi$，我们其实还是可以拿这些经验来估测 $Q^{\\pi}(s,a)$。这件事有点难解释，不过你就记得说 Experience Replay 在理论上也是没有问题的。\n\n## DQN\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/dqn.png \':size=550\')\n\nDQN 使用深度卷积神经网络近似拟合状态动作值函数 $Q(s,a)$，其网络结构如上图所示。DQN 模型的输入是距离当前时刻最近的 4 帧图像，该输入经过 3 个卷积层和 2 个全连接层的非线性变化后，最终在输出层输出每个动作对应的 Q 值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/6.19.png \':size=550\')\n\n\n上图就是一般的 `Deep Q-network(DQN)` 的算法。\n\n这个算法是这样的。初始化的时候，你初始化 2 个网络：Q 和 $\\hat{Q}$，其实 $\\hat{Q}$ 就等于 Q。一开始这个目标 Q 网络，跟你原来的 Q 网络是一样的。在每一个 episode，你拿你的演员去跟环境做互动，在每一次互动的过程中，你都会得到一个状态 $s_t$，那你会采取某一个动作 $a_t$。怎么知道采取哪一个动作 $a_t$ 呢？你就根据你现在的 Q-function。但是你要有探索的机制。比如说你用 Boltzmann 探索或是 Epsilon Greedy 的探索。那接下来你得到奖励 $r_t$，然后跳到状态 $s_{t+1}$。所以现在收集到一笔数据，这笔数据是 ($s_t$, $a_t$ ,$r_t$, $s_{t+1}$)。这笔数据就塞到你的 buffer 里面去。如果 buffer 满的话， 你就再把一些旧的资料丢掉。接下来你就从你的 buffer 里面去采样数据，那你采样到的是 $(s_{i}, a_{i}, r_{i}, s_{i+1})$。这笔数据跟你刚放进去的不一定是同一笔，你可能抽到一个旧的。要注意的是，其实你采样出来不是一笔数据，你采样出来的是一个 batch 的数据，你采样一个 batch 出来，采样一把经验出来。接下来就是计算你的目标。假设采样出这么一笔数据。根据这笔数据去算你的目标。你的目标是什么呢？目标记得要用目标网络 $\\hat{Q}$ 来算。目标是：\n\n$$\ny=r_{i}+\\max _{a} \\hat{Q}\\left(s_{i+1}, a\\right)\n$$\n其中 a 就是让 $\\hat{Q}$ 的值最大的 a。因为我们在状态 $s_{i+1}$会采取的动作 a，其实就是那个可以让 Q 值最大的那一个 a。接下来我们要更新 Q 的值，那就把它当作一个回归问题。希望 $Q(s_i,a_i)$  跟你的目标越接近越好。然后假设已经更新了某一个数量的次，比如说 C 次，设 C = 100， 那你就把 $\\hat{Q}$ 设成 Q，这就是 DQN。\n\nQ: DQN 和 Q-learning 有什么不同？\n\nA: 整体来说，DQN 与 Q-learning 的目标价值以及价值的更新方式都非常相似，主要的不同点在于：\n\n* DQN 将 Q-learning 与深度学习结合，用深度网络来近似动作价值函数，而 Q-learning 则是采用表格存储；\n* DQN 采用了经验回放的训练方法，从历史数据中随机采样，而 Q-learning 直接采用下一个状态的数据进行学习。\n\n## 一些技巧\n\n下面我们介绍下 DQN 的基本技巧：\n\n* 在 Atari 游戏里面，一般 mini-batch 设置为 32。\n* Experience replay 用在新问题上一般为 $10^6$。\n\n## References\n\n* [Intro to Reinforcement Learning (强化学习纲要）](https://github.com/zhoubolei/introRL)\n* [神经网络与深度学习](https://nndl.github.io/)\n* [强化学习基础 David Silver 笔记](https://zhuanlan.zhihu.com/c_135909947)\n* [百面深度学习](https://book.douban.com/subject/35043939/)\n* [机器学习(北理工)](https://www.icourse163.org/course/BIT-1449601164)\n* 苗光辉. 面向部分可观测环境的值迭代深度网络模型研究[D].北京理工大学,2018.\n','2021-12-10 12:43:49','2021-12-19 19:39:10'),
	(46,3,'第六章 习题','## 1 Keywords\r\n\r\n- **DQN(Deep Q-Network)：**  基于深度学习的Q-learninyang算法，其结合了 Value Function Approximation（价值函数近似）与神经网络技术，并采用了目标网络（Target Network）和经验回放（Experience Replay）等方法进行网络的训练。\r\n- **State-value Function：** 本质是一种critic。其输入为actor某一时刻的state，对应的输出为一个标量，即当actor在对应的state时，预期的到过程结束时间段中获得的value的数值。\r\n- **State-value Function Bellman Equation：** 基于state-value function的Bellman Equation，它表示在状态 $s_t$ 下带来的累积奖励 $G_t$ 的期望。\r\n- **Q-function:** 其也被称为state-action value function。其input 是一个 state 跟 action 的 pair，即在某一个 state 采取某一个action，假设我们都使用 actor $\\pi$ ，得到的 accumulated reward 的期望值有多大。\r\n- **Target Network：** 为了解决在基于TD的Network的问题时，优化目标 $\\mathrm{Q}^{\\pi}\\left(s_{t}, a_{t}\\right) \r\n  =r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大regression的难度。target network选择将上式的右部分即 $r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 固定，通过改变上式左部分的network的参数，进行regression，这也是一个DQN中比较重要的tip。\r\n- **Exploration：**  在我们使用Q-function的时候，我们的policy完全取决于Q-function，有可能导致出现对应的action是固定的某几个数值的情况，而不像policy gradient中的output为随机的，我们再从随机的distribution中sample选择action。这样会导致我们继续训练的input的值一样，从而“加重”output的固定性，导致整个模型的表达能力的急剧下降，这也就是`探索-利用窘境难题(Exploration-Exploitation dilemma)`。所以我们使用`Epsilon Greedy`和 `Boltzmann Exploration`等Exploration方法进行优化。\r\n- **Experience Replay（经验回放）：**  其会构建一个Replay Buffer（Replay Memory），用来保存许多data，每一个data的形式如下：在某一个 state $s_t$，采取某一个action $a_t$，得到了 reward $r_t$，然后跳到 state $s_{t+1}$。我们使用 $\\pi$ 去跟环境互动很多次，把收集到的数据都放到这个 replay buffer 中。当我们的buffer”装满“后，就会自动删去最早进入buffer的data。在训练时，对于每一轮迭代都有相对应的batch（与我们训练普通的Network一样通过sample得到），然后用这个batch中的data去update我们的Q-function。综上，Q-function再sample和训练的时候，会用到过去的经验数据，所以这里称这个方法为Experience Replay，其也是DQN中比较重要的tip。\r\n\r\n## 2 Questions\r\n\r\n- 为什么在DQN中采用价值函数近似（Value Function Approximation）的表示方法？\r\n\r\n  答：首先DQN为基于深度学习的Q-learning算法，而在Q-learning中，我们使用表格来存储每一个state下action的reward，即我们前面所讲的状态-动作值函数 $Q(s,a)$ 。但是在我们的实际任务中，状态量通常数量巨大并且在连续的任务中，会遇到维度灾难的问题，所以使用真正的Value Function通常是不切实际的，所以使用了价值函数近似（Value Function Approximation）的表示方法。\r\n\r\n- critic output通常与哪几个值直接相关？\r\n\r\n  答：critic output与state和actor有关。我们在讨论output时通常是对于一个actor下来衡量一个state的好坏，也就是state value本质上来说是依赖于actor。不同的actor在相同的state下也会有不同的output。\r\n\r\n- 我们通常怎么衡量state value function  $V^{\\pi}(s)$ ?分别的优势和劣势有哪些？\r\n\r\n  答：\r\n\r\n  - **基于Monte-Carlo（MC）的方法** ：本质上就是让actor与environment做互动。critic根据”统计“的结果，将actor和state对应起来，即当actor如果看到某一state $s_a$ ，将预测接下来的accumulated reward有多大如果它看到 state $s_b$，接下来accumulated reward 会有多大。 但是因为其普适性不好，其需要把所有的state都匹配到，如果我们我们是做一个简单的贪吃蛇游戏等state有限的问题，还可以进行。但是如果我们做的是一个图片型的任务，我们几乎不可能将所有的state（对应每一帧的图像）的都”记录“下来。总之，其不能对于未出现过的input state进行对应的value的输出。\r\n  - **基于MC的Network方法：** 为了解决上面描述的Monte-Carlo（MC）方法的不足，我们将其中的state value function  $V^{\\pi}(s)$ 定义为一个Network，其可以对于从未出现过的input state，根据network的泛化和拟合能力，也可以”估测“出一个value output。\r\n  - **基于Temporal-difference（时序差分）的Network方法，即TD based Network：** 与我们再前4章介绍的MC与TD的区别一样，这里两者的区别也相同。在 MC based 的方法中，每次我们都要算 accumulated reward，也就是从某一个 state $s_a$ 一直玩到游戏结束的时候，得到的所有 reward 的总和。所以要应用 MC based 方法时，我们必须至少把这个游戏玩到结束。但有些游戏非常的长，你要玩到游戏结束才能够 update network，花的时间太长了。因此我们会采用 TD based 的方法。TD based 的方法不需要把游戏玩到底，只要在游戏的某一个情况，某一个 state $s_t$ 的时候，采取 action $a_t$ 得到 reward $r_t$ ，跳到 state $s_{t+1}$，就可以应用 TD 的方法。公式与之前介绍的TD方法类似，即：$V^{\\pi}\\left(s_{t}\\right)=V^{\\pi}\\left(s_{t+1}\\right)+r_{t}$。\r\n  - **基于MC和基于TD的区别在于：** MC本身具有很大的随机性，我们可以将其 $G_a$  堪称一个random的变量，所以其最终的variance很大。而对于TD，其具有随机性的变量为 $r$ ,因为计算 $s_t$ 我们采取同一个 action，你得到的 reward 也不一定是一样的，所以对于TD来说，$r$ 是一个 random 变量。但是相对于MC的 $G_a$  的随机程度来说， $r$ 的随机性非常小，这是因为本身 $G_a$ 就是由很多的 $r$ 组合而成的。但另一个角度来说， 在TD中，我们的前提是 $r_t=V^{\\pi}\\left(s_{t+1}\\right)-V^{\\pi}\\left(s_{t}\\right)$ ,但是我们通常无法保证 $V^{\\pi}\\left(s_{t+1}\\right)、V^{\\pi}\\left(s_{t}\\right)$ 计算的误差为零。所以当 $V^{\\pi}\\left(s_{t+1}\\right)、V^{\\pi}\\left(s_{t}\\right)$  计算的不准确的话，那应用上式得到的结果，其实也会是不准的。所以 MC 跟 TD各有优劣。\r\n  - **目前， TD 的方法是比较常见的，MC 的方法其实是比较少用的。**\r\n\r\n- 基于我们上面说的network（基于MC）的方法，我们怎么训练这个网络呢？或者我们应该将其看做ML中什么类型的问题呢？\r\n\r\n  答：理想状态，我们期望对于一个input state输出其无误差的reward value。也就是说这个 value function 来说，如果 input 是 state $s_a$，正确的 output 应该是$G_a$。如果 input state $s_b$，正确的output 应该是value $G_b$。所以在训练的时候，其就是一个典型的ML中的回归问题（regression problem）。所以我们实际中需要输出的仅仅是一个非精确值，即你希望在 input $s_a$ 的时候，output value 跟 $G_a$ 越近越好，input $s_b$ 的时候，output value 跟 $G_b$ 越近越好。其训练方法，和我们在训练CNN、DNN时的方法类似，就不再一一赘述。\r\n\r\n- 基于上面介绍的基于TD的network方法，具体地，我们应该怎么训练模型呢？\r\n\r\n  答：核心的函数为 $V^{\\pi}\\left(s_{t}\\right)=V^{\\pi}\\left(s_{t+1}\\right)+r_{t}$。我们将state $s_t$  作为input输入network 里，因为 $s_t$ 丢到 network 里面会得到output $V^{\\pi}(s_t)$，同样将 $s_{t+1}$ 作为input输入 network 里面会得到$V^{\\pi}(s_{t+1})$。同时核心函数：$V^{\\pi}\\left(s_{t}\\right)=V^{\\pi}\\left(s_{t+1}\\right)+r_{t}$  告诉我们，  $V^{\\pi}(s_t)$ 减 $V^{\\pi}(s_{t+1})$ 的值应该是 $r_t$。然后希望它们两个相减的 loss 跟 $r_t$ 尽可能地接近。这也就是我们这个network的优化目标或者说loss function。\r\n\r\n- state-action value function（Q-function）和 state value function的有什么区别和联系？\r\n\r\n  答：\r\n\r\n  - state value function 的 input 是一个 state，它是根据 state 去计算出，看到这个state 以后的 expected accumulated reward 是多少。\r\n  - state-action value function 的 input 是一个 state 跟 action 的 pair，即在某一个 state 采取某一个action，假设我们都使用 actor $\\pi$ ，得到的 accumulated reward 的期望值有多大。\r\n\r\n- Q-function的两种表示方法？\r\n\r\n  答：\r\n\r\n  - 当input 是 state和action的pair时，output 就是一个 scalar。\r\n  - 当input 仅是一个 state时，output 就是好几个 value。\r\n\r\n- 当我们有了Q-function后，我们怎么找到更好的策略 $\\pi\'$ 呢？或者说这个 $\\pi\'$ 本质来说是什么？\r\n\r\n  答：首先， $\\pi\'$ 是由 $\\pi^{\\prime}(s)=\\arg \\max _{a} Q^{\\pi}(s, a)$ 计算而得，其表示假设你已经 learn 出 $\\pi$ 的Q-function，今天在某一个 state s，把所有可能的 action a 都一一带入这个 Q-function，看看说那一个 a 可以让 Q-function 的 value 最大，那这一个 action，就是 $\\pi\'$ 会采取的 action。所以根据上式决定的actoin的步骤一定比原来的 $\\pi$ 要好，即$V^{\\pi^{\\prime}}(s) \\geq V^{\\pi}(s)$。\r\n\r\n- 解决`探索-利用窘境(Exploration-Exploitation dilemma)`问题的Exploration的方法有哪些？他们具体的方法是怎样的？\r\n\r\n  答：\r\n\r\n  1. **Epsilon Greedy：** 我们有$1-\\varepsilon$ 的机率，通常 $\\varepsilon$ 很小，完全按照Q-function 来决定action。但是有 $\\varepsilon$ 的机率是随机的。通常在实现上 $\\varepsilon$ 会随着时间递减。也就是在最开始的时候。因为还不知道那个action 是比较好的，所以你会花比较大的力气在做 exploration。接下来随着training 的次数越来越多。已经比较确定说哪一个Q 是比较好的。你就会减少你的exploration，你会把 $\\varepsilon$ 的值变小，主要根据Q-function 来决定你的action，比较少做random，这是**Epsilon Greedy**。\r\n  2. **Boltzmann Exploration：** 这个方法就比较像是 policy gradient。在 policy gradient 里面network 的output 是一个 expected action space 上面的一个的 probability distribution。再根据 probability distribution 去做 sample。所以也可以根据Q value 去定一个 probability distribution，假设某一个 action 的 Q value 越大，代表它越好，我们采取这个 action 的机率就越高。这是**Boltzmann Exploration**。\r\n\r\n- 我们使用Experience Replay（经验回放）有什么好处？\r\n\r\n  答：\r\n\r\n  1. 首先，在强化学习的整个过程中， 最花时间的 step 是在跟环境做互动，使用GPU乃至TPU加速来训练 network 相对来说是比较快的。而用 replay buffer 可以减少跟环境做互动的次数，因为在训练的时候，我们的 experience 不需要通通来自于某一个policy（或者当前时刻的policy）。一些过去的 policy 所得到的 experience 可以放在 buffer 里面被使用很多次，被反复的再利用，这样让你的 sample 到 experience 的利用是高效的。\r\n  2. 另外，在训练网络的时候，其实我们希望一个 batch 里面的 data 越 diverse 越好。如果你的 batch 里面的 data 都是同样性质的，我们的训练出的模型拟合能力不会很乐观。如果 batch 里面都是一样的 data，你 train 的时候，performance 会比较差。我们希望 batch data 越 diverse 越好。那如果 buffer 里面的那些 experience 通通来自于不同的 policy ，那你 sample 到的一个 batch 里面的 data 会是比较 diverse 。这样可以保证我们模型的性能至少不会很差。\r\n\r\n- 在Experience Replay中我们是要观察 $\\pi$ 的 value，里面混杂了一些不是 $\\pi$ 的 experience ，这会有影响吗？\r\n\r\n  答：没关系。这并不是因为过去的 $\\pi$ 跟现在的 $\\pi$ 很像， 就算过去的$\\pi$ 没有很像，其实也是没有关系的。主要的原因是我们并不是去sample 一个trajectory，我们只sample 了一个experience，所以跟是不是 off-policy 这件事是没有关系的。就算是off-policy，就算是这些 experience 不是来自于 $\\pi$，我们其实还是可以拿这些 experience 来估测 $Q^{\\pi}(s,a)$。\r\n\r\n- DQN（Deep Q-learning）和Q-learning有什么异同点？\r\n\r\n  答：整体来说，从名称就可以看出，两者的目标价值以及价值的update方式基本相同，另外一方面，不同点在于：\r\n\r\n  - 首先，DQN 将 Q-learning 与深度学习结合，用深度网络来近似动作价值函数，而 Q-learning 则是采用表格存储。\r\n  - DQN 采用了我们前面所描述的经验回放（Experience Replay）训练方法，从历史数据中随机采样，而 Q-learning 直接采用下一个状态的数据进行学习。\r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：请问DQN（Deep Q-Network）是什么？其两个关键性的技巧分别是什么？\r\n\r\n  答：Deep Q-Network是基于深度学习的Q-learning算法，其结合了 Value Function Approximation（价值函数近似）与神经网络技术，并采用了目标网络（Target Network）和经验回放（Experience Replay）的方法进行网络的训练。\r\n\r\n- 高冷的面试官：接上题，DQN中的两个trick：目标网络和experience replay的具体作用是什么呢？\r\n\r\n  答：在DQN中某个动作值函数的更新依赖于其他动作值函数。如果我们一直更新值网络的参数，会导致\r\n  更新目标不断变化，也就是我们在追逐一个不断变化的目标，这样势必会不太稳定。 为了解决在基于TD的Network的问题时，优化目标 $\\mathrm{Q}^{\\pi}\\left(s_{t}, a_{t}\\right) =r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大regression的难度。target network选择将上式的右部分即 $r_{t}+\\mathrm{Q}^{\\pi}\\left(s_{t+1}, \\pi\\left(s_{t+1}\\right)\\right)$ 固定，通过改变上式左部分的network的参数，进行regression。对于经验回放，其会构建一个Replay Buffer（Replay Memory），用来保存许多data，每一个data的形式如下：在某一个 state $s_t$，采取某一个action $a_t$，得到了 reward $r_t$，然后跳到 state $s_{t+1}$。我们使用 $\\pi$ 去跟环境互动很多次，把收集到的数据都放到这个 replay buffer 中。当我们的buffer”装满“后，就会自动删去最早进入buffer的data。在训练时，对于每一轮迭代都有相对应的batch（与我们训练普通的Network一样通过sample得到），然后用这个batch中的data去update我们的Q-function。也就是，Q-function再sample和训练的时候，会用到过去的经验数据，也可以消除样本之间的相关性。\r\n\r\n- 高冷的面试官：DQN（Deep Q-learning）和Q-learning有什么异同点？\r\n\r\n  答：整体来说，从名称就可以看出，两者的目标价值以及价值的update方式基本相同，另外一方面，不同点在于：\r\n\r\n  - 首先，DQN 将 Q-learning 与深度学习结合，用深度网络来近似动作价值函数，而 Q-learning 则是采用表格存储。\r\n  - DQN 采用了我们前面所描述的经验回放（Experience Replay）训练方法，从历史数据中随机采样，而 Q-learning 直接采用下一个状态的数据进行学习。\r\n\r\n- 高冷的面试官：请问，随机性策略和确定性策略有什么区别吗？\r\n\r\n  答：随机策略表示为某个状态下动作取值的分布，确定性策略在每个状态只有一个确定的动作可以选。\r\n  从熵的角度来说，确定性策略的熵为0，没有任何随机性。随机策略有利于我们进行适度的探索，确定\r\n  性策略的探索问题更为严峻。\r\n\r\n- 高冷的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？\r\n\r\n  答：在神经网络中通常使用随机梯度下降法。随机的意思是我们随机选择一些样本来增量式的估计梯度，比如常用的\r\n  采用batch训练。如果样本是相关的，那就意味着前后两个batch的很可能也是相关的，那么估计的梯度也会呈现\r\n  出某种相关性。如果不幸的情况下，后面的梯度估计可能会抵消掉前面的梯度量。从而使得训练难以收敛。\r\n','2021-12-10 12:43:49','2021-12-19 19:39:25'),
	(47,3,'第七章 DQN进阶技巧','## Double DQN\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.1.png)\n\n接下来要讲的是训练 Q-learning 的一些 tips。第一个 tip 是做 `Double DQN`。为什么要有 Double DQN 呢？因为在实现上，你会发现 Q 值往往是被高估的。上图来自于 Double DQN 的原始 paper，它想要显示的结果就是 Q 值往往是被高估的。\n\n这边有 4 个不同的小游戏，横轴是训练的时间，红色锯齿状一直在变的线就是 Q-function 对不同的状态估计出来的平均 Q 值，有很多不同的状态，每个状态你都 sample 一下，然后算它们的 Q 值，把它们平均起来。\n\n这条红色锯齿状的线在训练的过程中会改变，但它是不断上升的。因为 Q-function 是取决于你的策略的。学习的过程中你的策略越来越强，你得到的 Q 值会越来越大。在同一个状态， 你得到 reward 的期望会越来越大，所以一般而言，这个值都是上升的，但这是 Q-network 估测出来的值。\n\n接下来你真地去算它，怎么真地去算？你有策略，然后真的去玩那个游戏，就玩很多次，玩个一百万次。然后就去真地算说，在某一个状态， 你会得到的 Q 值到底有多少。你会得到在某一个状态采取某一个动作。你接下来会得到累积奖励(accumulated reward)是多少。你会发现估测出来的值远比实际的值大，在每一个游戏都是这样，都大很多。所以今天要提出 Double DQN 的方法，它可以让估测的值跟实际的值是比较接近的。\n\n我们先看它的结果，蓝色的锯齿状的线是 Double DQN 的 Q-network 所估测出来的 Q 值，蓝色的无锯齿状的线是真正的 Q 值，你会发现它们是比较接近的。 用网络估测出来的就不用管它，比较没有参考价值。用 Double DQN 得出来真正的累积奖励，在这 3 种情况下都是比原来的 DQN 高的，代表 Double DQN 学习出来的那个策略比较强。所以它实际上得到的 reward 是比较大的。虽然一般的 DQN 的 Q-network 高估了自己会得到的 reward，但实际上它得到的 reward 是比较低的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.2.png)\n\nQ: 为什么 Q 值总是被高估了呢？\n\nA:因为实际上在做的时候，是要让左边这个式子跟右边这个目标越接近越好。你会发现目标的值很容易一不小心就被设得太高。因为在算这个目标的时候，我们实际上在做的事情是，看哪一个 a 可以得到最大的 Q 值，就把它加上去，就变成我们的目标。所以假设有某一个动作得到的值是被高估的。\n\n举例来说， 现在有 4 个动作，本来它们得到的值都是差不多的，它们得到的 reward 都是差不多的。但是在估计的时候，网络是有误差的。\n\n* 假设是第一个动作被高估了，假设绿色的东西代表是被高估的量，它被高估了，那这个目标就会选这个动作，然后就会选这个高估的 Q 值来加上 $r_t$，来当作你的目标。\n* 如果第四个动作被高估了，那就会选第四个动作来加上 $r_t$ 来当作你的目标值。所以你总是会选那个 Q 值被高估的，你总是会选那个 reward 被高估的动作当作这个 max 的结果去加上 $r_t$ 当作你的目标，所以你的目标总是太大。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.3.png)\nQ: 怎么解决目标值总是太大的问题呢？\n\nA: 在 Double DQN 里面，选动作的 Q-function 跟算值的 Q-function 不是同一个。在原来的 DQN 里面，你穷举所有的 a，把每一个 a 都带进去， 看哪一个 a 可以给你的 Q 值最高，那你就把那个 Q 值加上 $r_t$。但是在 Double DQN 里面，你有两个 Q-network：\n\n* 第一个 Q-network Q 决定哪一个动作的 Q 值最大（你把所有的 a 带入 Q 中，看看哪一个 Q 值最大）。\n* 你决定你的动作以后，你的 Q 值是用 $Q\'$ 算出来的。\n\n假设我们有两个 Q-function，\n\n* 假设第一个 Q-function 高估了它现在选出来的动作 a，只要第二个 Q-function $Q\'$ 没有高估这个动作 a 的值，那你算出来的就还是正常的值。\n* 假设 $Q\'$ 高估了某一个动作的值，那也没差，因为只要前面这个 Q 不要选那个动作出来就没事了，这个就是 Double DQN 神奇的地方。\n\nQ: 哪来 Q  跟 $Q\'$ 呢？哪来两个网络呢？\n\nA: 在实现上，你有两个 Q-network：目标的 Q-network 和你会更新的 Q-network。所以在 Double DQN 里面，你会拿你会更新参数的那个 Q-network 去选动作，然后你拿目标网络（固定住不动的网络）去算值。\n\nDouble DQN 相较于原来的 DQN 的更改是最少的，它几乎没有增加任何的运算量，连新的网络都不用，因为原来就有两个网络了。你唯一要做的事情只有，本来你在找 Q 值最大的 a 的时候，你是用 $Q\'$ 来算，你是用目标网络来算，现在改成用另外一个会更新的 Q-network 来算。\n\n假如你今天只选一个 tip 的话，正常人都是实现 Double DQN，因为很容易实现。\n\n## Dueling DQN\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.4.png)\n第二个 tip 是 `Dueling DQN`。其实 Dueling DQN 也蛮好做的，相较于原来的 DQN，它唯一的差别是改了网络的架构。Q-network 就是输入状态，输出就是每一个动作的 Q 值。Dueling DQN 唯一做的事情是改了网络的架构，其它的算法都不需要动。\n\nQ: Dueling DQN 是怎么改了网络的架构呢？\n\nA: 本来的 DQN 就是直接输出 Q 值的值。现在这个 dueling 的 DQN，就是下面这个网络的架构。它不直接输出 Q 值的值，它分成两条路径去运算：\n\n* 第一条路径会输出一个 scalar，这个 scalar 叫做 $V(s)$。因为它跟输入 s 是有关系，所以叫做 $V(s)$，$V(s)$ 是一个 scalar。\n* 第二条路径会输出一个 vector，这个 vector 叫做 $A(s,a)$。下面这个 vector，它是每一个动作都有一个值。\n\n你再把这两个东西加起来就可以得到你的 Q 值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.5.png)\n\nQ: 这么改有什么好处？\n\nA : 那我们假设说，原来的 $Q(s,a)$ 就是一个表格。我们假设状态是离散的，实际上状态不是离散的。为了说明方便，我们假设就是只有 4 个不同的状态，只有 3 个不同的动作，所以 $Q(s,a)$  你可以看作是一个表格。\n\n我们知道：\n$$\nQ(s,a) = V(s) + A(s,a)\n$$\n\n其中\n\n* $V(s)$ 是对不同的状态 它都有一个值。 \n* $A(s,a)$ 它是对不同的状态，不同的动作都有一个值。\n\n你把这个 V 的值加到 A 的每一列就会得到 Q 的值。把 2+1，2+(-1)，2+0，就得到 3，1，2，以此类推。\n\n如上图所示，假设说你在训练网络的时候，目标是希望这一个值变成 4，这一个值变成 0。但是你实际上能更改的并不是 Q 的值，你的网络更改的是 V 跟 A 的值。根据网络的参数，V 跟 A 的值输出以后，就直接把它们加起来，所以其实不是更动 Q 的值。\n\n然后在学习网络的时候，假设你希望这边的值，这个 3 增加 1 变成 4，这个 -1 增加 1 变成 0。最后你在训练网络的时候，网络 可能会说，我们就不要动这个 A 的值，就动 V 的值，把 V 的值从 0 变成 1。把 0 变成 1 有什么好处呢？你会发现说，本来你只想动这两个东西的值，那你会发现说，这个第三个值也动了，-2 变成  -1。所以有可能说你在某一个状态，你明明只 sample 到这 2 个动作，你没 sample 到第三个动作，但是你其实也可以更改第三个动作的 Q 值。这样的好处就是你不需要把所有的 state-action pair 都 sample 过，你可以用比较高效的方式去估计 Q 值出来。因为有时候你更新的时候，不一定是更新下面这个表格。而是只更新了 $V(s)$，但更新 $V(s)$ 的时候，只要一改所有的值就会跟着改。这是一个比较有效率的方法，去使用你的数据，这个是 Dueling DQN 可以带给我们的好处。\n\n那可是接下来有人就会问说会不会最后 学习出来的结果是说，反正 machine 就学到 V 永远都是 0，然后反正 A 就等于 Q，那你就没有得到任何 Dueling DQN 可以带给你的好处， 就变成跟原来的 DQN 一模一样。为了避免这个问题，实际上你要给 A 一些约束，让 更新 A 其实比较麻烦，让网络倾向于会想要去用 V 来解问题。\n\n举例来说，你可以看原始的文献，它有不同的约束 。一个最直觉的约束是你必须要让这个 A 的每一列的和都是 0，所以看我这边举的例子，列的和都是 0。如果这边列的和都是 0，这边这个 V 的值，你就可以想成是上面 Q 的每一列的平均值。这个平均值，加上这些值才会变成是 Q 的 值。所以今天假设你发现说你在更新参数的时候，你是要让整个列一起被更新。你就不会想要更新这边，因为你不会想要更新 Ａ 这个矩阵。因为 A 这个矩阵的每一列的和都要是 0，所以你没有办法说，让这边的值，通通都 +1，这件事是做不到的。因为它的约束就是你的和永远都是要 0。所以不可以都 +1，这时候就会强迫网络去更新 V 的值，然后让你可以用比较有效率的方法，去使用你的数据。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.6.png)\n\n实现时，你要给这个 A 一个约束。举个例子，假设你有 3 个动作，然后在这边输出的 vector 是 $[7,3,2]^{\\mathrm{T}}$，你在把这个 A 跟这个 V  加起来之前，先加一个归一化(normalization)，就好像做那个层归一化( layer normalization)一样。加一个归一化，这个归一化做的事情就是把 7+3+2 加起来等于 12，12/3 = 4。然后把这边通通减掉 4，变成 3, -1, 2。再把 3, -1, 2 加上 1.0，得到最后的 Q 值。这个归一化的步骤就是网络的其中一部分，在训练的时候，你从这边也是一路 back propagate 回来的，只是归一化是没有参数的，它只是一个归一化的操作。把它可以放到网络里面，跟网络的其他部分 jointly trained，这样 A 就会有比较大的约束。这样网络就会给它一些好处，倾向于去更新 V 的值，这个是 Dueling DQN。\n\n\n## Prioritized Experience Replay\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.7.png)\n有一个技巧叫做 `Prioritized Experience Replay`。Prioritized Experience Replay 是什么意思呢？\n\n我们原来在 sample 数据去训练你的 Q-network 的时候，你是均匀地从 experience buffer 里面去 sample 数据。那这样不见得是最好的， 因为也许有一些数据比较重要。假设有一些数据，你之前有 sample 过。你发现这些数据的 TD error 特别大（TD error 就是网络的输出跟目标之间的差距），那这些数据代表说你在训练网络的时候， 你是比较训练不好的。那既然比较训练不好， 那你就应该给它比较大的概率被 sample 到，即给它 `priority`。这样在训练的时候才会多考虑那些训练不好的训练数据。实际上在做 prioritized experience replay 的时候，你不仅会更改 sampling 的 process，你还会因为更改了 sampling 的过程，更改更新参数的方法。所以 prioritized experience replay 不仅改变了 sample 数据的分布，还改变了训练过程。\n## Balance between MC and TD\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.8.png)\n**另外一个可以做的方法是 balance MC 跟 TD。**MC 跟 TD 的方法各自有各自的优劣，怎么在 MC 跟 TD 里面取得一个平衡呢？我们的做法是这样，在 TD 里面，在某一个状态 $s_t$ 采取某一个动作$a_t$ 得到 reward $r_t$，接下来跳到那一个状态 $s_{t+1}$。但是我们可以不要只存一个步骤的数据，我们存 N 个步骤的数据。\n\n我们记录在 $s_t$ 采取 $a_t$，得到 $r_t$，会跳到什么样 $s_t$。一直纪录到在第 N 个步骤以后，在 $s_{t+N}$采取 $a_{t+N}$ 得到 reward $r_{t+N}$，跳到 $s_{t+N+1}$ 的这个经验，通通把它存下来。实际上你今天在做更新的时候， 在做 Q-network learning 的时候，你的 learning 的方法会是这样，你 learning 的时候，要让 $Q(s_t,a_t)$ 跟你的目标值越接近越好。$\\hat{Q}$ 所计算的不是 $s_{t+1}$，而是 $s_{t+N+1}$的。你会把 N 个步骤以后的状态 丢进来，去计算 N 个步骤以后，你会得到的 reward。要算目标值的话，要再加上多步(multi-step) 的 reward $\\sum_{t^{\\prime}=t}^{t+N} r_{t^{\\prime}}$ ，多步的 reward 是从时间 t 一直到 t+N 的 N 个 reward 的和。然后希望你的 $Q(s_t,a_t)$ 和目标值越接近越好。\n\n你会发现说这个方法就是 MC 跟 TD 的结合。因此它就有 MC 的好处跟坏处，也有 TD 的好处跟坏处。如果看它的这个好处的话，因为我们现在 sample 了比较多的步骤，之前是只 sample 了一个步骤， 所以某一个步骤得到的数据是 real 的，接下来都是 Q 值估测出来的。现在 sample 比较多步骤，sample N 个步骤才估测值，所以估测的部分所造成的影响就会比小。当然它的坏处就跟 MC 的坏处一样，因为你的 r 比较多项，你把 N 项的 r 加起来，方差就会比较大。但是你可以去调这个 N 的值，去在 方差 跟不精确的 Q 之间取得一个平衡。N 就是一个 hyperparameter，你要调这个 N 到底是多少，你是要多 sample 三步，还是多 sample 五步。\n\n## Noisy Net\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.9.png)\n我们还可以改进探索。Epsilon Greedy 这样的探索是在动作的空间上面加噪声，**但是有一个更好的方法叫做`Noisy Net`，它是在参数的空间上面加噪声。**\n\nNoisy Net 的意思是说，每一次在一个 episode 开始的时候，在你要跟环境互动的时候，你就把你的 Q-function 拿出来，Q-function 里面其实就是一个网络，就变成你把那个网络拿出来，在网络的每一个参数上面加上一个高斯噪声(Gaussian noise)，那你就把原来的 Q-function 变成 $\\tilde{Q}$ 。因为 $\\hat{Q}$ 已经用过，$\\hat{Q}$ 是那个目标网络，我们用 $\\tilde{Q}$  来代表一个 `Noisy Q-function`。我们把每一个参数都加上一个高斯噪声，就得到一个新的网络叫做 $\\tilde{Q}$。\n\n这边要注意在每个 episode 开始的时候，开始跟环境互动之前，我们就 sample 网络。接下来你就会用这个固定住的 noisy网络去玩这个游戏，直到游戏结束，你才重新再去 sample 新的噪声。OpenAI  跟 DeepMind 又在同时间提出了一模一样的方法，通通都发表在 ICLR 2018，两篇 paper 的方法就是一样的。不一样的地方是，他们用不同的方法，去加噪声。OpenAI 加的方法好像比较简单，他就直接加一个高斯噪声就结束了，就你把每一个参数，每一个 weight 都加一个高斯噪声就结束了。DeepMind 做比较复杂，他们的噪声是由一组参数控制的，也就是说网络可以自己决定说它那个噪声要加多大，但是概念就是一样的。总之就是把你的 Q-function 的里面的那个网络加上一些噪声，把它变得有点不一样，跟原来的 Q-function 不一样，然后拿去跟环境做互动。两篇 paper 里面都有强调说，你这个参数虽然会加噪声，但在同一个 episode 里面你的参数就是固定的，你是在换 episode， 玩第二场新的游戏的时候，你才会重新 sample 噪声，在同一场游戏里面就是同一个 noisy Q-network 在玩那一场游戏，这件事非常重要。为什么这件事非常重要呢？因为这是导致了 Noisy Net 跟原来的 Epsilon Greedy 或其它在动作做 sample 方法的本质上的差异。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.10.png)\n\n有什么样本质上的差异呢？在原来 sample 的方法，比如说 Epsilon Greedy 里面，就算是给同样的状态，你的 agent 采取的动作也不一定是一样的。因为你是用 sample 决定的，给定同一个状态，要根据 Q-function 的网络，你会得到一个动作，你 sample 到 random，你会采取另外一个 动作。所以给定同样的状态，如果你今天是用 Epsilon Greedy 的方法，它得到的动作是不一样的。但实际上你的策略并不是这样运作的啊。在一个真实世界的策略，给同样的状态，他应该会有同样的回应。而不是给同样的状态，它其实有时候吃 Q-function，然后有时候又是随机的，所以这是一个不正常的动作，是在真实的情况下不会出现的动作。但是如果你是在 Q-function 上面去加噪声的话， 就不会有这个情形。因为如果你今天在 Q-function 上加噪声，在 Q-function 的网络的参数上加噪声，那在整个互动的过程中，在同一个 episode 里面，它的网络的参数总是固定的，所以看到同样的状态，或是相似的状态，就会采取同样的动作，那这个是比较正常的。在 paper 里面有说，这个叫做 `state-dependent exploration`，也就是说你虽然会做探索这件事， 但是你的探索是跟状态有关系的，看到同样的状态， 你就会采取同样的探索的方式，而 noisy 的动作只是随机乱试。但如果你是在参数下加噪声，那在同一个 episode 里面，里面你的参数是固定的。那你就是有系统地在尝试，每次会试说，在某一个状态，我都向左试试看。然后再下一次在玩这个同样游戏的时候，看到同样的状态，你就说我再向右试试看，你是有系统地在探索这个环境。\n\n## Distributional Q-function\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.11.png)\n\n还有一个技巧叫做 `Distributional Q-function`。我们不讲它的细节，只告诉你大致的概念。Distributional Q-function 还蛮有道理的， 但是它没有红起来。你就发现说没有太多人真的在实现的时候用这个技术，可能一个原因就是它不好实现。Q-function 是累积奖励的期望值，所以我们算出来的这个 Q 值其实是一个期望值。因为环境是有随机性的，在某一个状态 采取某一个动作的时候，我们把所有的 reward 玩到游戏结束的时候所有的 reward 进行一个统计，你其实得到的是一个分布。也许在 reward 得到 0 的机率很高，在 -10 的概率比较低，在 +10 的概率比较低，但是它是一个分布。我们对这一个分布算它的平均值才是这个 Q 值，我们算出来是累积奖励的期望。所以累积奖励是一个分布，对它取期望，对它取平均值，你得到了 Q 值。但不同的分布，它们其实可以有同样的平均值。也许真正的分布是右边的分布，它算出来的平均值跟左边的分布 算出来的平均值 其实是一样的，但它们背后所代表的分布其实是不一样的。假设我们只用 Q 值的期望来代表整个 reward 的话，其实可能会丢失一些信息，你没有办法 model reward 的分布。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.12.png)\n\nDistributional Q-function 它想要做的事情是对分布(distribution)建模，怎么做呢？在原来的 Q-function 里面，假设你只能够采取 $a_1$, $a_2$, $a_3$ 3 个动作，那你就是输入一个状态，输出 3 个值。3 个值分别代表 3 个动作的 Q 值，但是这个 Q 值是一个分布的期望值。所以 Distributional Q-function 的想法就是何不直接输出那个分布。但是要直接输出一个分布也不知道怎么做。\n\n实际上的做法是说， 假设分布的值就分布在某一个 range 里面，比如说 -10 到 10，那把 -10 到 10 中间拆成一个一个的 bin，拆成一个一个的长条图。举例来说，在这个例子里面，每一个动作的 reward 的空间就拆成 5 个 bin。假设 reward 可以拆成 5 个 bin 的话，今天你的 Q-function 的输出是要预测说，你在某一个状态，采取某一个动作，你得到的 reward，落在某一个 bin 里面的概率。\n\n所以其实这边的概率的和，这些绿色的 bar 的和应该是 1，它的高度代表说，在某一个状态 采取某一个动作的时候，它落在某一个 bin 的机率。这边绿色的代表动作 1，红色的代表动作 2，蓝色的代表动作 3。所以今天你就可以真的用 Q-function 去估计 $a_1$ 的分布，$a_2$ 的分布，$a_3$ 的分布。那实际上在做测试的时候， 我们还是要选某一个动作去执行，那选哪一个动作呢？实际上在做的时候，还是选这个平均值最大的那个动作去执行。\n\n但假设我们可以对 distribution 建模的话，除了选平均值最大的以外，也许在未来你可以有更多其他的运用。举例来说，你可以考虑它的分布长什么样子。如果分布方差很大，代表说采取这个动作虽然平均值可能平均而言很不错，但也许风险很高，你可以训练一个网络它是可以规避风险的。就在 2 个动作平均值都差不多的情况下，也许可以选一个风险比较小的动作来执行，这是 Distributional Q-function 的好处。关于怎么训练这样的 Q-network 的细节，我们就不讲，你只要记得说  Q-network 有办法输出一个分布就对了。我们可以不只是估测得到的期望 reward 平均值的值。我们其实是可以估测一个分布的。\n\n## Rainbow\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.13.png)\n\n**最后一个技巧叫做 rainbow，把刚才所有的方法都综合起来就变成 rainbow 。**因为刚才每一个方法，就是有一种自己的颜色，把所有的颜色通通都合起来，就变成 rainbow，它把原来的 DQN 也算是一种方法，故有 7 色。\n\n那我们来看看这些不同的方法。横轴是训练过程，纵轴是玩了 10 几个 Atari 小游戏的平均的分数的和，但它取的是中位数的分数，为什么是取中位数不是直接取平均呢？因为它说每一个小游戏的分数，其实差很多。如果你取平均的话，到时候某几个游戏就控制了你的结果，所以它取中位数的值。\n\n如果你是一般的 DQN，就灰色这一条线，就没有很强。那如果是你换 Noisy DQN，就强很多。如果这边每一个单一颜色的线是代表说只用某一个方法，那紫色这一条线是 DDQN(Double DQN)，DDQN 还蛮有效的。然后 Prioritized DDQN、Dueling DDQN 和 Distributional DQN 都蛮强的，它们都差不多很强的。A3C 其实是 Actor-Critic 的方法。单纯的 A3C 看起来是比 DQN 强的。这边怎么没有多步的方法，多步的方法就是平衡 TD 跟 MC，我猜是因为 A3C 本身内部就有做多步的方法，所以他可能觉得说有实现 A3C 就算是有实现多步的方法。所以可以把这个 A3C 的结果想成是多步方法的结果。其实这些方法他们本身之间是没有冲突的，所以全部都用上去就变成七彩的一个方法，就叫做 rainbow，然后它很高。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/7.14.png)\n\n上图是说，在 rainbow 这个方法里面， 如果我们每次拿掉其中一个技术，到底差多少。因为现在是把所有的方法都加在一起，发现说进步很多，但会不会有些方法其实是没用的。所以看看说， 每一个方法哪些方法特别有用，哪些方法特别没用。这边的虚线就是拿掉某一种方法以后的结果，你会发现说，黄色的虚线，拿掉多步掉很多。Rainbow 是彩色这一条，拿掉多步会掉下来。拿掉 Prioritized  Experience Replay 后也马上就掉下来。拿掉分布，它也掉下来。\n\n这边有一个有趣的地方是说，在开始的时候，分布的训练的方法跟其他方法速度差不多。但是如果你拿掉分布的时候，你的训练不会变慢，但是性能(performance)最后会收敛在比较差的地方。拿掉 Noisy Net 后性能也是差一点。拿掉 Dueling 也是差一点。拿掉 Double 没什么差，所以看来全部合在一起的时候，Double 是比较没有影响的。其实在 paper 里面有给一个 make sense 的解释，其实当你有用 Distributional DQN 的时候，本质上就不会高估你的 reward。我们是为了避免高估 reward 才加了 Double DQN。那在 paper 里面有讲说，如果有做 Distributional DQN，就比较不会有高估的结果。 事实上他有真的算了一下发现说，其实多数的状况是低估 reward 的，所以变成 Double DQN 没有用。\n\n为什么做 Distributional DQN，不会高估 reward，反而会低估 reward 呢？因为这个 Distributional DQN 的输出的是一个分布的范围，输出的范围不可能是无限宽的，你一定是设一个范围， 比如说最大输出范围就是从 -10 到 10。假设今天得到的 reward 超过 10 怎么办？是 100 怎么办，就当作没看到这件事。所以 reward 很极端的值，很大的值其实是会被丢掉的， 所以用 Distributional DQN 的时候，你不会有高估的现象，反而会低估。','2021-12-10 12:43:49','2021-12-19 19:39:48'),
	(48,3,'第七章 习题','## 1 Keywords\r\n\r\n- **Double DQN：** 在Double DQN中存在有两个 Q-network，首先，第一个 Q-network，决定的是哪一个 action 的 Q value 最大，从而决定了你的action。另一方面， Q value 是用 $Q\'$ 算出来的，这样就可以避免 over estimate 的问题。具体来说，假设我们有两个 Q-function，假设第一个Q-function 它高估了它现在选出来的action a，那没关系，只要第二个Q-function $Q\'$ 没有高估这个action a 的值，那你算出来的，就还是正常的值。\r\n- **Dueling DQN：** 将原来的DQN的计算过程分为**两个path**。对于第一个path，会计算一个于input state有关的一个标量 $V(s)$；对于第二个path，会计算出一个vector $A(s,a)$ ，其对应每一个action。最后的网络是将两个path的结果相加，得到我们最终需要的Q value。用一个公式表示也就是 $Q(s,a)=V(s)+A(s,a)$ 。 \r\n- **Prioritized Experience Replay （优先经验回放）：** 这个方法是为了解决我们在chapter6中提出的**Experience Replay（经验回放）**方法不足进一步优化提出的。我们在使用Experience Replay时是uniformly取出的experience buffer中的sample data，这里并没有考虑数据间的权重大小。例如，我们应该将那些train的效果不好的data对应的权重加大，即其应该有更大的概率被sample到。综上， prioritized experience replay 不仅改变了 sample data 的 distribution，还改变了 training process。\r\n- **Noisy Net：** 其在每一个episode 开始的时候，即要和环境互动的时候，将原来的Q-function 的每一个参数上面加上一个Gaussian noise。那你就把原来的Q-function 变成$\\tilde{Q}$ ，即**Noisy Q-function**。同样的我们把每一个network的权重等参数都加上一个Gaussian noise，就得到一个新的network $\\tilde{Q}$。我们会使用这个新的network从与环境互动开始到互动结束。\r\n- **Distributional Q-function：** 对于DQN进行model distribution。将最终的网络的output的每一类别的action再进行distribution。\r\n- **Rainbow：** 也就是将我们这两节内容所有的七个tips综合起来的方法，7个方法分别包括：DQN、DDQN、Prioritized DDQN、Dueling DDQN、A3C、Distributional DQN、Noisy DQN，进而考察每一个方法的贡献度或者是否对于与环境的交互式正反馈的。\r\n\r\n## 2 Questions\r\n\r\n- 为什么传统的DQN的效果并不好？参考公式 $Q(s_t ,a_t)=r_t+\\max_{a}Q(s_{t+1},a)$ \r\n\r\n  答：因为实际上在做的时候，是要让左边这个式子跟右边这个 target 越接近越好。比较容易可以发现target 的值很容易一不小心就被设得太高。因为在算这个 target 的时候，我们实际上在做的事情是看哪一个a 可以得到最大的Q value，就把它加上去，就变成我们的target。\r\n\r\n  举例来说，现在有 4 个 actions，本来其实它们得到的值都是差不多的，它们得到的reward 都是差不多的。但是在estimate 的时候，那毕竟是个network。所以estimate 的时候是有误差的。所以假设今天是第一个action它被高估了，假设绿色的东西代表是被高估的量，它被高估了，那这个target 就会选这个action。然后就会选这个高估的Q value来加上$r_t$，来当作你的target。如果第4 个action 被高估了，那就会选第4 个action 来加上$r_t$ 来当作你的target value。所以你总是会选那个Q value 被高估的，你总是会选那个reward 被高估的action 当作这个max 的结果去加上$r_t$ 当作你的target。所以你的target 总是太大。\r\n\r\n- 接着上个思考题，我们应该怎么解决target 总是太大的问题呢？\r\n\r\n  答： 我们可以使用Double DQN解决这个问题。首先，在 Double DQN 里面，选 action 的 Q-function 跟算 value 的 Q-function不同。在原来的DQN 里面，你穷举所有的 a，把每一个a 都带进去， 看哪一个 a 可以给你的 Q value 最高，那你就把那个 Q value 加上$r_t$。但是在 Double DQN 里面，你**有两个 Q-network**，第一个 Q-network，决定哪一个 action 的 Q value 最大，你用第一个 Q-network 去带入所有的 a，去看看哪一个Q value 最大。然后你决定你的action 以后，你的 Q value 是用 $Q\'$ 算出来的，这样子有什么好处呢？为什么这样就可以避免 over estimate 的问题呢？因为今天假设我们有两个 Q-function，假设第一个Q-function 它高估了它现在选出来的action a，那没关系，只要第二个Q-function $Q\'$ 没有高估这个action a 的值，那你算出来的，就还是正常的值。假设反过来是 $Q\'$ 高估了某一个action 的值，那也没差， 因为反正只要前面这个Q 不要选那个action 出来就没事了。\r\n\r\n- 哪来 Q  跟 $Q\'$ 呢？哪来两个 network 呢？\r\n\r\n  答：在实现上，你有两个 Q-network， 一个是 target 的 Q-network，一个是真正你会 update 的 Q-network。所以在 Double DQN 里面，你的实现方法会是拿你会 update 参数的那个 Q-network 去选action，然后你拿target 的network，那个固定住不动的network 去算value。而 Double DQN 相较于原来的 DQN 的更改是最少的，它几乎没有增加任何的运算量，连新的network 都不用，因为你原来就有两个network 了。你唯一要做的事情只有，本来你在找最大的a 的时候，你在决定这个a 要放哪一个的时候，你是用$Q\'$ 来算，你是用target network 来算，现在改成用另外一个会 update 的 Q-network 来算。\r\n\r\n- 如何理解Dueling DQN的模型变化带来的好处？\r\n\r\n  答：对于我们的 $Q(s,a)$ 其对应的state由于为table的形式，所以是离散的，而实际中的state不是离散的。对于 $Q(s,a)$ 的计算公式， $Q(s,a)=V(s)+A(s,a)$ 。其中的 $V(s)$ 是对于不同的state都有值，对于 $A(s,a)$ 对于不同的state都有不同的action对应的值。所以本质上来说，我们最终的矩阵 $Q(s,a)$ 的结果是将每一个 $V(s)$ 加到矩阵 $A(s,a)$ 中得到的。从模型的角度考虑，我们的network直接改变的 $Q(s,a)$ 而是 更改的 $V、A$ 。但是有时我们update时不一定会将 $V(s)$ 和 $Q(s,a)$ 都更新。我们将其分成两个path后，我们就不需要将所有的state-action pair都sample一遍，我们可以使用更高效的estimate Q value方法将最终的 $Q(s,a)$ 计算出来。\r\n\r\n- 使用MC和TD平衡方法的优劣分别有哪些？\r\n\r\n  答：\r\n\r\n  - 优势：因为我们现在 sample 了比较多的step，之前是只sample 了一个step， 所以某一个step 得到的data 是真实值，接下来都是Q value 估测出来的。现在sample 比较多step，sample N 个step 才估测value，所以估测的部分所造成的影响就会比小。\r\n  - 劣势：因为我们的 reward 比较多，当我们把 N 步的 reward 加起来，对应的 variance 就会比较大。但是我们可以选择通过调整 N 值，去在variance 跟不精确的 Q 之间取得一个平衡。这里介绍的参数 N 就是一个hyper parameter，你要调这个N 到底是多少，你是要多 sample 三步，还是多 sample 五步。\r\n\r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：DQN都有哪些变种？引入状态奖励的是哪种？\r\n\r\n  答：DQN三个经典的变种：Double DQN、Dueling DQN、Prioritized Replay Buffer。\r\n\r\n  - Double-DQN：将动作选择和价值估计分开，避免价值过高估计。\r\n  - Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息。\r\n  - Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样。\r\n\r\n- 简述double DQN原理？\r\n\r\n  答：DQN由于总是选择当前值函数最大的动作值函数来更新当前的动作值函数，因此存在着过估计问题（估计的值函数大于真实的值函数）。为了解耦这两个过程，double DQN 使用了两个值网络，一个网络用来执行动作选择，然后用另一个值函数对一个的动作值更新当前网络。\r\n\r\n- 高冷的面试官：请问Dueling DQN模型有什么优势呢？\r\n\r\n  答：对于我们的 $Q(s,a)$ 其对应的state由于为table的形式，所以是离散的，而实际中的state不是离散的。对于 $Q(s,a)$ 的计算公式， $Q(s,a)=V(s)+A(s,a)$ 。其中的 $V(s)$ 是对于不同的state都有值，对于 $A(s,a)$ 对于不同的state都有不同的action对应的值。所以本质上来说，我们最终的矩阵 $Q(s,a)$ 的结果是将每一个 $V(s)$ 加到矩阵 $A(s,a)$ 中得到的。从模型的角度考虑，我们的network直接改变的 $Q(s,a)$ 而是更改的 $V、A$ 。但是有时我们update时不一定会将 $V(s)$ 和 $Q(s,a)$ 都更新。我们将其分成两个path后，我们就不需要将所有的state-action pair都sample一遍，我们可以使用更高效的estimate Q value方法将最终的 $Q(s,a)$ 计算出来。\r\n','2021-12-10 12:43:49','2021-12-19 19:39:52'),
	(49,3,'项目二 使用DQN实现CartPole-v0','## 使用DQN实现CartPole-v0\n\n推荐使用Double-DQN去解决，即建立两个初始参数相同的全连接网络target_net和policy_net。\n\n## CartPole-v0\n\nCartPole-v0是OpenAI gym中的一个经典环境，通过向左(action=0)或向右(action=1)推车能够实现平衡，所以动作空间由两个动作组成。每进行一个step就会给一个+1的reward，如果无法保持平衡那么done等于true，本次episode失败。\n\n**理想状态下，每个episode至少能进行200个step，也就是说每个episode的reward总和至少为200，step数目至少为200**。\n\n![p1](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/p1.png)\n\n环境建立如下：\n\n```python\nenv = gym.make(\'CartPole-v0\') \nenv.seed(1) # 设置env随机种子\nn_states = env.observation_space.shape[0] # 获取总的状态数\nn_actions = env.action_space.n # 获取总的动作数\n```\n\n## 强化学习基本接口\n\n```python\nrewards = [] # 记录总的rewards\nmoving_average_rewards = [] # 记录总的经滑动平均处理后的rewards\nep_steps = []\nfor i_episode in range(1, cfg.max_episodes+1): # cfg.max_episodes为最大训练的episode数\n    state = env.reset() # reset环境状态\n    ep_reward = 0\n    for i_step in range(1, cfg.max_steps+1): # cfg.max_steps为每个episode的补偿\n        action = agent.select_action(state) # 根据当前环境state选择action\n        next_state, reward, done, _ = env.step(action) # 更新环境参数\n        ep_reward += reward\n        agent.memory.push(state, action, reward, next_state, done) # 将state等这些transition存入memory\n        state = next_state # 跳转到下一个状态\n        agent.update() # 每步更新网络\n        if done:\n            break\n    # 更新target network，复制DQN中的所有weights and biases\n    if i_episode % cfg.target_update == 0: #  cfg.target_update为target_net的更新频率\n        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n    print(\'Episode:\', i_episode, \' Reward: %i\' %\n          int(ep_reward), \'n_steps:\', i_step, \'done: \', done,\' Explore: %.2f\' % agent.epsilon)\n    ep_steps.append(i_step)\n    rewards.append(ep_reward)\n    # 计算滑动窗口的reward\n    if i_episode == 1:\n        moving_average_rewards.append(ep_reward)\n    else:\n        moving_average_rewards.append(\n            0.9*moving_average_rewards[-1]+0.1*ep_reward)\n```\n\n## 任务要求\n\n训练并绘制reward以及滑动平均后的reward随episode的变化曲线图并记录超参数写成报告，图示如下：\n\n![rewards_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/rewards_train.png)\n\n![moving_average_rewards_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/moving_average_rewards_train.png)\n\n![steps_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/steps_train.png)\n\n同时也可以绘制测试(eval)模型时的曲线：\n\n![rewards_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/rewards_eval.png)\n\n![moving_average_rewards_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/moving_average_rewards_eval.png)\n\n![steps_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/steps_eval.png)\n\n也可以[tensorboard](https://pytorch.org/docs/stable/tensorboard.html)查看结果，如下：\n\n![image-20201015221032985](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/image-20201015221032985.png)\n\n### 代码清单\n\n**main.py**：保存强化学习基本接口，以及相应的超参数，可使用argparse\n\n**model.py**：保存神经网络，比如全链接网络\n\n**dqn.py**: 保存算法模型，主要包含select_action和update两个函数\n\n**memory.py**：保存Replay Buffer\n\n**plot.py**：保存相关绘制函数，可选\n\n[参考代码](https://github.com/datawhalechina/easy-rl/tree/master/codes/DQN)','2021-12-10 12:43:49','2021-12-19 19:40:10'),
	(50,3,'第八章 连续动作','## 方案 1 & 方案 2\n跟基于策略梯度的方法比起来，DQN 是比较稳的。策略梯度是没有太多游戏是玩得起来的，策略梯度比较不稳，在没有 近端策略优化 之前，我们很难用策略梯度做什么事情。DQN 相对而言是比较稳的。最早 DeepMind 的论文拿深度强化学习来玩雅达利的游戏，用的就是 DQN。DQN 比较容易训练的一个理由是：在 DQN 里面，你只要能够估计出Q函数，就保证你一定可以找到一个比较好的策略。也就是你只要能够估计出Q函数，就保证你可以改进策略。而估计Q函数这件事情，是比较容易的，因为它就是一个回归问题。在回归问题里面， 你可以轻易地知道模型学习得是不是越来越好，只要看那个回归的损失有没有下降，你就知道说模型学习得好不好，所以估计Q函数相较于学习一个策略是比较容易的。你只要估计Q函数，就可以保证说现在一定会得到比较好的策略。所以一般而言 DQN 比较容易操作。\n\nDQN 其实存在一些问题，最大的问题是它不太容易处理连续动作。很多时候动作是连续的，比如我们玩雅达利的游戏，智能体只需要决定比如说上下左右，这种动作是离散的。那很多时候动作是连续的。举例来说假设智能体要做的事情是开自驾车，它要决定说它方向盘要左转几度， 右转几度，这是连续的。假设智能体是一个机器人，它身上有 50 个 关节，它的每一个动作就对应到它身上的这 50 个关节的角度。而那些角度也是连续的。所以很多时候动作并不是一个离散的东西，它是一个向量。在这个向量里面，它的每一个维度都有一个对应的值，都是实数，它是连续的。假设动作是连续的，做 DQN 就会有困难。因为在做 DQN 里面一个很重要的一步是你要能够解这个优化问题。估计出 Q函数$Q(s,a)$ 以后，必须要找到一个 $a$，它可以让 $Q(s,a)$ 最大，如下式所示。\n\n$$\n  a=\\arg \\max _{a} Q(s, a)\n$$\n\n假设$a$是离散的，即$a$的可能性都是有限的。举例来说，雅达利的小游戏里面，$a$ 就是上下左右跟开火，它是有限的，我们可以把每一个可能的动作都带到 Q 里面算它的 Q 值。但假如$a$是连续的，你无法穷举所有可能的连续动作，试试看哪一个连续动作可以让 Q 的值最大。\n\n怎么解这个问题呢？就有各种不同的方案。\n\n第一个方案是假设你不知道怎么解这个问题，因为$a$是没有办法穷举的，怎么办？我们可以采样出 $N$ 个可能的 $a$：$\\left\\{a_{1}, a_{2}, \\cdots, a_{N}\\right\\}$ ，一个一个带到 Q函数里面，看谁最大。这个方法其实也不会太不高效， 因为你在运算的时候会使用 GPU，一次会把 $N$ 个连续动作都丢到 Q函数里面，一次得到 $N$ 个 Q 值，然后看谁最大。当然这不是一个非常精确的做法，因为你没有办法做太多的采样， 所以你估计出来的 Q 值，最后决定的动作可能不是非常的精确，这是第一个方案。\n\n第二个方案是什么呢？既然要解的是一个优化问题（optimization problem），其实是要最大化目标函数（objective function），要最大化一个东西， 就可以用梯度上升。我们就把$a$当作是参数，然后要找一组$a$去最大化Q函数，就用梯度上升去更新 $a$ 的值，最后看看能不能找到一个$a$去最大化Q函数，也就是目标函数。当然这样子你会遇到全局最大值（global maximum）的问题， 就不见得能够真的找到最优的结果，而且这个运算量显然很大， 因为你要迭代地更新 $a$。我们训练一个网络就很花时间了。如果你用梯度上升的方法来处理连续的问题， 等于是你每次要决定采取哪一个动作的时候，都还要做一次训练网络的过程，显然运算量是很大的。这是第二个方案。\n\n\n## 方案 3：设计网络\n\n第三个方案是特别设计一个网络的架构，特别设计Q函数，使得解 arg max 的问题变得非常容易。也就是这边的Q函数不是一个一般的Q函数，特别设计一下它的样子，让你要找让这个Q函数最大的 $a$ 的时候非常容易。\n\n下图是一个例子，这边有Q函数，这个Q函数的做法是这样。\n  通常输入状态 $s$ 就是一个图像，可以用一个向量或一个矩阵来表示。\n  输入 $s$，Q函数会输出 3 个东西。它会输出 $\\mu(s)$，这是一个向量。它会输出 $\\Sigma(s)$ ，这是一个矩阵。它会输出 $V(s)$，这是一个标量。\n  输出这 3 个东西以后，我们知道Q函数其实是吃一个$s$跟 $a$，然后决定一个值。Q函数意思是说在某一个状态，采取某一个动作的时候，你期望的奖励有多大。到目前为止这个Q函数只吃 $s$，它还没有吃$a$进来，$a$ 在哪里呢？当这个Q函数吐出 $\\mu$、 $\\Sigma$ 跟 $V$ 的时候，我们才把$a$引入，用$a$跟 $\\mu(s)、\\Sigma(s)、V$  互相作用一下，你才算出最终的 Q 值。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/8.2.png)\n\n $a$怎么和这 3 个东西互相作用呢？实际上 $Q(s,a)$，Q函数的运作方式是先输入 $s$，让你得到 $\\mu,\\Sigma$ 跟 $V$。然后再输入 $a$，然后接下来把$a$跟 $\\mu$ 相减。注意一下$a$现在是连续的动作，所以它也是一个向量。假设你现在是要操作机器人的话，这个向量的每一个维度，可能就对应到机器人的某一个关节，它的数值就是关节的角度，所以$a$是一个向量。把向量 $a$ 减掉向量 $\\mu$，取转置，所以它是一个横的向量。$\\Sigma$ 是一个矩阵。然后$a$减掉 $\\mu(s)$ ，$a$ 和 $\\mu(s)$ 都是向量，减掉以后还是一个竖的向量。所以 $-(a-\\mu(s))^{T} \\Sigma(s)(a-\\mu(s))+V(s)$ 是一个标量，这个数值就是 Q 值 $Q(s,a)$。\n\n  假设 $Q(s,a)$ 定义成这个样子，我们要怎么找到一个$a$去最大化这个 Q 值呢？这个方案非常简单。因为 $(a-\\mu(s))^{T} \\Sigma(s)(a-\\mu(s))$ 一定是正的，它前面乘上一个负号，所以第一项就假设我们不看这个负号的话，第一项的值越小，最终的 Q 值就越大。因为我们是把 $V(s)$ 减掉第一项，所以第一项的值越小，最后的 Q 值就越大。怎么让第一项的值最小呢？你直接把$a$代入 $\\mu$ 的值，让它变成 0，就会让第一项的值最小。\n\n  $\\Sigma$ 一定是正定的。因为这个东西就像是高斯分布（Gaussian distribution），所以 $\\mu$ 就是高斯分布的均值，$\\Sigma$ 就是高斯分布的方差。但方差是一个正定（positive definite）的矩阵，怎么样让这个 $\\Sigma$ 一定是正定的矩阵呢？其实在 $Q^{\\pi}$ 里面，它不是直接输出 $\\Sigma$，如果直接输出 一个 $\\Sigma$， 它不一定是正定的矩阵。它其实是输出 一个矩阵，然后再把那个矩阵跟另外一个矩阵做转置相乘， 然后可以确保 $\\Sigma$ 是正定的。这边要强调的点就是说，实际上它不是直接输出一个矩阵。你再去那个论文里面查看一下它的技巧，它可以保证说 $\\Sigma$ 是正定的。\n\n  你把$a$代入 $\\mu(s)$ 以后，你可以让 Q 的值最大。所以假设要你 arg max Q 函数，如下式所示。\n$$\n\\mu(s)=\\arg \\max _{a} Q(s, a)\n$$\n\n虽然一般而言，若 Q 是一个一般的函数， 你很难算，但是我们这边设计了 Q 这个函数，$a$ 只要设 $\\mu(s)$，我们就得到最大值。你在解这个 arg max 的问题的时候就变得非常容易。所以 DQN 也可以用在连续的情况，只是有一些局限，就是函数不能够随便乱设，它必须有一些限制。\n\n##  方案 4：不使用DQN\n第 4 招就是不要用 DQN。用 DQN 处理连续动作还是比较麻烦。\n基于策略的方法 PPO 和基于价值的方法 DQN，这两者其实是可以结合在一起的，如下图所示，也就是演员-评论员的方法。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/8.3.png)\n','2021-12-10 12:43:49','2021-12-19 19:40:48'),
	(51,3,'第八章 习题','## Questions\r\n\r\n- Q-learning相比于policy gradient based方法为什么训练起来效果更好，更平稳？\r\n\r\n  答：在 Q-learning 中，只要能够 estimate 出Q-function，就可以保证找到一个比较好的 policy，同样的只要能够 estimate 出 Q-function，就保证可以 improve 对应的 policy。而因为 estimate Q-function 作为一个回归问题，是比较容易的。在这个回归问题中， 我们可以时刻观察我们的模型训练的效果是不是越来越好，一般情况下我们只需要关注 regression 的 loss 有没有下降，你就知道你的 model learn 的好不好。所以 estimate Q-function 相较于 learn 一个 policy 是比较容易的。你只要 estimate Q-function，就可以保证说现在一定会得到比较好的 policy，同样其也比较容易操作。\r\n\r\n- Q-learning在处理continuous action时存在什么样的问题呢？\r\n\r\n  答：在日常的问题中，我们的问题都是continuous action的，例如我们的 agent 要做的事情是开自驾车，它要决定说它方向盘要左转几度， 右转几度，这就是 continuous 的；假设我们的 agent 是一个机器人，假设它身上有 50 个关节，它的每一个 action 就对应到它身上的这 50 个关节的角度，而那些角度也是 continuous 的。\r\n\r\n  然而在解决Q-learning问题时，很重要的一步是要求能够解对应的优化问题。当我们 estimate 出Q-function $Q(s,a)$ 以后,必须要找到一个 action，它可以让 $Q(s,a)$ 最大。假设 action 是 discrete 的，那 a 的可能性都是有限的。但如果action是continuous的情况下，我们就不能像离散的action一样，穷举所有可能的continuous action了。\r\n\r\n  为了解决这个问题，有以下几种solutions：\r\n\r\n  - 第一个解决方法：我们可以使用所谓的sample方法，即随机sample出N个可能的action，然后一个一个带到我们的Q-function中，计算对应的N个Q value比较哪一个的值最大。但是这个方法因为是sample所以不会非常的精确。\r\n  - 第二个解决方法：我们将这个continuous action问题，看为一个优化问题，从而自然而然地想到了可以用gradient ascend去最大化我们的目标函数。具体地，我们将action看为我们的变量，使用gradient ascend方法去update action对应的Q-value。但是这个方法通常的时间花销比较大，因为是需要迭代运算的。\r\n  - 第三个解决方法：设计一个特别的network架构，设计一个特别的Q-function，使得解我们 argmax Q-value的问题变得非常容易。也就是这边的 Q-function 不是一个 general 的 Q-function，特别设计一下它的样子，让你要找让这个 Q-function 最大的 a 的时候非常容易。但是这个方法的function不能随意乱设，其必须有一些额外的限制。具体的设计方法，可以我们的chapter8的详细教程。\r\n  - 第四个解决方法：不用Q-learning，毕竟用其处理continuous的action比较麻烦。\r\n','2021-12-10 12:43:49','2021-12-19 19:40:53'),
	(52,3,'第九章 演员-评论家算法','## Actor-Critic\n\n在 REINFORCE 算法中，每次需要根据一个策略采集一条完整的轨迹，并计算这条轨迹上的回报。这种采样方式的方差比较大，学习效率也比较低。我们可以借鉴时序差分学习的思想，使用动态规划方法来提高采样的效率，即从状态 $s$ 开始的总回报可以通过当前动作的即时奖励 $r(s,a,s\')$ 和下一个状态 $s\'$ 的值函数来近似估计。\n\n`演员-评论家算法(Actor-Critic Algorithm)`是一种结合`策略梯度`和`时序差分学习`的强化学习方法，其中：\n\n* 演员(Actor)是指策略函数 $\\pi_{\\theta}(a|s)$，即学习一个策略来得到尽量高的回报。\n* 评论家(Critic)是指值函数 $V^{\\pi}(s)$，对当前策略的值函数进行估计，即评估演员的好坏。\n* 借助于值函数，演员-评论家算法可以进行单步更新参数，不需要等到回合结束才进行更新。\n\n在  Actor-Critic 算法 里面，最知名的方法就是 `A3C(Asynchronous Advantage Actor-Critic)`。\n\n* 如果去掉 Asynchronous，只有 `Advantage Actor-Critic`，就叫做 `A2C`。\n* 如果加了 Asynchronous，变成 `Asynchronous Advantage Actor-Critic`，就变成 `A3C`。\n\n### Review: Policy Gradient\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.1.png)\n\n那我们复习一下 policy gradient，在 policy gradient，我们在更新 policy 的参数 $\\theta$ 的时候，我们是用了下面这个式子来算出 gradient。\n$$\n\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n这个式子是在说，我们先让 agent 去跟环境互动一下，那我们可以计算出在某一个状态 s，采取了某一个动作 a 的概率  $p_{\\theta}(a_t|s_t)$。接下来，我们去计算在某一个状态 s 采取了某一个动作 a 之后，到游戏结束为止，累积奖励有多大。我们把这些奖励从时间 t 到时间 T 的奖励通通加起来，并且会在前面乘一个折扣因子，可能设 0.9 或 0.99。我们会减掉一个 baseline b，减掉这个值 b 的目的，是希望括号这里面这一项是有正有负的。如果括号里面这一项是正的，我们就要增加在这个状态采取这个动作的机率；如果括号里面是负的，我们就要减少在这个状态采取这个动作的机率。\n\n我们把用 G 来表示累积奖励。但 G 这个值，其实是非常不稳定的。因为互动的过程本身是有随机性的，所以在某一个状态 s 采取某一个动作 a，然后计算累积奖励，每次算出来的结果都是不一样的，所以 G 其实是一个随机变量。给同样的状态 s，给同样的动作 a，G 可能有一个固定的分布。但我们是采取采样的方式，我们在某一个状态 s 采取某一个动作 a，然后玩到底，我们看看得到多少的奖励，我们就把这个东西当作 G。\n\n把 G 想成是一个随机变量的话，我们实际上是对这个 G 做一些采样，然后拿这些采样的结果，去更新我们的参数。但实际上在某一个状态 s 采取某一个动作 a，接下来会发生什么事，它本身是有随机性的。虽然说有个固定的分布，但它本身是有随机性的，而这个随机变量的方差可能会非常大。你在同一个状态采取同一个动作，你最后得到的结果可能会是天差地远的。\n\n假设我们可以采样足够的次数，在每次更新参数之前，我们都可以采样足够的次数，那其实没有什么问题。但问题就是我们每次做 policy gradient，每次更新参数之前都要做一些采样，这个采样的次数其实是不可能太多的，我们只能够做非常少量的采样。如果你正好采样到差的结果，比如说你采样到 G = 100，采样到 G = -10，那显然你的结果会是很差的。\n\n### Review: Q-learning\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.2.png)\n\nQ: 能不能让整个训练过程变得比较稳定一点，能不能够直接估测 G 这个随机变量的期望值？\n\nA: 我们在状态 s 采取动作 a 的时候，直接用一个网络去估测在状态 s 采取动作 a 的时候，G 的期望值。如果这件事情是可行的，那之后训练的时候，就用期望值来代替采样的值，这样会让训练变得比较稳定。\n\nQ: 怎么拿期望值代替采样的值呢？\n\nA: 这边就需要引入基于价值的(value-based)的方法。基于价值的方法就是 Q-learning。Q-learning 有两种函数，有两种 critics。\n\n* 第一种 critic 是 $V^{\\pi}(s)$，它的意思是说，假设 actor 是 $\\pi$，拿 $\\pi$ 去跟环境做互动，当我们看到状态 s 的时候，接下来累积奖励 的期望值有多少。\n* 还有一个 critic 是 $Q^{\\pi}(s,a)$。$Q^{\\pi}(s,a)$ 把 s 跟 a 当作输入，它的意思是说，在状态 s 采取动作 a，接下来都用 actor $\\pi$ 来跟环境进行互动，累积奖励的期望值是多少。\n\n* $V^{\\pi}$ 输入 s，输出一个标量。\n\n* $Q^{\\pi}$ 输入 s，然后它会给每一个 a 都分配一个 Q value。\n\n* 你可以用  TD 或 MC 来估计。用 TD 比较稳，用 MC 比较精确。\n\n### Actor-Critic\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.3.png)\n\n随机变量 $G$ 的期望值正好就是 Q ，即\n$$\nE\\left[G_{t}^{n}\\right]=Q^{\\pi_{\\theta}} \\left(s_{t}^{n}, a_{t}^{n}\\right)\n$$\n\n因为这个就是 Q 的定义。Q-function 的定义就是在某一个状态 s，采取某一个动作 a，假设 policy 就是 $\\pi$ 的情况下会得到的累积奖励的期望值有多大，而这个东西就是 G 的期望值。累积奖励的期望值就是 G 的期望值。\n\n所以假设用 $E\\left[G_{t}^{n}\\right]$ 来代表 $\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}$ 这一项的话，把 Q-function 套在这里就结束了，我们就可以把 Actor 跟 Critic 这两个方法结合起来。\n\n有不同的方法来表示 baseline，但一个常见的做法是用价值函数 $V^{\\pi_{\\theta}}\\left(s_{t}^{n}\\right)$ 来表示 baseline。价值函数是说，假设 policy 是 $\\pi$，在某一个状态 s 一直互动到游戏结束，期望奖励(expected reward)有多大。 $V^{\\pi_{\\theta}}\\left(s_{t}^{n}\\right)$ 没有涉及到动作，$Q^{\\pi_{\\theta}}\\left(s_{t}^{n}, a_{t}^{n}\\right)$ 涉及到动作。\n\n其实 $V^{\\pi_{\\theta}}\\left(s_{t}^{n}\\right)$ 会是 $Q^{\\pi_{\\theta}}\\left(s_{t}^{n}, a_{t}^{n}\\right)$ 的期望值，所以 $Q^{\\pi_{\\theta}}\\left(s_{t}^{n}, a_{t}^{n}\\right)-V^{\\pi_{\\theta}}\\left(s_{t}^{n}\\right)$ 会有正有负，所以 $\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b$ 这一项就会是有正有负的。\n\n所以我们就把 policy gradient 里面 $\\sum_{t^{\\prime}=t}^{T_{n}} \\gamma^{t^{\\prime}-t} r_{t^{\\prime}}^{n}-b$ 这一项换成了 $Q^{\\pi_{\\theta}}\\left(s_{t}^{n}, a_{t}^{n}\\right)-V^{\\pi_{\\theta}}\\left(s_{t}^{n}\\right)$。\n\n### Advantage Actor-Critic\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.4.png)\n\n如果你这么实现的话，有一个缺点是：你要估计 2 个 网络：Q-network 和 V-network，你估测不准的风险就变成两倍。所以我们何不只估测一个网络？\n\n事实上在这个 Actor-Critic 方法里面。你可以只估测 V 这个网络，你可以用 V 的值来表示 Q 的值，$Q^{\\pi}\\left(s_{t}^{n}, a_{t}^{n}\\right)$ 可以写成 $ r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)$ 的期望值，即\n$$\nQ^{\\pi}\\left(s_{t}^{n}, a_{t}^{n}\\right)=E\\left[r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)\\right]\n$$\n\n你在状态 s 采取动作 a，会得到奖励 r，然后跳到状态 $s_{t+1}$。但是你会得到什么样的奖励 r，跳到什么样的状态 $s_{t+1}$，它本身是有随机性的。所以要把右边这个式子，取期望值它才会等于 Q-function。但我们现在把期望值这件事情去掉，即\n$$\nQ^{\\pi}\\left(s_{t}^{n}, a_{t}^{n}\\right)=r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)\n$$\n\n我们就可以把 Q-function 用 r + V 取代掉，然后得到下式：\n$$\nr_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)-V^{\\pi}\\left(s_{t}^{n}\\right)\n$$\n把这个期望值去掉的好处就是你不需要估计 Q 了，你只需要估计 V 就够了，你只要估计 一个网络就够了。但这样你就引入了一个随机的东西 r ，它是有随机性的，它是一个随机变量。但是这个随机变量，相较于累积奖励 G 可能还好，因为它是某一个步骤会得到的奖励，而 G 是所有未来会得到的奖励的总和。G 的方差比较大，r 虽然也有一些方差，但它的方差会比 G 要小。所以把原来方差比较大的 G 换成方差比较小的 r 也是合理的。\n\nQ: 为什么可以直接把期望值拿掉？\n\nA: 原始的 A3C paper 试了各种方法，最后做出来就是这个最好。当然你可能说，搞不好估计 Q 和 V，也可以估计 很好，那我告诉你就是做实验的时候，最后结果就是这个最好，所以后来大家都用这个。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.5.png)\n\n因为 $r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)-V^{\\pi}\\left(s_{t}^{n}\\right)$ 叫做 `Advantage function`。所以这整个方法就叫 `Advantage Actor-Critic`。\n\n整个流程是这样子的。我们有一个 $\\pi$，有个初始的 actor 去跟环境做互动，先收集资料。在 policy gradient 方法里面收集资料以后，你就要拿去更新 policy。但是在 actor-critic 方法里面，你不是直接拿那些资料去更新 policy。你先拿这些资料去估计价值函数，你可以用 TD 或 MC 来估计价值函数 。接下来，你再基于价值函数，套用下面这个式子去更新 $\\pi$。\n$$\n\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)-V^{\\pi}\\left(s_{t}^{n}\\right)\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\n$$\n然后你有了新的 $\\pi$ 以后，再去跟环境互动，再收集新的资料，去估计价值函数。然后再用新的价值函数 去更新 policy，去更新 actor。\n\n整个 actor-critic 的算法就是这么运作的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.6.png)\n\n实现 Actor-Critic 的时候，有两个一定会用的 tip。\n\n* 第一个 tip 是说，我们需要估计两个网络：V function 和 policy 的网络（也就是 actor）。\n  *  Critic 网络 $V^\\pi(s)$ 输入一个状态，输出一个标量。\n  *  Actor 网络 $\\pi(s)$ 输入一个状态，\n     * 如果动作是离散的，输出就是一个动作的分布。\n     * 如果动作是连续的，输出就是一个连续的向量。\n  * 上图是举的是离散的例子，但连续的情况也是一样的。输入一个状态，然后它决定你现在要采取哪一个动作。**这两个网络，actor 和 critic 的输入都是 s，所以它们前面几个层(layer)，其实是可以共享的。**\n    * 尤其是假设你今天是玩 Atari 游戏，输入都是图像。输入的图像都非常复杂，图像很大，通常你前面都会用一些 CNN 来处理，把那些图像抽象成高级(high level)的信息。把像素级别的信息抽象成高级信息这件事情，其实对 actor 跟 critic 来说是可以共用的。所以通常你会让 actor 跟 critic 的共享前面几个层，你会让 actor 跟 critic 的前面几个层共用同一组参数，那这一组参数可能是 CNN 的参数。\n    * 先把输入的像素变成比较高级的信息，然后再给 actor 去决定说它要采取什么样的行为，给这个 critic，给价值函数去计算期望奖励。\n* **第二个 tip 是我们一样需要探索(exploration)的机制。**在做 Actor-Critic 的时候，有一个常见的探索的方法是你会对你的 $\\pi$ 的输出的分布下一个约束。这个约束是希望这个分布的熵(entropy)不要太小，希望这个分布的熵可以大一点，也就是希望不同的动作它的被采用的概率平均一点。这样在测试的时候，它才会多尝试各种不同的动作，才会把这个环境探索的比较好，才会得到比较好的结果。\n\n这个就是 Advantage Actor-Critic。\n\n## A3C\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.7.png)\n\n强化学习有一个问题就是它很慢，那怎么增加训练的速度呢？举个例子，火影忍者就是有一次鸣人说，他想要在一周之内打败晓，所以要加快修行的速度，他老师就教他一个方法：用影分身进行同样修行。两个一起修行的话，经验值累积的速度就会变成 2 倍，所以鸣人就开了 1000 个影分身来进行修行。这个其实就是 `Asynchronous(异步的) Advantage Actor-Critic`，也就是 A3C 这个方法的精神。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.8.png)\n\n**A3C 这个方法就是同时开很多个 worker，那每一个 worker 其实就是一个影分身。那最后这些影分身会把所有的经验，通通集合在一起。**你如果没有很多个 CPU，可能也是不好实现的，你可以实现 A2C 就好。\n\nQ: A3C 是怎么运作的？\n\nA: \n\n* A3C 一开始有一个 global network。那我们刚才有讲过，其实 policy network 跟 value network 是绑(tie)在一起的，它们的前几个层会被绑一起。我们有一个 global network，它们有包含 policy 的部分和 value 的部分。\n\n* 假设 global network 的参数是 $\\theta_1$，你会开很多个 worker。每一个 worker 就用一张 CPU 去跑。比如你就开 8 个 worker，那你至少 8 张 CPU。每一个 worker 工作前都会 global network 的参数复制过来。\n* 接下来你就去跟环境做互动，每一个 actor 去跟环境做互动的时候，要收集到比较多样性的数据。举例来说，如果是走迷宫的话，可能每一个 actor 起始的位置都会不一样，这样它们才能够收集到比较多样性的数据。\n* 每一个 actor 跟环境做互动，互动完之后，你就会计算出梯度。计算出梯度以后，你要拿梯度去更新你的参数。你就计算一下你的梯度，然后用你的梯度去更新 global network 的参数。就是这个 worker 算出梯度以后，就把梯度传回给中央的控制中心，然后中央的控制中心就会拿这个梯度去更新原来的参数。\n* 注意，所有的 actor 都是平行跑的，每一个 actor 就是各做各的，不管彼此。所以每个人都是去要了一个参数以后，做完就把参数传回去。所以当第一个 worker 做完想要把参数传回去的时候，本来它要的参数是 $\\theta_1$，等它要把梯度传回去的时候。可能别人已经把原来的参数覆盖掉，变成 $\\theta_2$了。但是没有关系，它一样会把这个梯度就覆盖过去就是了。Asynchronous actor-critic 就是这么做的，这个就是 A3C。\n\n ## Pathwise Derivative Policy Gradient\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.9.png)\n\n讲完 A3C 之后，我们要讲另外一个方法叫做 `Pathwise Derivative Policy Gradient`。这个方法可以看成是 Q-learning 解连续动作的一种特别的方法，也可以看成是一种特别的 Actor-Critic 的方法。\n\n用棋灵王来比喻的话，阿光是一个 actor，佐为是一个 critic。阿光落某一子以后，\n\n* 如果佐为是一般的 Actor-Critic，他会告诉阿光说这时候不应该下小马步飞，他会告诉你，你现在采取的这一步算出来的 value 到底是好还是不好，但这样就结束了，他只告诉你说好还是不好。因为一般的这个 Actor-Critic 里面那个 critic 就是输入状态或输入状态跟动作的对(pair)，然后给你一个 value 就结束了。所以对 actor 来说，它只知道它做的这个行为到底是好还是不好。\n* 但如果是在 pathwise derivative policy gradient 里面，这个 critic 会直接告诉 actor 说采取什么样的动作才是好的。所以今天佐为不只是告诉阿光说，这个时候不要下小马步飞，同时还告诉阿光说这个时候应该要下大马步飞，所以这个就是 Pathwise Derivative Policy Gradient 中的 critic。critic 会直接告诉 actor 做什么样的动作才可以得到比较大的 value。\n\n从 Q-learning 的观点来看，Q-learning 的一个问题是你在用 Q-learning 的时候，考虑 continuous vector 会比较麻烦，比较没有通用的解决方法(general solution)，怎么解这个优化问题呢？\n\n我们用一个 actor 来解这个优化的问题。本来在 Q-learning 里面，如果是一个连续的动作，我们要解这个优化问题。但是现在这个优化问题由 actor 来解，假设 actor 就是一个 solver，这个 solver 的工作就是给定状态 s，然后它就去解，告诉我们说，哪一个动作可以给我们最大的 Q value，这是从另外一个观点来看 pathwise derivative policy gradient 这件事情。\n\n在 GAN 中也有类似的说法。我们学习一个 discriminator 来评估东西好不好，要 discriminator 生成东西的话，非常困难，那怎么办？因为要解一个 arg max 的问题非常的困难，所以用 generator 来生成。\n\n所以今天的概念其实是一样的，Q 就是那个 discriminator。要根据这个 discriminator 决定动作非常困难，怎么办？另外学习一个网络来解这个优化问题，这个东西就是 actor。\n\n所以两个不同的观点是同一件事。从两个不同的观点来看，\n\n* 一个观点是说，我们可以对原来的 Q-learning 加以改进，我们学习一个 actor 来决定动作以解决 arg max 不好解的问题。\n* 另外一个观点是，原来的 actor-critic 的问题是 critic 并没有给 actor 足够的信息，它只告诉它好或不好，没有告诉它说什么样叫好，那现在有新的方法可以直接告诉 actor 说，什么样叫做好。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.10.png)\n\n那我们讲一下它的算法。假设我们学习了一个 Q-function，Q-function 就是输入 s 跟 a，输出就是 $Q^{\\pi}(s,a)$。那接下来，我们要学习一个 actor，这个 actor 的工作就是解这个 arg max 的问题。这个 actor 的工作就是输入一个状态 s，希望可以输出一个动作 a。这个动作 a 被丢到 Q-function 以后，它可以让 $Q^{\\pi}(s,a)$ 的值越大越好。\n\n那实际上在训练的时候，你其实就是把 Q 跟 actor 接起来变成一个比较大的网络。Q 是一个网络，输入 s 跟 a，输出一个 value。Actor 在训练的时候，它要做的事情就是输入 s，输出 a。把 a 丢到 Q 里面，希望输出的值越大越好。在训练的时候会把 Q 跟 actor 接起来，当作是一个大的网络。然后你会固定住 Q 的参数，只去调 actor 的参数，就用 gradient ascent 的方法去最大化 Q 的输出。这就是一个 GAN，这就是 conditional GAN。Q 就是 discriminator，但在强化学习就是 critic，actor 在 GAN 里面就是 generator，其实它们就是同一件事情。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.11.png)\n\n我们来看一下 pathwise derivative policy gradient 的算法。一开始你会有一个 actor $\\pi$，它去跟环境互动，然后，你可能会要它去估计 Q value。估计完 Q value 以后，你就把 Q value 固定，只去学习一个 actor。假设这个 Q 估得是很准的，它知道在某一个状态采取什么样的动作，会真的得到很大的 value。接下来就学习这个 actor，actor 在给定 s 的时候，它采取了 a，可以让最后 Q-function 算出来的 value 越大越好。你用这个 criteria 去更新你的 actor $\\pi$。然后有新的 $\\pi$ 再去跟环境做互动，再估计 Q，再得到新的 $\\pi$ 去最大化 Q 的输出。本来在 Q-learning 里面，你用得上的技巧，在这边也几乎都用得上，比如说 replay buffer、exploration 等等。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.12.png)\n\n上图是原来 Q-learning 的算法。你有一个 Q-function Q，你有另外一个目标的 Q-function 叫做 $\\hat{Q}$。然后在每一次 训练，在每一个回合的每一个时间点里面，你会看到一个状态 $s_t$，你会采取某一个动作 $a_{t}$。至于采取哪一个动作是由 Q-function 所决定的，因为解一个 arg max 的问题。如果是离散的话没有问题，你就看说哪一个 a 可以让 Q 的 value 最大，就采取哪一个动作。那你需要加一些探索，这样表现才会好。你会得到奖励 $r_t$，跳到新的状态 $s_{t+1}$。你会把 $s_t$, $a_{t}$, $r_t$, $s_{t+1}$ 塞到你的 buffer 里面去。你会从你的 buffer 里面采样一个批量的数据，在这个批量数据里面，可能某一笔是 $s_i, a_i, r_i, s_{i+1}$。接下来你会算一个目标，这个目标叫做 $y$ ，$y=r_{i}+\\max _{a} \\hat{Q}\\left(s_{i+1}, a\\right)$。然后怎么学习你的 Q 呢？你希望 $Q(s_i,a_i)$ 跟 y 越接近越好，这是一个回归的问题，最后每 C 个步骤，你要把用 Q 替代 $\\hat{Q}$ 。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.13.png)\n\n 接下来我们把 Q-learning 改成 Pathwise Derivative Policy Gradient，这边需要做四个改变。\n\n* 第一个改变是，你要把 Q 换成 $\\pi$，本来是用 Q 来决定在状态 $s_t$ 产生那一个动作, $a_{t}$ 现在是直接用 $\\pi$ 。我们不用再解 arg max 的问题了，我们直接学习了一个 actor。这个 actor 输入 $s_t$ 就会告诉我们应该采取哪一个 $a_{t}$。所以本来输入 $s_t$，采取哪一个 $a_t$，是 Q 决定的。在 Pathwise Derivative Policy Gradient 里面，我们会直接用 $\\pi$ 来决定，这是第一个改变。\n* 第二个改变是，本来这个地方是要计算在 $s_{i+1}$，根据你的 policy 采取某一个动作 a 会得到多少的 Q value。那你会采取让 $\\hat{Q}$ 最大的那个动作 a。那现在因为我们其实不好解这个 arg max 的问题，所以 arg max 问题，其实现在就是由 policy $\\pi$ 来解了，所以我们就直接把 $s_{i+1}$ 代到 policy $\\pi$ 里面，你就会知道说给定 $s_{i+1}$ ，哪一个动作会给我们最大的 Q value，那你在这边就会采取那一个动作。在 Q-function 里面，有两个 Q network，一个是真正的 Q network，另外一个是目标 Q network。那实际上你在实现这个算法 的时候，你也会有两个 actor，你会有一个真正要学习的 actor $\\pi$，你会有一个目标 actor $\\hat{\\pi}$ 。这个原理就跟为什么要有目标 Q network 一样，我们在算目标 value 的时候，我们并不希望它一直的变动，所以我们会有一个目标的 actor 和一个目标的 Q-function，它们平常的参数就是固定住的，这样可以让你的这个目标的 value 不会一直地变化。所以本来到底是要用哪一个动作 a，你会看说哪一个动作 a 可以让 $\\hat{Q}$  最大。但现在因为哪一个动作 a 可以让 $\\hat{Q}$ 最大这件事情已经用 policy 取代掉了，所以我们要知道哪一个动作 a 可以让 $\\hat{Q}$ 最大，就直接把那个状态带到 $\\hat{\\pi}$ 里面，看它得到哪一个 a，那个 a 就是会让 $\\hat{Q}(s,a)$ 的值最大的那个 a 。其实跟原来的这个 Q-learning 也是没什么不同，只是原来你要解 arg max 的地方，通通都用 policy 取代掉了，那这个是第二个不同。\n* 第三个不同就是之前只要学习 Q，现在你多学习一个 $\\pi$，那学习 $\\pi$ 的时候的方向是什么呢？学习 $\\pi$ 的目的，就是为了最大化 Q-function，希望你得到的这个 actor，它可以让你的 Q-function 输出越大越好，这个跟学习 GAN 里面的 generator 的概念。其实是一样的。\n* 第四个步骤，就跟原来的 Q-function 一样。你要把目标的 Q network 取代掉，你现在也要把目标 policy 取代掉。\n\n## Connection with GAN\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/9.14.png)\n\n其实 GAN 跟 Actor-Critic 的方法是非常类似的。这边就不细讲，你可以去找到一篇 paper 叫做 `Connecting Generative Adversarial Network and Actor-Critic Methods`。\n\nQ: 知道 GAN 跟 Actor-Critic 非常像有什么帮助呢？\n\nA: 一个很大的帮助就是 GAN 跟 Actor-Critic 都是以难训练而闻名的。所以在文献上就会收集各式各样的方法，告诉你说怎么样可以把 GAN 训练起来。怎么样可以把 Actor-Critic 训练起来。但是因为做 GAN 跟 Actor-Critic 的人是两群人，所以这篇 paper 里面就列出说在 GAN 上面有哪些技术是有人做过的，在 Actor-Critic 上面，有哪些技术是有人做过的。也许在 GAN 上面有试过的技术，你可以试着应用在 Actor-Critic 上，在 Actor-Critic 上面做过的技术，你可以试着应用在 GAN 上面，看看是否 work。\n\n## References\n\n* [神经网络与深度学习](https://nndl.github.io/)\n\n','2021-12-10 12:43:49','2021-12-19 19:41:15');

INSERT INTO `learn_detail` (`id`, `learn_id`, `title`, `content`, `create_time`, `modify_time`)
VALUES
	(53,3,'第九章 习题','## 1 Keywords\r\n\r\n- **A2C：** Advantage Actor-Critic的缩写，一种Actor-Critic方法。\r\n\r\n- **A3C：** Asynchronous（异步的）Advantage Actor-Critic的缩写，一种改进的Actor-Critic方法，通过异步的操作，进行RL模型训练的加速。\r\n-  **Pathwise Derivative Policy Gradient：** 其为使用 Q-learning 解 continuous action 的方法，也是一种 Actor-Critic 方法。其会对于actor提供value最大的action，而不仅仅是提供某一个action的好坏程度。\r\n\r\n## 2 Questions\r\n\r\n- 整个Advantage actor-critic（A2C）算法的工作流程是怎样的？\r\n\r\n  答：在传统的方法中，我们有一个policy $\\pi$ 以及一个初始的actor与environment去做互动，收集数据以及反馈。通过这些每一步得到的数据与反馈，我们就要进一步更新我们的policy $\\pi$ ，通常我们所使用的方式是policy gradient。但是对于actor-critic方法，我们不是直接使用每一步得到的数据和反馈进行policy $\\pi$ 的更新，而是使用这些数据进行 estimate value function，这里我们通常使用的算法包括前几个chapters重点介绍的TD和MC等算法以及他们的优化算法。接下来我们再基于value function来更新我们的policy，公式如下：\r\n  $$\r\n  \\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{t=1}^{T_{n}}\\left(r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)-V^{\\pi}\\left(s_{t}^{n}\\right)\\right) \\nabla \\log p_{\\theta}\\left(a_{t}^{n} \\mid s_{t}^{n}\\right)\r\n  $$\r\n  其中，上式中的 $r_{t}^{n}+V^{\\pi}\\left(s_{t+1}^{n}\\right)-V^{\\pi}\\left(s_{t}^{n}\\right)$ 我们称为Advantage function，我们通过上式得到新的policy后，再去与environment进行交互，然后再重复我们的estimate value function的操作，再用value function来更新我们的policy。以上的整个方法我们称为Advantage Actor-Critic。\r\n\r\n- 在实现 Actor-Critic 的时候，有哪些我们用到的tips?\r\n\r\n  答：与我们上一章讲述的东西有关：\r\n\r\n  1. **estimate 两个 network：** 一个是estimate V function，另外一个是 policy 的 network，也就是你的 actor。 V-network的input 是一个 state，output 是一个 scalar。然后 actor 这个 network的input 是一个 state，output 是一个 action 的 distribution。这两个 network，actor 和 critic 的 input 都是 s，所以它们前面几个 layer，其实是可以 share 的。尤其是假设你今天是玩 Atari 游戏，input 都是 image。那 input 那个 image 都非常复杂，image 很大，通常前面都会用一些 CNN 来处理，把那些 image 抽象成 high level 的 information，所以对 actor 跟 critic 来说是可以共用的。我们可以让 actor 跟 critic 的前面几个 layer 共用同一组参数。那这一组参数可能是 CNN。先把 input 的 pixel 变成比较 high level 的信息，然后再给 actor 去决定说它要采取什么样的行为，给这个 critic，给 value function 去计算 expected reward。\r\n  2. **exploration 机制：** 其目的是对policy $\\pi$ 的 output 的分布进行一个限制，从而使得 distribution 的 entropy 不要太小，即希望不同的 action 被采用的机率平均一点。这样在 testing 的时候，它才会多尝试各种不同的 action，才会把这个环境探索的比较好，才会得到比较好的结果。\r\n\r\n- A3C（Asynchronous Advantage Actor-Critic）在训练是回有很多的worker进行异步的工作，最后再讲他们所获得的“结果”再集合到一起。那么其具体的如何运作的呢？\r\n\r\n  答：A3C一开始会有一个 global network。它们有包含 policy 的部分和 value 的部分，假设它的参数就是 $\\theta_1$。对于每一个 worker 都用一张 CPU 训练（举例子说明），第一个 worker 就把 global network 的参数 copy 过来，每一个 worker 工作前都会global network 的参数 copy 过来。然后这个worker就要去跟environment进行交互，每一个 actor 去跟environment做互动后，就会计算出 gradient并且更新global network的参数。这里要注意的是，所有的 actor 都是平行跑的、之间没有交叉。所以每个worker都是在global network“要”了一个参数以后，做完就把参数传回去。所以当第一个 worker 做完想要把参数传回去的时候，本来它要的参数是 $\\theta_1$，等它要把 gradient 传回去的时候。可能别人已经把原来的参数覆盖掉，变成 $\\theta_2$了。但是没有关系，它一样会把这个 gradient 就覆盖过去就是了。\r\n\r\n- 对比经典的Q-learning算法，我们的Pathwise Derivative Policy Gradient有哪些改进之处？\r\n\r\n  答：\r\n\r\n  1. 首先，把 $Q(s,a)$ 换成 了 $\\pi$，之前是用 $Q(s,a)$ 来决定在 state $s_t$ 产生那一个 action, $a_{t}$ 现在是直接用 $\\pi$ 。原先我们需要解 argmax 的问题，现在我们直接训练了一个 actor。这个 actor input $s_t$ 就会告诉我们应该采取哪一个 $a_{t}$。综上，本来 input $s_t$，采取哪一个 $a_t$，是 $Q(s,a)$ 决定的。在 Pathwise Derivative Policy Gradient 里面，我们会直接用 $\\pi$ 来决定。\r\n  2. 另外，原本是要计算在 $s_{i+1}$ 时对应的 policy 采取的 action a 会得到多少的 Q value，那你会采取让 $\\hat{Q}$ 最大的那个 action a。现在因为我们不需要再解argmax 的问题。所以现在我们就直接把 $s_{i+1}$ 代入到 policy $\\pi$ 里面，直接就会得到在 $s_{i+1}$ 下，哪一个 action 会给我们最大的 Q value，那你在这边就会 take 那一个 action。在 Q-function 里面，有两个 Q network，一个是真正的 Q network，另外一个是 target Q network。那实际上你在 implement 这个 algorithm 的时候，你也会有两个 actor，你会有一个真正要 learn 的 actor $\\pi$，你会有一个 target actor $\\hat{\\pi}$ 。但现在因为哪一个 action a 可以让 $\\hat{Q}$ 最大这件事情已经被用那个 policy 取代掉了，所以我们要知道哪一个 action a 可以让 $\\hat{Q}$ 最大，就直接把那个 state 带到 $\\hat{\\pi}$ 里面，看它得到哪一个 a，就用那一个 a，其也就是会让 $\\hat{Q}(s,a)$ 的值最大的那个 a 。\r\n  3. 还有，之前只要 learn Q，现在你多 learn 一个 $\\pi$，其目的在于maximize Q-function，希望你得到的这个 actor，它可以让你的 Q-function output 越大越好，这个跟 learn GAN 里面的 generator 的概念类似。\r\n  4. 最后，与原来的 Q-function 一样。我们要把 target 的 Q-network 取代掉，你现在也要把 target policy 取代掉。\r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：请简述一下A3C算法吧，另外A3C是on-policy还是off-policy呀？\r\n\r\n  答：A3C就是异步优势演员-评论家方法（Asynchronous Advantage Actor-Critic）：评论家学习值函数，同时有多个actor并行训练并且不时与全局参数同步。A3C旨在用于并行训练，是 on-policy 的方法。 \r\n\r\n- 高冷的面试官：请问Actor - Critic有何优点呢？\r\n\r\n  答：\r\n\r\n  - 相比以值函数为中心的算法，Actor - Critic应用了策略梯度的做法，这能让它在连续动作或者高维动作空间中选取合适的动作，而 Q-learning 做这件事会很困难甚至瘫痪。\r\n  - 相比单纯策略梯度，Actor - Critic应用了Q-learning或其他策略评估的做法，使得Actor Critic能进行单步更新而不是回合更新，比单纯的Policy Gradient的效率要高。\r\n\r\n- 高冷的面试官：请问A3C算法具体是如何异步更新的？\r\n\r\n  答：下面是算法大纲：\r\n\r\n  - 定义全局参数 $\\theta$ 和 $w$ 以及特定线程参数 $θ′$ 和 $w′$。\r\n  - 初始化时间步 $t=1$。\r\n  - 当 $T<=T_{max}$：\r\n    - 重置梯度：$dθ=0$ 并且 $dw=0$。\r\n    - 将特定于线程的参数与全局参数同步：$θ′=θ$ 以及 $w′=w$。\r\n    - 令 $t_{start} =t$ 并且随机采样一个初始状态 $s_t$。\r\n    - 当 （$s_t!=$ 终止状态）并$t−t_{start}<=t_{max}$：\r\n      - 根据当前线程的策略选择当前执行的动作 $a_t∼π_{θ′}(a_t|s_t)$，执行动作后接收回报$r_t$然后转移到下一个状态st+1。\r\n      - 更新 t 以及 T：t=t+1 并且 T=T+1。\r\n    - 初始化保存累积回报估计值的变量\r\n    - 对于 $i=t_1,…,t_{start}$：\r\n      - r←γr+ri；这里 r 是 Gi 的蒙特卡洛估计。\r\n      - 累积关于参数 θ′的梯度：$dθ←dθ+∇θ′logπθ′(ai|si)(r−Vw′(si))$;\r\n      - 累积关于参数 w′ 的梯度：$dw←dw+2(r−Vw′(si))∇w′(r−Vw′(si))$.\r\n    - 分别使用 dθ以及 dw异步更新 θ以及 w。\r\n    \r\n- 高冷的面试官：Actor-Critic两者的区别是什么？\r\n\r\n  答：Actor是策略模块，输出动作；critic是判别器，用来计算值函数。\r\n\r\n- 高冷的面试官：actor-critic框架中的critic起了什么作用？\r\n\r\n  答：critic表示了对于当前决策好坏的衡量。结合策略模块，当critic判别某个动作的选择时有益的，策略就更新参数以增大该动作出现的概率，反之降低动作出现的概率。\r\n\r\n- 高冷的面试官：简述A3C的优势函数？\r\n  \r\n  答：$A(s,a)=Q(s,a)-V(s)$是为了解决value-based方法具有高变异性。它代表着与该状态下采取的平均行动相比所取得的进步。\r\n\r\n    - 如果 A(s,a)>0: 梯度被推向了该方向\r\n    - 如果 A(s,a)<0: (我们的action比该state下的平均值还差) 梯度被推向了反方\r\n\r\n    但是这样就需要两套 value function，所以可以使用TD error 做估计：$A(s,a)=r+\\gamma V(s\')-V(s)$。\r\n','2021-12-10 12:43:49','2021-12-19 19:41:26'),
	(54,3,'第十章 稀疏奖励','## Sparse Reward \n实际上用 reinforcement learning learn agent 的时候，多数的时候 agent 都是没有办法得到 reward 的。在没有办法得到 reward 的情况下，训练 agent 是非常困难的。举例来说，假设你要训练一个机器手臂，然后桌上有一个螺丝钉跟螺丝起子，那你要训练它用螺丝起子把螺丝钉栓进去，这个很难，为什么？因为一开始你的 agent 是什么都不知道的，它唯一能够做不同的 action 的原因是 exploration。举例来说，你在做 Q-learning 的时候，会有一些随机性，让它去采取一些过去没有采取过的 action，那你要随机到说，它把螺丝起子捡起来，再把螺丝栓进去，然后就会得到 reward 1，这件事情是永远不可能发生的。所以，不管你的 actor 做了什么事情，它得到 reward 永远都是 0，对它来说不管采取什么样的 action 都是一样糟或者是一样的好。所以，它最后什么都不会学到。\n\n如果环境中的 reward 非常 sparse，reinforcement learning 的问题就会变得非常的困难，但是人类可以在非常 sparse 的 reward 上面去学习。我们的人生通常多数的时候，我们就只是活在那里，都没有得到什么 reward 或是 penalty。但是，人还是可以采取各种各式各样的行为。所以，一个真正厉害的 AI 应该能够在 sparse reward 的情况下也学到要怎么跟这个环境互动。\n\n我们可以通过三个方向来解决 sparse reward 的问题。\n\n## Reward Shaping\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.1.png)\n\n第一个方向是 `reward shaping`。**Reward shaping 的意思是说环境有一个固定的 reward，它是真正的 reward，但是为了让 agent 学出来的结果是我们要的样子，我们刻意地设计了一些 reward 来引导我们的 agent。**\n\n举例来说，如果是把小孩当成一个 agent 的话。那一个小孩，他可以 take 两个 actions，一个 action 是他可以出去玩，那他出去玩的话，在下一秒钟它会得到 reward 1。但是他在月考的时候，成绩可能会很差。所以在100 个小时之后呢，他会得到 reward -100。然后，他也可以决定要念书，然后在下一个时间，因为他没有出去玩，所以他觉得很不爽，所以他得到 reward -1。但是在 100 个小时后，他可以得到 reward 100。但对一个小孩来说，他可能就会想要 take play 而不是 take study。我们计算的是 accumulated reward，但也许对小孩来说，他的 discount factor 会很大，所以他就不太在意未来的reward。而且因为他是一个小孩，他还没有很多 experience，所以他的 Q-function estimate 是非常不精准的。所以要他去 estimate 很远以后会得到的 accumulated reward，他其实是预测不出来的。所以这时候大人就要引导他，怎么引导呢？就骗他说，如果你坐下来念书我就给你吃一个棒棒糖。所以，对他来说，下一个时间点会得到的 reward 就变成是positive 的。所以他就觉得说，也许 take 这个 study 是比 play 好的。虽然这并不是真正的 reward，而是其他人骗他的reward，告诉他说你采取这个 action 是好的。Reward shaping 的概念是一样的，简单来说，就是你自己想办法 design 一些 reward，它不是环境真正的 reward。在玩 Atari 游戏里面，真的 reward 是游戏主机给你的 reward，但你自己去设计一些 reward 好引导你的 machine，做你想要它做的事情。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.2.png)\n\n举例来说，这个例子是 Facebook 玩 VizDoom 的 agent。VizDoom 是一个第一人射击游戏，在这个射击游戏中，杀了敌人就得到 positive reward，被杀就得到 negative reward。他们设计了一些新的 reward，用新的 reward 来引导 agent 让他们做得更好，这不是游戏中真正的 reward。比如说掉血就扣 0.05 的分数，弹药减少就扣分，捡到补给包就加分，呆在原地就扣分，移动就加分。 活着会扣一个很小的分数，因为不这样做的话，machine 会只想活着，一直躲避敌人，这样会让 machine 好战一点。表格中的参数都是调出来的。\n\nReward shaping 是有问题的，因为我们需要 domain knowledge，举例来说，机器人想要学会的事情是把蓝色的板子从这个柱子穿过去。机器人很难学会，我们可以做 reward shaping。一个貌似合理的说法是，蓝色的板子离柱子越近，reward 越大。但是 machine 靠近的方式会有问题，它会用蓝色的板子打柱子。而我们要把蓝色板子放在柱子上面去，才能把蓝色板子穿过柱子。 这种 reward shaping 的方式是没有帮助的，那至于什么 reward shaping 有帮助，什么 reward shaping 没帮助，会变成一个 domain knowledge，你要去调的。\n\n###  Curiosity\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.3.png)\n\n接下来就是介绍各种你可以自己加进去，in general 看起来是有用的 reward。举例来说，一个技术是给 machine 加上 curiosity，所以叫 `curiosity driven reward`。如上图所示，我们有一个 reward function，它给你某一个 state，给你某一个 action，它就会评断说在这个 state 采取这个 action 得到多少的 reward。那我们当然希望 total reward 越大越好。\n\n在 curiosity driven 的这种技术里面，你会加上一个新的 reward function。这个新的 reward function 叫做 `ICM(intrinsic curiosity module)`，它就是要给机器加上好奇心。ICM 会吃 3 个东西，它会吃 state $s_1$、action $a_1$ 和 state $s_2$。根据 $s_1$ 、$a_1$、$s_2$，它会 output 另外一个 reward $r_1^i$。对 machine 来说，total reward 并不是只有 r 而已，还有 $r^i$。它不是只有把所有的 r 都加起来，它还把所有 $r^i$ 加起来当作 total reward。所以，它在跟环境互动的时候，它不是只希望 r 越大越好，它还同时希望 $r^i$ 越大越好，它希望从 ICM 的 module 里面得到的 reward 越大越好。ICM 就代表了一种 curiosity。\n\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.4.png)\n\n怎么设计这个 ICM ？这个是最原始的设计。这个设计是这样，curiosity module 就是 input 3 个东西，input 现在的 state，input 在这个 state 采取的 action，然后 input 下一个 state $s_{t+1}$。接下来会 output 一个 reward $r^i_t$。那这个 $r^i_t$  是怎么算出来的呢？在 ICM 里面，你有一个 network，这个 network 会 take $a_t$ 跟$s_t$，然后去 output $\\hat{s}_{t+1}$，也就是这个 network 根据 $a_t$ 和 $s_t$ 去 predict  $\\hat{s}_{t+1}$ 。接下来再看说，这个 network 的预测  $\\hat{s}_{t+1}$ 跟真实的情况 $s_{t+1}$ 像不像，越不像那得到的 reward 就越大。所以这个 reward $r_t^i$ 的意思是说，如果未来的 state 越难被预测的话，那得到的 reward 就越大。这就是鼓励 machine 去冒险，现在采取这个 action，未来会发生什么事越没有办法预测的话，这个 action 的 reward 就大。所以如果有这样子的 ICM，machine 就会倾向于采取一些风险比较大的 action，它想要去探索未知的世界，它想要去看看说，假设某一个 state 是它没有办法预测，它会特别去想要采取那个 state，这可以增加 machine exploration 的能力。\n\n这个 network 1 其实是另外 train 出来的。Training 的时候，这个 network 1，你会给它 $a_t$、 $s_t$、 $s_{t+1}$，然后让这个network 1 去学说 given $a_t, s_t$，怎么 predict $\\hat{s}_{t+1}$。Apply 到 agent 互动的时候，其实要把 ICM module fix 住。其实，这一整个想法里面是有一个问题的。这个问题是某一些 state它很难被预测并不代表它就是好的，它就应该要去被尝试的。举例来说，俄罗斯轮盘的结果也是没有办法预测的，并不代表说，人应该每天去玩俄罗斯轮盘这样子。所以只是鼓励 machine 去冒险是不够的，因为如果光是只有这个 network 的架构，machine 只知道说什么东西它无法预测。如果在某一个 state 采取某一个 action，它无法预测接下来结果，它就会采取那个 action，但并不代表这样的结果一定是好的。举例来说，可能在某个游戏里面，背景会有风吹草动，会有树叶飘动。那也许树叶飘动这件事情，是很难被预测的，对 machine 来说它在某一个 state 什么都不做，看着树叶飘动，然后，发现这个树叶飘动是没有办法预测的，接下来它就会一直站在那边，看树叶飘动。所以说，光是有好奇心是不够的，还要让它知道说，什么事情是真正重要的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.5.png)\n\n怎么让 machine 知道说什么事情是真正重要的？你要加上另外一个 module，我们要 learn 一个`feature extractor`，黄色的格子代表 feature extractor，它是 input 一个 state，然后 output 一个 feature vector 来代表这个 state，那我们期待这个 feature extractor 可以把那种没有意义的画面，state 里面没有意义的东西把它过滤掉，比如说风吹草动、白云的飘动、树叶的飘动这种没有意义的东西直接把它过滤掉，\n\n假设这个 feature extractor 真的可以把无关紧要的东西过滤掉以后，network 1 实际上做的事情是，给它一个 actor，给它一个 state $s_t$ 的 feature representation，让它预测 state $s_{t+1}$ 的 feature representation。接下来我们再看说，这个预测的结果跟真正的 state $s_{t+1}$ 的 feature representation 像不像，越不像，reward 就越大。怎么 learn 这个 feature extractor 呢？让这个 feature extractor 可以把无关紧要的事情滤掉呢？这边的 learn 法就是 learn 另外一个 network 2。这个 network 2 是吃 $\\phi(s_t)$、$\\phi(s_{t+1})$ 这两个 vector 当做 input，然后接下来它要 predict action a 是什么，然后它希望呢这个 action a 跟真正的 action a 越接近越好。这个 network 2 会 output 一个 action，它 output 说，从 state $s_t$ 跳到 state $s_{t+1}$，要采取哪一个 action 才能够做到，那希望这个 action 跟真正的 action 越接近越好。加上这个 network 2 的好处就是因为要用 $\\phi(s_t)$、$\\phi(s_{t+1})$  预测 action。所以，今天我们抽出来的 feature 跟预测 action 这件事情是有关的。所以风吹草动等与 machine 要采取的 action 无关的东西就会被滤掉，就不会被放在抽出来的 vector representation 里面。\n\n\n\n## Curriculum Learning\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.6.png)\n第二个方向是 `curriculum learning` 。Curriculum learning 不是 reinforcement learning 所独有的概念，其实在 machine learning，尤其是 deep learning 里面，你都会用到 curriculum learning 的概念。举例来说，curriculum learning 的意思是说，你为机器的学习做规划，你给他喂 training data 的时候，是有顺序的，通常都是由简单到难。就好比说，假设你今天要交一个小朋友作微积分，他做错就打他一巴掌，这样他永远都不会做对，太难了。你要先教他九九乘法，然后才教他微积分。所以 curriculum learning 的意思就是在教机器的时候，从简单的题目教到难的题目。就算不是 reinforcement learning，一般在 train deep network 的时候，你有时候也会这么做。举例来说，在 train RNN 的时候，已经有很多的文献都 report 说，你给机器先看短的 sequence，再慢慢给它长的 sequence，通常可以学得比较好。那用在 reinforcement learning 里面，你就是要帮机器规划一下它的课程，从最简单的到最难的。\n\n*  举例来说，在 Facebook 玩 VizDoom 的 agent 里面，Facebook 玩 VizDoom 的 agent 蛮强的。他们在参加这个 VizDoom 的比赛，机器的 VizDoom 比赛是得第一名的，他们是有为机器规划课程的。先从课程 0 一直上到课程 7。在这个课程里面，怪物的速度跟血量是不一样的。所以，在越进阶的课程里面，怪物的速度越快，然后他的血量越多。在 paper 里面也有讲说，如果直接上课程 7，machine 是学不起来的。你就是要从课程 0 一路玩上去，这样 machine 才学得起来。\n\n* 再举个例子，把蓝色的板子穿过柱子，怎么让机器一直从简单学到难呢？\n  * 如第一张图所示，也许一开始机器初始的时候，它的板子就已经在柱子上了。这个时候，机器要做的事情只有把蓝色的板子压下去，就结束了。这比较简单，它应该很快就学的会。它只有往上跟往下这两个选择嘛，往下就得到 reward，就结束了，他也不知道学的是什么。\n  * 如第二张图所示，这边就是把板子挪高一点，挪高一点，所以它有时候会很笨的往上拉，然后把板子拿出来了。如果它压板子学得会的话，拿板子也比较有机会学得会。假设它现在学的到说，只要板子接近柱子，它就可以把这个板子压下去的话。接下来，你再让它学更 general 的 case。\n  * 如第三张图所示，一开始，让板子离柱子远一点。然后，板子放到柱子上面的时候，它就会知道把板子压下去，这个就是 curriculum learning 的概念。当然 curriculum learning 有点 ad hoc(特别)，就是需要人去为机器设计它的课程。\n\n### Reverse Curriculum Generation\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.7.png)\n\n有一个比较 general 的方法叫做 `Reverse Curriculum Generation`。你可以用一个比较通用的方法来帮机器设计课程，这个比较通用的方法是怎么样呢？假设你现在一开始有一个 state $s_g$，这是你的 gold state，也就是最后最理想的结果。如果拿刚才那个板子和柱子的实验作为例子的话，就把板子放到柱子里面，这样子叫做 gold state。你就已经完成了，或者你让机器去抓东西，你训练一个机器手臂抓东西，抓到东西以后叫做 gold state。接下来你根据你的 gold state 去找其他的 state，这些其他的 state 跟 gold state 是比较接近的。举例来说，如果是让机器抓东西的例子里面，你的机器手臂可能还没有抓到东西。假设这些跟 gold state 很近的 state 叫做 $s_1$。你的机械手臂还没有抓到东西，但它离 gold state 很近，那这个叫做$s_1$。至于什么叫做近，这是 case dependent，你要根据你的 task 来 design 说怎么从 $s_g$ sample 出 $s_1$。如果是机械手臂的例子，可能就比较好想。其他例子可能就比较难想。接下来呢，你再从这些 $s_1$ 开始做互动，看它能不能够达到 gold state $s_g$，那每一个 state，你跟环境做互动的时候，你都会得到一个 reward R。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.8.png)\n\n接下来，我们把 reward 特别极端的 case 去掉。Reward 特别极端的 case 的意思就是说那些 case 太简单或是太难了。如果 reward 很大，代表说这个 case 太简单了，就不用学了，因为机器已经会了，它可以得到很大的 reward。如果 reward 太小，代表这个 case 太难了，依照机器现在的能力这个课程太难了，它学不会，所以就不要学这个，所以只找一些 reward 适中的 case。\n\n什么叫做适中，这个就是你要调的参数，找一些 reward 适中的 case。接下来，再根据这些 reward 适中的 case 去 sample 出更多的 state。假设你一开始，你机械手臂在这边，可以抓的到以后。接下来，就再离远一点，看看能不能够抓得到，又抓的到以后，再离远一点，看看能不能抓得到。这是一个有用的方法，它叫做`Reverse Curriculum learning`。刚才讲的是 curriculum learning，就是你要为机器规划它学习的顺序。而 reverse curriculum learning 是从 gold state 去反推，就是说你原来的目标是长这个样子，我们从目标去反推，所以这个叫做 reverse。  \n\n## Hierarchical RL\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.9.png)\n\n第三个方向是`分层强化学习(hierarchical reinforcement learning，HRL)`。分层强化学习是说，我们有好几个 agent。然后，有一些 agent 负责比较 high level 的东西，它负责订目标，然后它订完目标以后，再分配给其他的 agent，去把它执行完成。\n\n这样的想法其实也是很合理的。因为人在一生之中，并不是时时刻刻都在做决定。举例来说，假设你想要写一篇 paper，你会说就我先想个梗这样子，然后想完梗以后，你还要跑个实验。跑完实验以后，你还要写。写完以后呢，你还要这个去发表。每一个动作下面又还会再细分，比如说怎么跑实验呢？你要先 collect data，collect 完 data 以后，你要再 label，你要弄一个 network，然后又 train 不起来，要 train 很多次。然后重新 design network 架构好几次，最后才把 network train 起来。\n\n所以，我们要完成一个很大的 task 的时候，我们并不是从非常底层的那些 action 开始想起，我们其实是有个 plan。我们先想说，如果要完成这个最大的任务，那接下来要拆解成哪些小任务。每一个小任务要再怎么拆解成小小的任务。举例来说，叫你直接写一本书可能很困难，但叫你先把一本书拆成好几个章节，每个章节拆成好几段，每一段又拆成好几个句子，每一个句子又拆成好几个词汇，这样你可能就比较写得出来，这个就是分层的 reinforcement learning 的概念。\n\n这边是举一个例子，就是假设校长、教授和研究生通通都是 agent。那今天假设我们只要进入百大就可以得到 reward。假设进入百大的话，校长就要提出愿景告诉其他的 agent 说，现在你要达到什么样的目标。那校长的愿景可能就是说教授每年都要发三篇期刊。然后接下来这些 agent 都是有分层的，所以上面的 agent，他的动作就是提出愿景这样。那他把他的愿景传给下一层的 agent，下一层的 agent 就把这个愿景吃下去。如果他下面还有其他人的话，它就会提出新的愿景。比如说，校长要教授发期刊，但其实教授自己也是不做实验的。所以，教授也只能够叫下面的研究生做实验。所以教授就提出愿景，就做出实验的规划，然后研究生才是真的去执行这个实验的人。然后，真的把实验做出来，最后大家就可以得到reward。那现在是这样子的，在 learn 的时候，其实每一个 agent 都会 learn。那他们的整体的目标就是要达到最后的reward。那前面的这些 agent，他提出来的 actions 就是愿景这样。你如果是玩游戏的话，他提出来的就是，我现在想要产生这样的游戏画面。但是，假设他提出来的愿景是下面的 agent 达不到的，那就会被讨厌。举例来说，教授对研究生都一直逼迫研究生做一些很困难的实验，研究生都做不出来的话，研究生就会跑掉，所以他就会得到一个 penalty。所以如果今天下层的 agent 没有办法达到上层 agent 所提出来的 goal 的话，上层的 agent 就会被讨厌，它就会得到一个 negative reward。所以他要避免提出那些愿景是底下的 agent 所做不到的。那每一个 agent 都是把上层的 agent 所提出来的愿景当作输入，然后决定他自己要产生什么输出。\n\n但是你知道说，就算你看到上面的的愿景说，叫你做这一件事情。你最后也不一定能做成这一件事情。假设本来教授目标是要写期刊，但是不知道怎么回事，他就要变成一个 YouTuber。这个 paper 里面的 solution，我觉得非常有趣。给大家做一个参考，这其实本来的目标是要写期刊，但却变成 YouTuber，那怎么办呢? 把原来的愿景改成变成 YouTuber 就行了，在 paper 里面就是这么做的，为什么这么做呢? 因为虽然本来的愿景是要写期刊，但是后来变成 YouTuber，难道这些动作都浪费了吗? 不是，这些动作是没有被浪费的。我们就假设说，本来的愿景其实就是要成为 YouTuber，那你就知道成为 YouTuber 要怎做了。这个是分层 RL，是可以做得起来的 tip。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/10.10.png)\n\n\n上图是真实的例子。实际上呢，这里面就做了一些比较简单的游戏，这个是走迷宫，蓝色是 agent，蓝色的 agent 要走到黄色的目标。这边也是，这个单摆要碰到黄色的球。那愿景是什么呢？\n\n在这个 task 里面，它只有两个 agent ，下层的一个 agent 负责决定说要怎么走，上层的 agent 就负责提出愿景。虽然，实际上你可以用很多层，但 paper 就用了两层。\n\n走迷宫的例子是说粉红色的这个点代表的就是愿景。上层这个 agent，它告诉蓝色的这个 agent 说，你现在的第一个目标是先走到这个地方，蓝色的 agent 走到以后，再说你的新的目标是走到这里。蓝色的 agent 再走到以后，新的目标在这里。接下来又跑到这边，最后希望蓝色的 agent 就可以走到黄色的这个位置。\n\n单摆的例子也一样，就是粉红色的这个点代表的是上层的 agent 所提出来的愿景，所以这个 agent 先摆到这边，接下来，新的愿景又跑到这边，所以它又摆到这里。然后，新的愿景又跑到上面。然后又摆到上面，最后就走到黄色的位置了。这个就是 hierarchical 的 reinforcement learning。\n\n最后总结下分层强化学习。分层强化学习是指将一个复杂的强化学习问题分解成多个小的、简单的子问题，每个子问题都可以单独用马尔可夫决策过程来建模。这样，我们可以将智能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何执行低层次策略。这样，智能体就可以解决一些非常复杂的任务。\n\n## References\n\n* [神经网络与深度学习](https://nndl.github.io/)\n\n','2021-12-10 12:43:49','2021-12-19 19:41:37'),
	(55,3,'第十章 习题','## 1 Keywords\r\n\r\n- **reward shaping：** 在我们的agent与environment进行交互时，我们人为的设计一些reward，从而“指挥”agent，告诉其采取哪一个action是最优的，而这个reward并不是environment对应的reward，这样可以提高我们estimate Q-function时的准确性。\r\n- **ICM（intrinsic curiosity module）：** 其代表着curiosity driven这个技术中的增加新的reward function以后的reward function。\r\n- **curriculum learning：** 一种广义的用在RL的训练agent的方法，其在input训练数据的时候，采取由易到难的顺序进行input，也就是认为设计它的学习过程，这个方法在ML和DL中都会普遍使用。\r\n- **reverse curriculum learning：** 相较于上面的curriculum learning，其为更general的方法。其从最终最理想的state（我们称之为gold state）开始，依次去寻找距离gold state最近的state作为想让agent达到的阶段性的“理想”的state，当然我们应该在此过程中有意的去掉一些极端的case（太简单、太难的case）。综上，reverse curriculum learning 是从 gold state 去反推，就是说你原来的目标是长这个样子，我们从我们的目标去反推，所以这个叫做 reverse curriculum learning。  \r\n- **hierarchical （分层） reinforcement learning：** 将一个大型的task，横向或者纵向的拆解成多个 agent去执行。其中，有一些agent 负责比较high level 的东西，负责订目标，然后它订完目标以后，再分配给其他的 agent把它执行完成。（看教程的 hierarchical  reinforcement learning部分的示例就会比较明了）\r\n\r\n## 2 Questions\r\n\r\n- 解决sparse reward的方法有哪些？\r\n\r\n  答：Reward Shaping、curiosity driven reward、（reverse）curriculum learning 、Hierarchical Reinforcement learning等等。\r\n\r\n- reward shaping方法存在什么主要问题？\r\n\r\n  答：主要的一个问题是我们人为设计的reward需要domain knowledge，需要我们自己设计出符合environment与agent更好的交互的reward，这需要不少的经验知识，需要我们根据实际的效果进行调整。\r\n\r\n- ICM是什么？我们应该如何设计这个ICM？\r\n\r\n  答：ICM全称为intrinsic curiosity module。其代表着curiosity driven这个技术中的增加新的reward function以后的reward function。具体来说，ICM在更新计算时会考虑三个新的东西，分别是 state $s_1$、action $a_1$ 和 state $s_2$。根据$s_1$ 、$a_1$、 $a_2$，它会 output 另外一个新的 reward $r_1^i$。所以在ICM中我们total reward 并不是只有 r 而已，还有 $r^i$。它不是只有把所有的 r 都加起来，它还把所有 $r^i$ 加起来当作total reward。所以，它在跟环境互动的时候，它不是只希望 r 越大越好，它还同时希望 $r^i$ 越大越好，它希望从 ICM 的 module 里面得到的 reward 越大越好。ICM 就代表了一种curiosity。\r\n\r\n  对于如何设计ICM，ICM的input就像前面所说的一样包括三部分input 现在的 state $s_1$，input 在这个 state 采取的 action $a_1$，然后接 input 下一个 state $s_{t+1}$，对应的output就是reward $r_1^i$，input到output的映射是通过network构建的，其使用 $s_1$ 和 $a_1$ 去预测 $\\hat{s}_{t+1}$ ,然后继续评判预测的$\\hat{s}_{t+1}$和真实的$s_{t+1}$像不像，越不相同得到的reward就越大。通俗来说这个reward就是，如果未来的状态越难被预测的话，那么得到的reward就越大。这也就是curiosity的机制，倾向于让agent做一些风险比较大的action，从而增加其machine exploration的能力。\r\n\r\n  同时为了进一步增强network的表达能力，我们通常讲ICM的input优化为feature extractor，这个feature extractor模型的input就是state，output是一个特征向量，其可以表示这个state最主要、重要的特征，把没有意义的东西过滤掉。\r\n','2021-12-10 12:43:49','2021-12-19 19:41:41'),
	(56,3,'第十一章 模仿学习','## 模仿学习\n`模仿学习（imitation learning，IL）`讨论的问题是：假设我们连奖励都没有，那要怎么办呢？模仿学习又叫做`示范学习（learning from demonstration）`，`学徒学习（apprenticeship learning）`，`观察学习（learning by watching）`。在模仿学习里面，你有一些专家的示范，那机器也可以跟环境互动，但它没有办法从环境里面得到任何的奖励，它只能看着专家的示范来学习什么是好，什么是不好。其实，多数的情况，我们都没有办法真的从环境里面得到非常明确的奖励。举例来说，如果是棋类游戏或者是电玩，你有非常明确的奖励。但是其实多数的任务，都是没有奖励的。以聊天机器人为例，机器跟人聊天，聊得怎么样算是好，聊得怎么样算是不好，你无法给出明确的奖励。所以很多任务是根本就没有办法给出奖励的。\n\n虽然没有办法给出奖励，但是收集专家的示范是可以做到的。举例来说，在自动驾驶汽车里面，虽然你没有办法给出自动驾驶汽车的奖励，但你可以收集很多人类开车的纪录。在聊天机器人里面，你可能没有办法定义什么叫做好的对话，什么叫做不好的对话。但是收集很多人的对话当作范例，这一件事情也是可行的。\n\n所以模仿学习的使用性非常高。假设你不知道该怎么定义奖励，你就可以收集到专家的示范。如果你可以收集到一些范例的话，你可以收集到一些很厉害的智能体（比如人）跟环境实际上的互动的话，那你就可以考虑模仿学习这个技术。在模仿学习里面，我们介绍两个方法。第一个叫做`行为克隆（behavior cloning，BC）`，第二个叫做`逆强化学习（inverse reinforcement learning，IRL）` 或者叫做`逆最优控制（inverse optimal control）`。\n\n##  行为克隆\n其实行为克隆跟监督学习是一模一样的。如下图所示，以自动驾驶汽车为例，你可以收集到人开自动驾驶汽车的所有资料，比如说可以通过行车记录器进行收集。看到下图的观测的时候，人会决定向前。机器就采取跟人一样的行为，也向前，就结束了。这个就叫做行为克隆，专家做什么，机器就做一模一样的事。\n\n怎么让机器学会跟专家一模一样的行为呢？我们可以把它当作一个监督学习的问题，去收集很多行车记录器，然后再收集人在具体情境下会采取什么样的行为（训练数据）。你知道说人在状态$s_1$ 会采取动作$a_1$，人在状态$s_2$ 会采取动作$a_2$。人在状态, $s_3$ 会采取动作$a_3$。接下来，你就学习一个网络。这个网络就是演员，它输入$s_i$ 的时候，你就希望它的输出 是$a_i$，就这样结束了。它就是一个的监督学习的问题。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.2.png \':size=400\')\n\n行为克隆虽然非常简单，但它的问题是如果你只收集专家的资料，你可能看过的观测会是非常有限的。举例来说，如下图所示，假设你要学习一部自动驾驶汽车，自动驾驶汽车就是要过这个弯道。如果是专家的话，它就是把车顺着这个红线就开过去了。但假设智能体很笨，它开着开着就撞墙了，它永远不知道撞墙这种状况要怎么处理。因为训练数据里面从来没有撞过墙，所以它根本就不知道撞墙这一种情况要怎么处理。打电玩也是一样，让人去玩马里奥（Mario），那专家可能非常强，它从来不会跳不上水管，所以机器根本不知道跳不上水管时要怎么处理。\n\n\n所以光是做行为克隆是不够的，只观察专家的行为是不够的，需要一个招数，这个招数叫作`数据集聚合（dataset aggregation，DAgger）`。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.3.png \':size=300\')\n\n我们会希望收集更多样性的数据，而不是只收集专家所看到的观测。我们会希望能够收集专家在各种极端的情况下，它会采取什么样的行为。如下图所示，以自动驾驶汽车为例的话，假设一开始，我们有演员 $\\pi_1$，并且让 $\\pi_1$去开这个车，但车上坐了一个专家。这个专家会不断地告诉机器说，如果在这个情境里面，我会怎么样开。所以 $\\pi_1$ 自己开自己的，但是专家会不断地表示它的想法。比如说，一开始的时候，专家可能说往前走。在拐弯的时候，专家可能就会说往右转。但 $\\pi_1$ 是不管专家的指令的，所以它会继续去撞墙。虽然专家说往右转，但是不管它怎么下指令都是没有用的，$\\pi_1$ 会自己做自己的事情，因为我们要做的记录的是说，专家在 $\\pi_1$ 看到这种观测的情况下，它会做什么样的反应。这个方法显然是有一些问题的，因为你每开一次自动驾驶汽车就会牺牲一个人。那你用这个方法，你牺牲一个专家以后，你就会知道，人类在这样子的状态下，在快要撞墙的时候，会采取什么样的行为。再把这个数据拿去训练新的 $\\pi_2$。这个过程就反复继续下去，这个方法就叫做数据集聚合。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.4.png \':size=300\')\n\n行为克隆还有一个问题：机器会完全模仿专家的行为，不管专家的行为是否有道理，就算没有道理，没有什么用的，就算这是专家本身的习惯，机器也会硬把它记下来。如果机器确实可以记住所有专家的行为，也许还好。因为如果专家这么做，有些行为是多余的。但是没有问题，假设机器的行为可以完全仿造专家行为，也就算了，它就是跟专家一样得好，只是做一些多余的事。但问题是机器是一个网络，网络的容量是有限的。就算给网络训练数据，它在训练数据上得到的正确率往往也不是 100%，它有些事情是学不起来的。这个时候，什么该学，什么不该学就变得很重要。\n\n举例来说，如下图所示，在学习中文的时候，老师有语音、行为和知识，但其实只有语音部分是重要的，知识的部分是不重要的。也许机器只能够学一件事，也许它就只学到了语音，那没有问题。如果它只学到了手势，这样子就有问题了。所以让机器学习什么东西是需要模仿，什么东西是不需要模仿，这件事情是重要的。而单纯的行为克隆就没有把这件事情学进来，因为机器只是复制专家所有的行为而已，它不知道哪些行为是重要，是对接下来有影响的，哪些行为是不重要的，是对接下来是没有影响的。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.5.png \':size=450\')\n\n行为克隆还有一个问题：在做行为克隆的时候，训练数据跟测试数据是不匹配的。我们可以用数据集聚合的方法来缓解这个问题。在训练跟测试的时候，数据分布其实是不一样的。因为在强化学习里面，动作会影响到接下来所看到的状态。我们是先有状态$s_1$，然后采取动作$a_1$，动作$a_1$ 其实会决定接下来你看到什么样的状态$s_2$。所以在强化学习里面有一个很重要的特征，就是你采取了动作会影响你接下来所看到的状态，也就是会影响状态的分布。如果做了行为克隆的话，我们只能观察到专家$\\hat{\\pi}$的一堆状态跟动作的对$(s,a)$。\n\n然后我们希望可以学习一个 $\\pi^*$，我们希望 $\\pi^*$ 跟 $\\hat{\\pi}$ 越接近越好。如果 $\\pi^*$ 可以跟 $\\hat{\\pi}$ 一模一样的话，训练的时候看到的状态跟测试的时候所看到的状态会是一样的。因为虽然动作会影响我们看到的状态，但假设两个策略一模一样， 在同一个状态都会采取同样的动作，那你接下来所看到的状态都会是一样的。但问题就是你很难让学习出来的策略跟专家的策略一模一样。专家可是一个人，网络要跟人一模一样，有点困难。\n\n如果 $\\pi^*$ 跟 $\\hat{\\pi}$ 有一点误差。这个误差在一般监督学习问题里面，每一个样本（example）都是独立的，也许还好。但对强化学习的问题来说，可能在某个地方就是失之毫厘，差之千里。可能在某个地方，也许机器没有办法完全复制专家的行为，它复制的差了一点点，也许最后得到的结果就会差很多这样。所以行为克隆并不能够完全解决模仿学习这件事情，我们就有另外一个比较好的做法叫做逆强化学习。\n\n##  逆强化学习\n为什么叫逆强化学习，因为原来的强化学习里面，有一个环境和一个奖励函数。根据环境和奖励函数，通过强化学习这个技术，你会找到一个演员，你会学习出一个最优演员。但逆强化学习刚好是相反的，你没有奖励函数，你只有一堆专家的示范。但你还是有环境的。逆强化学习的做法是说假设我们现在有一堆专家的示范，我们用 $\\hat{\\tau}$ 来代表专家的示范。如果是在玩电玩的话，每一个 $\\tau$ 就是一个很会玩电玩的人玩一场游戏的纪录，如果是自动驾驶汽车的话，就是人开自动驾驶汽车的纪录。这一边就是专家的示范，每一个 $\\tau$ 是一个轨迹。\n\n\n把所有专家示范收集起来，然后，使用逆强化学习这个技术。使用逆强化学习技术的时候，机器是可以跟环境互动的。但它得不到奖励。它的奖励必须要从专家那边推出来，有了环境和专家示范以后，去反推出奖励函数长什么样子。之前强化学习是由奖励函数反推出什么样的动作、演员是最好的。逆强化学习是反过来，我们有专家的示范，我们相信它是不错的，我就反推说，专家是因为什么样的奖励函数才会采取这些行为。你有了奖励函数以后，接下来，你就可以套用一般的强化学习的方法去找出最优演员。所以逆强化学习是先找出奖励函数，找出奖励函数以后，再去用强化学习找出最优演员。\n\n把这个奖励函数学习出来，相较于原来的强化学习有什么样好处。一个可能的好处是也许奖励函数是比较简单的。也许，虽然这个专家的行为非常复杂，但也许简单的奖励函数就可以导致非常复杂的行为。一个例子就是也许人类本身的奖励函数就只有活着这样，每多活一秒，你就加一分。但人类有非常复杂的行为，但是这些复杂的行为，都只是围绕着要从这个奖励函数里面得到分数而已。有时候很简单的奖励函数也许可以推导出非常复杂的行为。\n\n\n逆强化学习实际上是怎么做的呢？如下图所示，首先，我们有一个专家$\\hat{\\pi}$，这个专家去跟环境互动，给我们很多轨迹：{$\\hat{\\tau_1}$,$\\hat{\\tau_2}$,$\\hat{\\tau_N}$}。如果是玩游戏的话，就让某一个电玩高手，去玩 $N$ 场游戏。把 $N$ 场游戏的状态跟动作的序列都记录下来。接下来，你有一个演员 $\\pi$，一开始演员很烂，这个演员也去跟环境互动。它也去玩了 $N$ 场游戏，它也有 $N$ 场游戏的纪录。接下来，我们要反推出奖励函数。怎么推出奖励函数呢？原则就是专家永远是最棒的，是先射箭，再画靶的概念。\n专家去玩一玩游戏，得到这一些游戏的纪录，演员也去玩一玩游戏，得到这些游戏的纪录。接下来，你要定一个奖励函数，这个奖励函数的原则就是专家得到的分数要比演员得到的分数高（先射箭，再画靶），所以我们就学习出一个奖励函数。你就找出一个奖励函数。这个奖励函数会使专家所得到的奖励大过于演员所得到的奖励。你有了新的奖励函数以后，就可以套用一般强化学习的方法去学习一个演员，这个演员会针对奖励函数去最大化它的奖励。它也会采取一大堆的动作。但是这个演员虽然可以最大化这个奖励函数，采取一大堆的行为，得到一大堆游戏的纪录。\n\n但接下来，我们就改奖励函数。这个演员就会很生气，它已经可以在这个奖励函数得到高分。但是它得到高分以后，我们就改奖励函数，仍然让专家可以得到比演员更高的分数。这个就是逆强化学习。有了新的奖励函数以后，根据这个新的奖励函数，你就可以得到新的演员，新的演员再去跟环境做一下互动，它跟环境做互动以后， 你又会重新定义奖励函数，让专家得到的奖励比演员大。\n\n怎么让专家得到的奖励大过演员呢？如下图所示，其实我们在学习的时候，奖励函数也许就是神经网络。这个神经网络就是输入 $\\tau$，输出就是应该要给这个 $\\tau$ 多少的分数。或者说，你假设觉得输入整个 $\\tau$ 太难了。因为 $\\tau$ 是 $s$ 和 $a$ 的一个很强的序列。也许它就是输入一个 $s$ 和 $a$ 的对，然后输出一个实数。把整个 $\\tau$ 会得到的实数都加起来就得到 $R(\\tau)$。在训练的时候，对于 $\\left\\{\\hat{\\tau}_{1}, \\hat{\\tau}_{2}, \\cdots, \\hat{\\tau}_{N}\\right\\}$，我们希望它输出的 $R$ 越大越好。对于 $\\left\\{\\tau_{1}, \\tau_{2}, \\cdots, \\tau_{N}\\right\\}$，我们就希望 $R$ 的值越小越好。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.7.png \':size=450\')\n\n什么叫做一个最好的奖励函数。最后你学习出来的奖励函数应该就是专家和演员在这个奖励函数都会得到一样高的分数。最终奖励函数没有办法分辨出谁应该会得到比较高的分数。通常在训练的时候，你会迭代地去做。最早的逆强化学习对奖励函数有些限制，它是假设奖励函数是线性的（linear） 。如果奖励函数是线性的话，你可以证明这个算法会收敛（converge）。但是如果不是线性的，你就没有办法证明说它会收敛。\n\n逆强化学习的框架如下图所示，其实我们只要把逆强化学习中的演员看成生成器，把奖励函数看成判别器，它就是 GAN。所以逆强化学习会不会收敛这个问题就等于是问说 GAN 会不会收敛。如果你已经实现过，你会知道不一定会收敛。但除非你对 $R$ 下一个非常严格的限制，如果 $R$ 是一个一般的网络的话，你就会有很大的麻烦。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.8.png \':size=450\')\n\n我们可以把逆强化学习跟 GAN 比较一下。\n如下图所示，GAN 里面，我们有一堆很好的图、一个生成器和一个判别器。一开始生成器不知道要产生什么样的图，它就乱画。判别器的工作就是给画的图打分，专家画的图就是高分，生成器画的图就是低分。生成器会想办法去骗过判别器，生成器会希望判别器 也会给它画的图高分。整个过程跟逆强化学习是一模一样的。画的图就是专家的示范。生成器就是 演员，生成器画很多图，演员 会去跟环境互动，产生很多轨迹。这些轨迹 跟环境互动的记录，游戏的纪录其实就是等于 GAN 里面的这些图。然后你学习一个奖励函数。奖励函数就是判别器。奖励函数要给专家的示范高分，给演员互动的结果低分。\n接下来，演员会想办法，从这个已经学习出来的奖励函数里面得到高分，然后迭代地去循环。跟 GAN 其实是一模一样的，我们只是换个说法而已。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.9.png \':size=450\')\n\n逆强化学习有很多的应用，比如可以用开来自动驾驶汽车，有人用这个技术来学开自动驾驶汽车的不同风格。每个人在开车的时候会有不同风格，举例来说，能不能够压到线，能不能够倒退，要不要遵守交通规则等等。每个人的风格是不同的，然后用逆强化学习可以让自动驾驶汽车学会各种不同的开车风格。\n\n下图是文献上真实的例子。在这个例子里面，逆强化学习有一个有趣的地方，通常你不需要太多的训练数据，因为训练数据往往都是个位数。因为逆强化学习只是一种示范，只是一种范例，实际上机器可以去跟环境互动非常多次。所以在逆强化学习的文献， 往往会看到说只用几笔数据就训练出一些有趣的结果。\n比如说，在这个例子里面是要让自动驾驶汽车学会在停车场里面停。这边的示范是这样，蓝色是终点，自动驾驶汽车要开到蓝色终点停车。给机器只看一行的四个示范，然后让它去学怎么样开车，最后它就可以学出，在红色的终点位置，如果它要停车的话，它会这样开。给机器看不同的示范，最后它学出来开车的风格就会不太一样。举例来说，上图第二行是不守规矩的开车方式，因为它会开到道路之外，这边，它会穿过其它的车，然后从这边开进去。所以机器就会学到说，不一定要走在道路上，它可以走非道路的地方。上图第三行是倒退来停车，机器也会学会说，它可以倒退，\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.11.png \':size=450\')\n\n这种技术也可以拿来训练机器人。你可以让机器人，做一些你想要它做的动作，过去如果你要训练机器人，做你想要它做的动作，其实是比较麻烦的。怎么麻烦呢？过去如果你要操控机器的手臂，你要花很多力气去写程序才让机器做一件很简单的事看。假设你有模仿学习的技术，你可以让人做一下示范，然后机器就跟着人的示范来进行学习，比如学会摆盘子，拉着机器人的手去摆盘子，机器自己动。让机器学会倒水，人只教它 20 次，杯子每次放的位置不太一样。用这种方法来教机械手臂。\n\n## 第三人称视角模仿学习\n\n其实还有很多相关的研究，如下图所示，举例来说，你在教机械手臂的时候，要注意就是也许机器看到的视野跟人看到的视野是不太一样的。在刚才那个例子里面，人跟机器的动作是一样的。但是在未来的世界里面，也许机器是看着人的行为学的。刚才是人拉着，假设你要让机器学会打高尔夫球，在刚才的例子里面就是人拉着机器人手臂去打高尔夫球，但是在未来有没有可能机器就是看着人打高尔夫球，它自己就学会打高尔夫球了呢？但这个时候，要注意的事情是机器的视野跟它真正去采取这个行为的时候的视野是不一样的。机器必须了解到当它是第三人的视角的时候，看到另外一个人在打高尔夫球，跟它实际上自己去打高尔夫球的时候，看到的视野显然是不一样的。但它怎么把它是第三人称视角所观察到的经验把它泛化到它是第一人称视角的时候所采取的行为，这就需要用到`第三人称视角模仿学习（third person imitation learning）`的技术。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.13.png \':size=450\')\n\n这个怎么做呢？它的技术其实也是不只是用到模仿学习，它用到了`领域对抗训练（domain-adversarial Training）`。我们在讲领域对抗训练的时候，我们有讲说这也是一个 GAN 的技术。如下图 所示，我们希望有一个提取器，有两个不同领域（domain）的图像，通过特征提取器以后，没有办法分辨出它来自哪一个领域。其实第一人称视角和第三人称视角，模仿学习用的技术其实也是一样的，希望学习一个特征提取器，机器在第三人称的时候跟它在第一人称的时候看到的视野其实是一样的，就是把最重要的东西抽出来就好了。 \n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.14.png \':size=450\')\n\n## 序列生成和聊天机器人\n在讲序列生成对抗网络（sequence GAN）的时候，我们有讲过句子生成（sentence generation）跟聊天机器人。那其实句子生成或聊天机器人 也可以想成是模仿学习。\n如下图所示，机器在模仿人写的句子，你在写句子的时候，你写下去的每一个字都想成是一个动作，所有的字合起来就是一个回合。举例来说，句子生成里面，你会给机器看很多人类写的文字。你要让机器学会写诗，那你就要给它看唐诗三百首。人类写的文字其实就是专家的示范。每一个词汇其实就是一个动作。你让机器做句子生成的时候，其实就是在模仿专家的轨迹。聊天机器人也是一样，在聊天机器人 里面你会收集到很多人互动对话的纪录，那些就是专家的示范。\n\n如果我们单纯用最大似然（maximum likelihood）这个技术来最大化会得到似然（likelihood），这个其实就是行为克隆。行为克隆就是看到一个状态，接下来预测我们会得到什么样的动作，有一个标准答案（ground truth）告诉机器说什么样的动作是最好的。在做似然的时候也是一样，给定句子已经产生的部分。接下来机器要预测说接下来要写哪一个字才是最好的。所以，其实最大似然在做序列生成（sequence generation）的时候，它对应到模仿学习里面就是行为克隆。只有最大似然是不够的，我们想要用序列生成对抗网络。其实序列生成对抗网络就是对应到逆强化学习，逆强化学习就是一种 GAN 的技术。你把逆强化学习的技术放在句子生成，放到聊天机器人里面，其实就是序列生成对抗网络跟它的种种的变形。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/11.15.png \':size=450\')\n\n## References\n\n* [机器学习](https://book.douban.com/subject/26708119//)\n* [模仿学习（Imitation Learning）介绍](https://zhuanlan.zhihu.com/p/25688750)\n','2021-12-10 12:43:49','2021-12-19 19:42:12'),
	(57,3,'第十一章 习题','## 1 Keywords\r\n\r\n- **Imitation learning：**  其讨论我们没有reward或者无法定义reward但是有与environment进行交互时怎么进行agent的学习。这与我们平时处理的问题中的情况有些类似，因为通常我们无法从环境中得到明确的reward。Imitation learning 又被称为 learning from demonstration (示范学习) ，apprenticeship learning (学徒学习)，learning by watching (观察学习)等。\r\n- **Behavior Cloning：** 类似于ML中的监督学习，通过收集expert的state与action的对应信息，训练我们的network（actor）。在使用时input state时，得到对应的outpur action。\r\n- **Dataset Aggregation：** 用来应对在Behavior Cloning中expert提供不到的data，其希望收集expert在各种极端state下expert的action。\r\n- **Inverse Reinforcement learning（IRL）：** Inverse Reinforcement Learning 是先找出 reward function，再去用 Reinforcement Learning 找出 optimal actor。这么做是因为我们没有环境中reward，但是我们有expert 的demonstration，使用IRL，我们可以推断expert 是因为什么样的 reward function 才会采取这些action。有了reward function 以后，接下来，就可以套用一般的 reinforcement learning 的方法去找出 optimal actor。\r\n- **Third Person Imitation Learning：** 一种把第三人称视角所观察到的经验 generalize 到第一人称视角的经验的技术。\r\n\r\n## 2 Questions\r\n\r\n- 对于Imitation Learning 的方法有哪些？\r\n\r\n  答：Behavior Cloning、Inverse Reinforcement Learning（IRL）或者称为Inverse Optimal Control。\r\n\r\n- Behavior Cloning存在哪些问题呢？我们可以如何处理呢？\r\n\r\n  答：\r\n\r\n  1. 首先，如果只收集expert 的data（看到某一个state输出的action），你可能看过的 observation 会是非常 limited。所以我们要收集expert在各种极端state下的action，或者说是要收集更多的、复杂的data，可以使用教程中提到的Dataset Aggregation。\r\n  2. 另外，使用传统意义上的Behavior Cloning的话，机器会完全 copy expert 的行为，不管 expert 的行为是否有道理，就算没有道理，没有什么用的，这是expert 本身的习惯，机器也会硬把它记下来。我们的agent是一个 machine，它是一个 network，network 的capacity 是有限的。就算给 network training data，它在training data 上得到的正确率往往也不是100%，他有些事情是学不起来的。这个时候，什么该学，什么不该学就变得很重要。不过极少数expert的行为是没有意义的，但是至少也不会产生较坏的影响。\r\n  3. 还有，在做 Behavior Cloning 的时候，training data 跟 testing data 是 mismatch 的。我们可以用 Dataset Aggregation 的方法来缓解这个问题。这个问题是，在 training 跟 testing 的时候，data distribution 其实是不一样的。因为在 reinforcement learning 里面，action 会影响到接下来所看到的 state。我们是先有 state $s_1$，然后采取 action $a_1$，action $a_1$ 其实会决定接下来你看到什么样的 state $s_2$。所以在 reinforcement learning 里面有一个很重要的特征，就是你采取了 action 会影响你接下来所看到的 state。如果做了Behavior Cloning 的话，我们只能观察到 expert 的一堆 state 跟 action 的pair。然后我们希望可以 learn 一个 $\\pi^*$，我们希望 $\\pi^*$ 跟 $\\hat{\\pi}$ 越接近越好。如果 $\\pi^*$ 可以跟 $\\hat{\\pi}$ 一模一样的话，你 training 的时候看到的 state 跟 testing 的时候所看到的 state 会是一样的，这样模型的泛化性能就会变得比较差。而且，如果你的 $\\pi^*$ 跟 $\\hat{\\pi}$ 有一点误差。这个误差在一般 supervised learning problem 里面，每一个 example 都是 independent 的，也许还好。但对 reinforcement learning 的 problem 来说，可能在某个地方，也许 machine 没有办法完全复制 expert 的行为，也许最后得到的结果就会差很多。所以 Behavior Cloning 并不能够完全解决 Imatation learning 这件事情，我们可以使用另外一个比较好的做法叫做 Inverse Reinforcement Learning。\r\n\r\n\r\n- Inverse Reinforcement Learning 是怎么运行的呢？\r\n\r\n  答：首先，我们有一个 expert $\\hat{\\pi}$，这个 expert 去跟环境互动，给我们很多 $\\hat{\\tau_1}$ 到 $\\hat{\\tau_n}$，我们需要将其中的state、action这个序列都记录下来。然后对于actor $\\pi$ 也需要进行一样的互动和序列的记录。接着我们需要指定一个reward function，并且保证expert对应的分数一定要比actor的要高，用过这个reward function继续learning更新我们的训练并且套用一般条件下的RL方法进行actor的更新。在这个过程中，我们也要同时进行我们一开始制定的reward function的更新，使得actor得得分越来越高，但是不超过expert的得分。最终的reward function应该让expert和actor对应的reward function都达到比较高的分数，并且从最终的reward function中无法分辨出谁应该得到比较高的分数。\r\n\r\n- Inverse Reinforcement Learning 方法与GAN在图像生成中有什么异曲同工之处?\r\n\r\n  答：在GAN 中，我们有一些比较好的图片数据集，也有一个generator，一开始他根本不知道要产生什么样的图，只能随机生成。另外我们有一个discriminator，其用来给生成的图打分，expert 生成的图得分高，generator 生成的图得分低。有了discriminator 以后，generator 会想办法去骗过 discriminator。Generator 会希望 discriminator 也会给它生成得图高分。整个 process 跟 IRL 的过程是类似的。我们一一对应起来看：\r\n\r\n  * 生成的图就是 expert 的 demonstration，generator 就是actor，generator 会生成很多的图并让actor 与环境进行互动，从而产生很多 trajectory。这些 trajectory 跟环境互动的记录等价于 GAN 里面的生成图。\r\n  * 在IRL中 learn 的 reward function 就是 discriminator。Rewards function 要给 expert 的 demonstration 高分，给 actor 互动的结果低分。\r\n  * 考虑两者的过程，在IRL中，actor 会想办法，从这个已经 learn 出来的 reward function 里面得到高分，然后 iterative 地去循环这其实是与 GAN 的过程是一致的。\r\n','2021-12-10 12:43:49','2021-12-19 19:42:27'),
	(58,3,'第十二章 DDPG算法','## 离散动作 vs. 连续动作\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.1.png)\n离散动作与连续动作是相对的概念，一个是可数的，一个是不可数的。 \n\n* 离散动作有如下几个例子：\n  * 在 CartPole 环境中，可以有向左推小车、向右推小车两个动作。\n  * 在 Frozen Lake 环境中，小乌龟可以有上下左右四个动作。\n  * 在 Atari 的 Pong 游戏中，游戏有 6 个按键的动作可以输出。\n* 但在实际情况中，经常会遇到连续动作空间的情况，也就是输出的动作是不可数的。比如：\n  * 推小车力的大小，\n  * 选择下一时刻方向盘的转动角度，\n  * 四轴飞行器的四个螺旋桨给的电压的大小。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.2.png)\n\n对于这些连续的动作控制空间，Q-learning、DQN 等算法是没有办法处理的。那我们怎么输出连续的动作呢，这个时候，万能的神经网络又出现了。\n\n* 在离散动作的场景下，比如说我输出上、下或是停止这几个动作。有几个动作，神经网络就输出几个概率值，我们用 $\\pi_\\theta(a_t|s_t)$ 来表示这个随机性的策略。\n\n* 在连续的动作场景下，比如说我要输出这个机器人手臂弯曲的角度，这样子的一个动作，我们就输出一个具体的浮点数。我们用 $\\mu_{\\theta}(s_t)$ 来代表这个确定性的策略。\n\n我们再解释一下随机性策略跟确定性策略。\n\n* 对随机性的策略来说，输入某一个状态 s，采取某一个 action 的可能性并不是百分之百，而是有一个概率 P 的，就好像抽奖一样，根据概率随机抽取一个动作。\n* 而对于确定性的策略来说，它没有概率的影响。当神经网络的参数固定下来了之后，输入同样的 state，必然输出同样的 action，这就是确定性的策略。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.3.png)\n\n* 要输出离散动作的话，我们就是加一层 softmax 层来确保说所有的输出是动作概率，而且所有的动作概率加和为 1。\n* 要输出连续动作的话，一般可以在输出层这里加一层 tanh。\n  * tanh 的图像的像右边这样子，它的作用就是把输出限制到 [-1,1] 之间。\n  * 我们拿到这个输出后，就可以根据实际动作的范围再做一下缩放，然后再输出给环境。\n  * 比如神经网络输出一个浮点数是 2.8，然后经过 tanh 之后，它就可以被限制在 [-1,1] 之间，它输出 0.99。假设小车速度的动作范围是 [-2,2] 之间，那我们就按比例从 [-1,1] 扩放到 [-2,2]，0.99 乘 2，最终输出的就是 1.98，作为小车的速度或者说推小车的力输出给环境。\n\n## DDPG(Deep Deterministic Policy Gradient)\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.4.png)\n\n在连续控制领域，比较经典的强化学习算法就是 `深度确定性策略梯度(Deep Deterministic Policy Gradient，简称 DDPG)`。DDPG 的特点可以从它的名字当中拆解出来，拆解成 Deep、Deterministic 和 Policy Gradient。\n\n* Deep 是因为用了神经网络；\n* Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；\n\n* Policy Gradient 代表的是它用到的是策略网络。REINFORCE 算法每隔一个 episode 就更新一次，但 DDPG 网络是每个 step 都会更新一次 policy 网络，也就是说它是一个单步更新的 policy 网络。\n\nDDPG 是 DQN 的一个扩展的版本。\n\n* 在 DDPG 的训练中，它借鉴了 DQN 的技巧：目标网络和经验回放。\n\n* 经验回放这一块跟 DQN 是一样的，但 target network 这一块的更新跟 DQN 有点不一样。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.5.png)\n**提出 DDPG 是为了让 DQN 可以扩展到连续的动作空间**，就是我们刚才提到的小车速度、角度和电压的电流量这样的连续值。\n\n* DDPG 直接在 DQN 基础上加了一个策略网络来直接输出动作值，所以 DDPG 需要一边学习 Q 网络，一边学习策略网络。\n* Q 网络的参数用 $w$ 来表示。策略网络的参数用 $\\theta$ 来表示。\n* 我们称这样的结构为 `Actor-Critic` 的结构。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.6.png)\n**通俗地解释一下 Actor-Critic 的结构**，\n\n* 策略网络扮演的就是 actor 的角色，它负责对外展示输出，输出舞蹈动作。\n* Q 网络就是评论家(critic)，它会在每一个 step 都对 actor 输出的动作做一个评估，打一个分，估计一下 actor 的 action 未来能有多少收益，也就是去估计这个 actor 输出的这个 action 的 Q 值大概是多少，即 $Q_w(s,a)$。 Actor 就需要根据舞台目前的状态来做出一个 action。\n* 评论家就是评委，它需要根据舞台现在的状态和演员输出的 action 对 actor 刚刚的表现去打一个分数 $Q_w(s,a)$。\n  * Actor 根据评委的打分来调整自己的策略，也就是更新 actor 的神经网络参数 $\\theta$， 争取下次可以做得更好。\n  * Critic 则是要根据观众的反馈，也就是环境的反馈 reward 来调整自己的打分策略，也就是要更新 critic 的神经网络的参数 $w$ ，它的目标是要让每一场表演都获得观众尽可能多的欢呼声跟掌声，也就是要最大化未来的总收益。\n* 最开始训练的时候，这两个神经网络参数是随机的。所以 critic 最开始是随机打分的，然后 actor 也跟着乱来，就随机表演，随机输出动作。但是由于我们有环境反馈的 reward 存在，所以 critic 的评分会越来越准确，也会评判的那个 actor 的表现会越来越好。\n* 既然 actor 是一个神经网络，是我们希望训练好的策略网络，那我们就需要计算梯度来去更新优化它里面的参数 $\\theta$ 。简单的说，我们希望调整 actor 的网络参数，使得评委打分尽可能得高。注意，这里的 actor 是不管观众的，它只关注评委，它就是迎合评委的打分 $Q_w(s,a)$ 而已。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.7.png)\n\n接下来就是类似 DQN。\n\n* DQN 的最佳策略是想要学出一个很好的 Q 网络，学好这个网络之后，我们希望选取的那个动作使你的 Q 值最大。\n\n* DDPG 的目的也是为了求解让 Q 值最大的那个 action。\n  * Actor 只是为了迎合评委的打分而已，所以用来优化策略网络的梯度就是要最大化这个 Q 值，所以构造的 loss 函数就是让 Q 取一个负号。\n  * 我们写代码的时候就是把这个 loss 函数扔到优化器里面，它就会自动最小化 loss，也就是最大化 Q。\n\n这里要注意，除了策略网络要做优化，DDPG 还有一个 Q 网络也要优化。\n\n* 评委一开始也不知道怎么评分，它也是在一步一步的学习当中，慢慢地去给出准确的打分。\n* 我们优化 Q 网络的方法其实跟 DQN 优化 Q 网络的方法是一样的，我们用真实的 reward $r$ 和下一步的 Q 即 Q\' 来去拟合未来的收益 Q_target。\n\n* 然后让 Q 网络的输出去逼近这个 Q_target。\n  * 所以构造的 loss function 就是直接求这两个值的均方差。\n  * 构造好 loss 后，我们就扔进去那个优化器，让它自动去最小化 loss 就好了。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.8.png)\n\n我们可以把两个网络的 loss function 构造出来。\n\n策略网络的 loss function 是一个复合函数。我们把 $a = \\mu_\\theta(s)$ 代进去，最终策略网络要优化的是策略网络的参数 $\\theta$ 。Q 网络要优化的是 $Q_w(s,a)$ 和 Q_target 之间的一个均方差。\n\n但是 Q 网络的优化存在一个和 DQN 一模一样的问题就是它后面的 Q_target 是不稳定的。此外，后面的 $Q_{\\bar{w}}\\left(s^{\\prime}, a^{\\prime}\\right)$ 也是不稳定的，因为 $Q_{\\bar{w}}\\left(s^{\\prime}, a^{\\prime}\\right)$ 也是一个预估的值。\n\n**为了稳定这个 Q_target，DDPG 分别给 Q 网络和策略网络都搭建了 target network。**\n\n* target_Q 网络就为了来计算 Q_target 里面的 $Q_{\\bar{w}}\\left(s^{\\prime}, a^{\\prime}\\right)$。\n* $Q_{\\bar{w}}\\left(s^{\\prime}, a^{\\prime}\\right)$  里面的需要的 next action $a\'$  就是通过 target_P 网络来去输出，即 $a^{\\prime}=\\mu_{\\bar{\\theta}}\\left(s^{\\prime}\\right)$。\n\n* 为了区分前面的 Q 网络和策略网络以及后面的 target_Q 网络和 target_P 策略网络，前面的网络的参数是 $w$，后面的网络的参数是 $\\bar{w}$。\n* DDPG 有四个网络，策略网络的 target 网络 和 Q 网络的 target 网络就是颜色比较深的这两个，它只是为了让计算 Q_target 的时候能够更稳定一点而已。因为这两个网络也是固定一段时间的参数之后再跟评估网络同步一下最新的参数。\n\n这里面训练需要用到的数据就是 $s,a,r,s\'$，我们只需要用到这四个数据。我们就用 Replay Memory 把这些数据存起来，然后再 sample 进来训练就好了。这个经验回放的技巧跟 DQN 是一模一样的。注意，因为 DDPG 使用了经验回放这个技巧，所以 DDPG 是一个 `off-policy` 的算法。\n\n### Exploration vs. Exploitation\nDDPG 通过 off-policy 的方式来训练一个确定性策略。因为策略是确定的，如果 agent 使用同策略来探索，在一开始的时候，它会很可能不会尝试足够多的 action 来找到有用的学习信号。为了让 DDPG 的策略更好地探索，我们在训练的时候给它们的 action 加了噪音。DDPG 的原作者推荐使用时间相关的 [OU noise](https://en.wikipedia.org/wiki/Ornstein–Uhlenbeck_process)，但最近的结果表明不相关的、均值为 0 的 Gaussian noise 的效果非常好。由于后者更简单，因此我们更喜欢使用它。为了便于获得更高质量的训练数据，你可以在训练过程中把噪声变小。\n\n在测试的时候，为了查看策略利用它学到的东西的表现，我们不会在 action 中加噪音。\n\n## Twin Delayed DDPG(TD3)\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.9.png \'size=500\')\n\n虽然 DDPG 有时表现很好，但它在超参数和其他类型的调整方面经常很敏感。DDPG 常见的问题是已经学习好的 Q 函数开始显著地高估 Q 值，然后导致策略被破坏了，因为它利用了 Q 函数中的误差。\n\n我们可以拿实际的 Q 值跟这个 Q-network 输出的 Q 值进行对比。实际的 Q 值可以用 MC 来算。根据当前的 policy 采样 1000 条轨迹，得到 G 后取平均，得到实际的 Q 值。\n\n`双延迟深度确定性策略梯度(Twin Delayed DDPG，简称 TD3)`通过引入三个关键技巧来解决这个问题：\n\n* **截断的双 Q 学习(Clipped Dobule Q-learning)** 。TD3 学习两个 Q-function（因此名字中有 “twin”）。TD3 通过最小化均方差来同时学习两个 Q-function：$Q_{\\phi_1}$ 和 $Q_{\\phi_2}$。两个 Q-function 都使用一个目标，两个 Q-function 中给出较小的值会被作为如下的 Q-target：\n\n$$\ny\\left(r, s^{\\prime}, d\\right)=r+\\gamma(1-d) \\min _{i=1,2} Q_{\\phi_{i, t a r g}}\\left(s^{\\prime}, a_{T D 3}\\left(s^{\\prime}\\right)\\right)\n$$\n\n* **延迟的策略更新(“Delayed” Policy Updates)** 。相关实验结果表明，同步训练动作网络和评价网络，却不使用目标网络，会导致训练过程不稳定；但是仅固定动作网络时，评价网络往往能够收敛到正确的结果。因此 TD3 算法以较低的频率更新动作网络，较高频率更新评价网络，通常每更新两次评价网络就更新一次策略。\n* **目标策略平滑(Target Policy smoothing)** 。TD3 引入了 smoothing 的思想。TD3 在目标动作中加入噪音，通过平滑 Q 沿动作的变化，使策略更难利用 Q 函数的误差。\n\n这三个技巧加在一起，使得性能相比基线 DDPG 有了大幅的提升。\n\n\n目标策略平滑化的工作原理如下：\n\n$$\na_{T D 3}\\left(s^{\\prime}\\right)=\\operatorname{clip}\\left(\\mu_{\\theta, t a r g}\\left(s^{\\prime}\\right)+\\operatorname{clip}(\\epsilon,-c, c), a_{\\text {low }}, a_{\\text {high }}\\right)\n$$\n\n其中 $\\epsilon$ 本质上是一个噪声，是从正态分布中取样得到的，即 $\\epsilon \\sim N(0,\\sigma)$。\n\n目标策略平滑化是一种正则化方法。\n\n![](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/12.10.png)\n\n我们可以将 TD3 跟其他算法进行对比。这边作者自己实现的 DDPG(our DDPG) 和官方实现的 DDPG 的表现不一样，这说明 DDPG 对初始化和调参非常敏感。TD3 对参数不是这么敏感。在 TD3 的论文中，TD3 的性能比 SAC(Soft Actor-Critic) 高。但在 SAC 的论文中，SAC 的性能比 TD3 高，这是因为强化学习的很多算法估计对参数和初始条件敏感。\n\nTD3 的作者给出了对应的实现：[TD3 Pytorch implementation](https://github.com/sfujim/TD3/)，代码写得很棒，我们可以将其作为一个强化学习的标准库来学习。\n\n### Exploration vs. Exploitation\n\nTD3 以 off-policy 的方式训练确定性策略。由于该策略是确定性的，因此如果智能体要探索策略，则一开始它可能不会尝试采取足够广泛的动作来找到有用的学习信号。为了使 TD3 策略更好地探索，我们在训练时在它们的动作中添加了噪声，通常是不相关的均值为零的高斯噪声。为了便于获取高质量的训练数据，你可以在训练过程中减小噪声的大小。\n\n在测试时，为了查看策略对所学知识的利用程度，我们不会在动作中增加噪音。\n\n## References\n\n* [百度强化学习](https://aistudio.baidu.com/aistudio/education/lessonvideo/460292)\n\n* [OpenAI Spinning Up ](https://spinningup.openai.com/en/latest/algorithms/ddpg.html#)\n\n* [Intro to Reinforcement Learning (强化学习纲要）](https://github.com/zhoubolei/introRL)\n\n* [天授文档](https://tianshou.readthedocs.io/zh/latest/index.html)\n\n\n','2021-12-10 12:43:49','2021-12-19 19:42:44'),
	(59,3,'第十二章 习题','## 1 Keywords\r\n\r\n- **DDPG(Deep Deterministic Policy Gradient)：** 在连续控制领域经典的RL算法，是DQN在处理连续动作空间的一个扩充。具体地，从命名就可以看出，Deep是使用了神经网络；Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；Policy Gradient 代表的是它用到的是策略网络，并且每个 step 都会更新一次 policy 网络，也就是说它是一个单步更新的 policy 网络。其与DQN都有目标网络和经验回放的技巧，在经验回放部分是一致的，在目标网络的更新有些许不同。\r\n\r\n## 2 Questions\r\n\r\n- 请解释随机性策略和确定性策略。\r\n\r\n  答：\r\n\r\n  - 对于随机性的策略 $\\pi_\\theta(a_t|s_t)$ ，我们输入某一个状态 s，采取某一个 action 的可能性并不是百分之百，而是有一个概率 P 的，就好像抽奖一样，根据概率随机抽取一个动作。\r\n  - 对于确定性的策略 $\\mu_{\\theta}(s_t)$ ，其没有概率的影响。当神经网络的参数固定下来了之后，输入同样的state，必然输出同样的 action，这就是确定性的策略。\r\n\r\n- 对于连续动作的控制空间和离散动作的控制空间，如果我们都采取使用Policy网络的话，分别应该如何操作？\r\n\r\n  答：首先需要说明的是，对于连续的动作控制空间，Q-learning、DQN等算法是没有办法处理的，所以我们需要使用神经网络进行处理，因为其可以既输出概率值 $\\pi_\\theta(a_t|s_t)$ ，也可以输出确定的策略 $\\mu_{\\theta}(s_t)$ 。\r\n\r\n  - 要输出离散动作的话，最后的output的激活函数使用 softmax 就可以实现。其可以保证输出是的动作概率，而且所有的动作概率加和为 1。\r\n  - 要输出连续的动作的话，可以在输出层这里加一层 tanh激活函数。其作用可以把输出限制到 [-1,1] 之间。我们拿到这个输出后，就可以根据实际动作的一个范围再做一下缩放，然后再输出给环境。比如神经网络输出一个浮点数是 2.8，然后经过 tanh 之后，它就可以被限制在 [-1,1] 之间，它输出 0.99。然后假设说小车的一个速度的那个动作范围是 [-2,2] 之间，那我们就按比例从 [-1,1] 扩放到 [-2,2]，0.99 乘 2，最终输出的就是1.98，作为小车的速度或者说推小车的力输出给环境。\r\n\r\n\r\n\r\n## 3 Something About Interview\r\n\r\n- 高冷的面试官：请简述一下DDPG算法？\r\n\r\n  答：深度确定性策略梯度(Deep Deterministic Policy Gradient，简称 DDPG) 使用 Actor Critic 结构，但是输出的不是行为的概率,，而是具体的行为，用于连续动作的预测。优化的目的是为了将DQN扩展到连续的动作空间。另外，其字如其名：\r\n\r\n  - Deep 是因为用了神经网络；\r\n  - Deterministic 表示 DDPG 输出的是一个确定性的动作，可以用于连续动作的一个环境；\r\n  - Policy Gradient 代表的是它用到的是策略网络。REINFORCE 算法每隔一个 episode 就更新一次，但 DDPG 网络是每个 step 都会更新一次 policy 网络，也就是说它是一个单步更新的 policy 网络。\r\n\r\n- 高冷的面试官：你好，请问DDPG是on-policy还是off-policy，原因是什么呀？\r\n\r\n  答：off-policy。解释方法一，DDPG是优化的DQN，其使用了经验回放，所以为off-policy方法；解释方法二，因为DDPG为了保证一定的探索，对于输出动作加了一定的噪音，也就是说行为策略不再是优化的策略。\r\n\r\n- 高冷的面试官：你是否了解过D4PG算法呢？描述一下吧。\r\n\r\n  答：分布的分布式DDPG（Distributed Distributional DDPG ，简称 D4PG)，相对于DDPG其优化部分为： \r\n\r\n  - 分布式 critic: 不再只估计Q值的期望值，而是去估计期望Q值的分布, 即将期望Q值作为一个随机变量来进行估计。\r\n  - N步累计回报: 当计算TD误差时，D4PG计算的是N步的TD目标值而不仅仅只有一步，这样就可以考虑未来更多步骤的回报。\r\n  - 多个分布式并行actor：D4PG使用K个独立的演员并行收集训练样本并存储到同一个replay buffer中。\r\n  - 优先经验回放（Prioritized Experience Replay，PER）：使用一个非均匀概率 $\\pi$ 从replay buffer中采样。\r\n  \r\n  \r\n  \r\n  \r\n','2021-12-10 12:43:49','2021-12-19 19:42:56'),
	(60,3,'项目三 使用Policy-Based方法实现Pendulum-v0','## 使用Policy-Based方法实现Pendulum-v0\n\n使用Policy-Based方法比如DDPG等实现Pendulum-v0环境\n\n## Pendulum-v0\n\n![image-20200820174814084](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/image-20200820174814084.png)\n\n钟摆以随机位置开始，目标是将其摆动，使其保持向上直立。动作空间是连续的，值的区间为[-2,2]。每个step给的reward最低为-16.27，最高为0。\n\n环境建立如下：\n\n```python\nenv = gym.make(\'Pendulum-v0\') \nenv.seed(1) # 设置env随机种子\nn_states = env.observation_space.shape[0] # 获取总的状态数\n```\n\n## 强化学习基本接口\n\n```python\nrewards = [] # 记录总的rewards\nmoving_average_rewards = [] # 记录总的经滑动平均处理后的rewards\nep_steps = []\nfor i_episode in range(1, cfg.max_episodes+1): # cfg.max_episodes为最大训练的episode数\n    state = env.reset() # reset环境状态\n    ep_reward = 0\n    for i_step in range(1, cfg.max_steps+1): # cfg.max_steps为每个episode的补偿\n        action = agent.select_action(state) # 根据当前环境state选择action\n        next_state, reward, done, _ = env.step(action) # 更新环境参数\n        ep_reward += reward\n        agent.memory.push(state, action, reward, next_state, done) # 将state等这些transition存入memory\n        state = next_state # 跳转到下一个状态\n        agent.update() # 每步更新网络\n        if done:\n            break\n    # 更新target network，复制DQN中的所有weights and biases\n    if i_episode % cfg.target_update == 0: #  cfg.target_update为target_net的更新频率\n        agent.target_net.load_state_dict(agent.policy_net.state_dict())\n    print(\'Episode:\', i_episode, \' Reward: %i\' %\n          int(ep_reward), \'n_steps:\', i_step, \'done: \', done,\' Explore: %.2f\' % agent.epsilon)\n    ep_steps.append(i_step)\n    rewards.append(ep_reward)\n    # 计算滑动窗口的reward\n    if i_episode == 1:\n        moving_average_rewards.append(ep_reward)\n    else:\n        moving_average_rewards.append(\n            0.9*moving_average_rewards[-1]+0.1*ep_reward)\n```\n\n## 任务要求\n\n训练并绘制reward以及滑动平均后的reward随episode的变化曲线图并记录超参数写成报告，图示如下：\n\n![rewards_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_rewards_train.png)\n\n![moving_average_rewards_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_moving_average_rewards_train.png)\n\n![steps_train](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_steps_train.png)\n\n同时也可以绘制测试(eval)模型时的曲线：\n\n![rewards_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_rewards_eval.png)\n\n![moving_average_rewards_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_moving_average_rewards_eval.png)\n\n![steps_eval](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/p3_steps_eval.png)\n\n也可以[tensorboard](https://pytorch.org/docs/stable/tensorboard.html)查看结果，如下：\n\n![image-20201015221602396](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/assets/image-20201015221602396.png)\n\n### 注意\n\n1. 本次环境action范围在[-2,2]之间，而神经网络中输出的激活函数tanh在[0,1]，可以使用NormalizedActions(gym.ActionWrapper)的方法解决\n2. 由于本次环境为惯性系统，建议增加Ornstein-Uhlenbeck噪声提高探索率，可参考[知乎文章](https://zhuanlan.zhihu.com/p/96720878)\n3. 推荐多次试验保存rewards，然后使用searborn绘制，可参考[CSDN](https://blog.csdn.net/JohnJim0/article/details/106715402)\n\n### 代码清单\n\n**main.py**：保存强化学习基本接口，以及相应的超参数，可使用argparse\n\n**model.py**：保存神经网络，比如全链接网络\n\n**ddpg.py**: 保存算法模型，主要包含select_action和update两个函数\n\n**memory.py**：保存Replay Buffer\n\n**plot.py**：保存相关绘制函数\n\n**noise.py**：保存噪声相关\n\n[参考代码](https://github.com/datawhalechina/easy-rl/tree/master/codes/DDPG)\n\n','2021-12-10 12:43:49','2021-12-19 19:43:10'),
	(61,3,'第十三章 AlphaStar论文解读','## AlphaStar以及背景简介\n\n相比于之前的深蓝和go，对于星际争霸2等策略对战型游戏，使用AI与人类对战难度更大。比如在星际争霸2中，操作枯燥是众所周知的，要想在PVP中击败对方，就得要学会各种战术，各种微操和Timing。在游戏中你还得侦查对方的发展，做出正确判断进行转型，甚至要欺骗对方以达到战术目的。总而言之，想要上手这款游戏是非常困难的，对不起，DeepMind就做到了。\n\nAlphaStar是DeepMind公司与暴雪使用深度强化学习技术进行PC与星际争霸2人类玩家进行对战的产品，其在近些年在星际争霸2中打败了职业选手以及99.8%的欧服玩家而被人所熟知。北京时间2019年1月25日凌晨2点，暴雪与谷歌DeepMind团队合作研究的星际争霸人工智能“AlphaStar”正式通过直播亮相。按照直播安排，AlphaStar与两位《星际争霸2》人类职业选手进行了5场比赛对决演示。加上并未在直播中演示的对决，在人类vs AlphaStar人工智能的共计11场比赛中，人类只取得了一场胜利。DeepMind也将研究工作发表在了2019年10月的《Nature》杂志上。我们也将对于这篇Paper进行深入的分析，下面是论文的链接：\n\n[Vinyals, Oriol, et al. \"Grandmaster level in StarCraft II using multi-agent reinforcement learning.\" Nature (2019): 1-5.](https://www.nature.com/articles/s41586-019-1724-z?)\n\n## AlphaStar的模型输入输出是什么？——环境设计\n\n构建DRL模型的第一部分就是构建输入输出，对于星际争霸2这个复杂的环境，paper第一步做的就是将游戏的环境抽象成为许多的数据信息。\n\n### 状态（网络的输入）\n\nAlphaStar将星际争霸2的环境状态分为四部分，分别为实体信息（Entities）、地图信息（Map）、玩家数据信息（Player data）、游戏统计信息（Game statistics）。\n\n![img1](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.1.png)\n\n- 第一部分：实体信息，例如当前时刻环境中有什么建筑、兵种等等，并且我们将每一个实体的属性信息以向量的形式表示，例如对于一个建筑，其当前时刻的向量中包含此建筑的血量、等级、位置以及冷却时间等等信息。所以对于当前帧的全部实体信息，环境会给神经网络 $N$ 个长度为 $K$ 的向量，各表示此刻智能体能够看见的 $N$ 个实体的具体信息。（向量信息）\n- 第二部分：地图信息，这个比较好理解，也就是将地图中的信息以矩阵的形式送入神经网络中，来表示当前状态全局地图的信息。（向量信息或者说是图像信息）\n- 第三部分：玩家数据信息，也就是当前状态下，玩家的等级、种族等等信息。（标量信息）\n- 第四部分：游戏统计信息，相机的位置（小窗口的位置，区别于第二部分的全局地图信息），还有当前游戏的开始时间等等信息。（标量信息）\n\n### 动作（网络的输出）\n\nAlphaStar的动作信息主要分为六个部分，分别为动作类型（Action type）、选中的单元（Selected units）、目标（Target）、执行动作的队列（Queued）、是否重复（Repeat）、延时（Delay），每一个部分间是有关联的。\n\n![img2](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.2.png)\n\n- 第一部分：动作类型，即下一次要进行的动作的类型是移动小兵、升级建筑还是移动小窗口的位置等等\n- 第二部分：选中的单元，即承接第一部分，例如我们要进行的动作类型是移动小兵，那么我们就应该选择具体“操作”哪一个小兵\n- 第三部分：目标，承接第二部分，我们操作小兵A后，是要去地图的某一个位置还是去攻击对手的哪一个目标等等，即选择目的地和攻击的对象\n- 第四部分：执行动作的队列，具体说是是否立即执行动作，对于小兵A，我们是到达目的地后直接进行攻击还是等待\n- 第五部分：是否重复做动作，如果需要小兵A持续攻击，那么就不需要再通过网络计算得到下一个的动作了，直接重复以上一个动作的相同的动作即可。\n- 第六部分：延时，也就是等候多久才接收网络的输入，可以理解为我们人类玩家的一个操作的延迟等等\n\n## AlphaStar的计算模型是什么呢？——网络结构\n\n上面我们说明了AlphaStar网络的输入和输出，即状态和动作，那么从状态怎么得到动作呢？其网络结构是怎么样的呢？\n\n![img3](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.3.png)\n\n### 输入部分\n\n![img4](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.4.png)\n\n从上图的红框可以看出，模型的输入框架中主要有三个部分，即Scalar features（标量特征），例如前面叙述的玩家的等级、小窗口的位置等等信息、Entities（实体），是向量即前面所叙述的一个建筑一个兵的当前的所有属性信息、Minimap（地图），即上面说的图像的数据。\n\n- 对于Scalar features（标量特征），使用多层感知器（MLP），就可以得到对应的向量，或者说是一个embedding的过程。\n- 对于Entities，使用NLP中常用的transformer作为encoder\n- 对于Minimap，使用图像中常用的Resnet作为encoder，得到一个定长的向量。\n\n### 中间过程\n\n中间过程比较简单，即通过一个deep LSTM进行融合三种当前状态下的embedding进行下一时刻的embedding输出，并且将该结果分别送入ValueNetwork、Residual MLP以及Actoin type的后续的MLP中。\n\n![img5](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.5.png)\n\n### 输出部分\n\n正如前面介绍的，输出的动作是前后有关联的，按照顺序\n\n![img6](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.6.png)\n\n- 首先是动作类型（Action type）：使用Deep LSTM的embedding的向量作为输入，使用residual MLP得到Action type的softmax的输出结果，并传给下一个子模型进行embedding。\n- 然后是延时（Delay）：使用上一个上面的embedding的结果以及Deep LSTM的结果一起输入MLP后得到结果，并传给下一个子模型进行embedding。\n- 接下来是执行动作的队列（Queued）：使用delay的结果以及embedding的结果一起输入MLP后得到结果，并传给下一个子模型进行embedding。\n- 然后是选中的单元（Selected units）：使用queued的结果、embedding的结果以及Entity encoder的全部结果（非平均的结果）一起送入到Pointer Network中得到结果，并传给下一个子模型进行embedding。这里的Pointer Netowrk为指针网络，即输入的是一个序列，输出是另外一个序列，并且，输出序列的元素来源于输入的序列，主要用于NLP中，在这里很适合与我们的Selected units的计算。\n- 接着是目标单元（Target unit）和目标区域（Target point）两者二选一进行，对于Target unit，使用attention机制得到最优的动作作用的一个对象，对于target point，使用反卷积神经网络，将embedding的向量，反卷积为map的大小，从而执行目标移动到某一点的对应的动作。\n\n## 庞大的AlphaStar如何训练呢？——学习算法\n\n对于上面复杂的模型，AlphaStar究竟如何来进行训练呢？总结下来一共分为4个部分，即监督学习（主要是解决训练的初始化问题）、强化学习、模仿学习（配合强化学习）以及多智能体学习和自学习（面向对战的具体问题），下面我们一一分析:\n\n### 监督学习\n\n在训练一开始首先使用监督学习利用人类的数据进行一个比较好的初始化。模型的输入是收集到的人类的对局的信息，输出是训练好的神经网络。具体的做法是，对于收集到了人类的对局数据，在对于每一个时刻解码游戏的状态，将每一时刻的状态送入网络中得到以上每一个动作的概率分布，最终计算模型的输出以及人类数据的KL Loss，并以此进行网络的优化，其中在KL Loss中需要使用不同的 Loss 函数，例如，Action类型的输出，即分类问题的loss就需要使用Cross Entropy。而对于target location等类似于回归问题的就需要计算MSE。当然还有一些细节，大家可以自行阅读paper。总之，经过监督学习，我们的模型输出的概率分布就可以与人类玩家输出的概率分布类似。\n\n### 强化学习\n\n这里的目标就是通过优化策略使得期望的reward最大，即\n$$\nJ(\\pi_{\\theta}) = \\Epsilon_{\\pi_{\\theta}} \\sum_{t=0}r(s_t,a_t)\n$$\n但AlphaStar的训练的模型使用不是采样的模型，即off-policy的模型，这是因为其使用的架构为类似于IMPALA的结构，即Actor负责与环境交互并采样，learner负责优化网络并更新参数，而Actor和learner通常是异步进行计算的，并且由于前面介绍的输出的动作的类型空间复杂，所以导致我们的value function的拟合比较困难。\n\n这里AlphaStar利用了以下的方式进行强化学习模型的构建：\n\n- 首先是采取了经典的Actor-critic的结构，使用策略网络给出当前状态下的智能体的动作，即计算$\\pi(a_t|s_t)$ ，使用价值网络计算当前状态下的智能体的期望收益，即计算 $V(s_t) = \\Epsilon \\sum_{t\'=t}r_{t\'} = \\Epsilon_{a_t}[r(s_t,a_t)+V(s_{t+1})]$。具体的计算方法是：\n  - 对于当前的状态 $s$ ，计算当前计算出的动作 $a$ 相对于“平均动作”所能额外获得的奖励。$A(s_t,a_t)=[r(s_t,a_t)+V(s_{t+1})]-V(s_t)$，即当前动作的预期收益减去当前状态的预期收益。在AlphaStar中，UPGO（Upgoing Policy Update）也得到了应用，即UPGO使用了一个迭代变量 $G_t$ 来取代原来的动作预期收益的 $r(s_t,a_t)+V(s_{t+1})$ ，即把未来乐观的信息纳入到我们额外奖励中，上式可改写为：\n\n$$\nA(s_t,a_t)=G_t-V(s_t)\n$$\n\n$$\nG_t=\\left\\{\n\\begin{aligned}\nr_t+G_{t+1} && Q(s_{s+1},a_{t+1})\\geq V(s_{t+1}) \\\\\nr_t+V(s_{t+1}) && otherwise \\\\\n\\end{aligned}\n\\right.\n$$\n\n- 在基于上面计算得到的action，更新策略梯度，即 $\\nabla_{\\theta}J = A(s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)$，在我们之前的笔记中也介绍了，如果基于 $\\pi_{\\theta}$ 的分布不好求解，或者说学习策略 $\\pi_{\\theta}$ 与采集策略 $\\pi_{\\mu}$ 不同的话，我们需要使用重要性采样的方法，即 $\\nabla_{\\theta}J = E_{\\pi_{\\mu}}\\frac{\\pi_{\\theta} (a_t|s_t)}{\\pi_{\\mu} (a_t|s_t)} A^{\\pi_{\\theta}}(s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)$。当然我们还需防止 $\\frac{\\pi_{\\theta} (a_t|s_t)}{\\pi_{\\mu} (a_t|s_t)}$ 出现无穷大的情况，我们需要使用V-trace限制重要性系数。这也是用于off-policy的一个更新方法，在 IMPALA 论文中的4.1小节有所体现。即将重要性系数的最大值限制为1，公式可表达如下：\n\n$$\n\\nabla_{\\theta}J = E_{\\pi_{\\mu}}\\rho_tA^{\\pi_{\\theta}}(s_t,a_t)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)\n$$\n\n$$\n\\rho_t = min(\\frac{\\pi_{\\theta} (a_t|s_t)}{\\pi_{\\mu} (a_t|s_t)},1)\n$$\n\n- 利用了TD($\\lambda$) 来优化价值网络，并同时输入对手的数据。对于我们的价值函数 $V^{\\pi_{\\theta}}(s_t)=E_{\\pi_{\\theta}}\\sum_{t\'=t}\\gamma^{t\'-t}r(s_t,a_t)=E_{a_t\\sim\\pi_{\\theta}(\\cdot|s_t)}[r(s_t,a_t)+\\gamma V(s_{t+1})]$，可以使用TD的方法计算MSE损失，有如下几种：\n  - $TD(0)$ ，表达式为 $L = [(r_t+\\gamma V_{t+1})-V_t]^2$ ，即当前step的信息，有偏小方差\n  - $TD(1)$也就是MC方法，表达式为 $L = [(\\sum_{t\'=t}^\\infty\\gamma^{t\'-t}r_{t\'})-V_t]^2$，即未来无穷个step的信息，无偏大方差\n  - $TD(\\lambda)$ ，以上两个方法的加权平均。即平衡当前step、下一个step到无穷个step后的结果\n    - 已知对于 $\\lambda \\in (0,1)$, $(1-\\lambda)+(1-\\lambda)\\lambda+(1-\\lambda)\\lambda ^2+...=1$\n    - $R_t = \\lim_{T\\rightarrow\\infty} (1-\\lambda)(r_t+V_{t+1})+(1-\\lambda)\\lambda(r_t+\\gamma r_{t+1}+\\gamma^2 V_{t+2})+...$\n\n### 模仿学习\n\n使用模仿学习额外引入了监督学习Loss以及人类的统计量 $Z$ ，即对于Build order（建造顺序）、Build Units（建造单元）、Upgrades（升级）、Effects（技能）等信息进行了奖励。对于统计量 $Z$ ，本质来说是一系列的数据，将其作为输入信息输入到策略网络和价值网络中。另外对于人类信息的利用还体现在前面介绍的使用监督学习进行网络的预训练工作。\n\n### 多智能体学习/自学习\n\n自学习在AlphaGo中得到了应用也就是自己和自己玩，Alpha对此做了一些更新，即有优先级的虚拟自学习策略，对于虚拟自学习就是在训练过程中，每一些时间就进行存档，并随机均匀的从存档中选出对手与正在训练的智能体对战。而有优先级的虚拟自学习指的是优先挑选能击败我的或者说常能打败智能体的对手进行训练对战，评判指标就是概率。对于AlphaStar中，其训练的agent分为了三种，\n\n- Main Agent （主智能体），即正在训练的智能体及其祖先；其中有50%的概率从联盟中的所有人中挑选，使用有优先级的虚拟自学习策略，即能打败我的概率高，不能打败我的概率低，有35%的概率与自己对战，有15%的概率与能打败我的联盟利用者或者老的主智能体对战，通过利用了有优先级的虚拟自学习策略。\n- League Exploiter（联盟利用者）：能打败联盟中的所有智能体的agent；其按照有优先级的虚拟自学习策略计算的概率与全联盟的对手训练，在以70%的胜率打败所有的agent或者距离上次存档 $2 \\times10^9$ step后就保存，并且在存档的时候，有25%概率把场上的联盟利用者的策略重设成监督学习给出的初始化。\n- Main Exploiter（主利用者）：能打败训练中的所有agent，在训练的过程中，随机从3个中挑1个主智能体，如果可以以高于10%的概率打败该agent就与其进行训练，如果不能就从其他的老主智能体中再挑选对手，当以70%的胜率打败全部三个正在学习的策略主智能体，或者距上次存档 $4 \\times10^9 $ 个step之后就存，并且进行重设初始化的操作。\n\n他们的区别在于：\n\n- 如何选取训练过程中对战的对象\n- 在什么情况下存档 (snapshot) 现在的策略\n- 以多大的概率将策略的参数重设为监督训练给出的初始化\n\n## AlphaStar实验结果如何呢？——实验结果\n\n### 宏观结果\n\n![img7](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.7.png)\n\n图A为训练后的agent与人类对战的结果（天梯图），具体地，刚刚结束监督学习后的AlphaStar可以达到钻石级别，而训练到一半（20天）以及训练完结（40天）的AlphaStar可以达到GM的级别。AlphaStar已经可以击败绝大多数的普通玩家。\n\n图B为不同种族间对战的胜率。\n\n图C为《星际争霸II》报告的每分钟有效行动分布情况（EPM），其中蓝色为AlphaStar Final的结果，红色为人类选手的结果虚线显示平均值。\n\n### 其他实验（消融实验）\n\nAlphaStar的论文中也使用了消融实验，即控制变量法，来进一步分析每一个约束条件对于对战结果的影响。下面举一个特别的例子：\n\n![img8](http://www.shadowingszy.top/images/datawhale-homepage-assets/easy-rl/img/13.8.png)\n\n上面的图片表示的是人类对局数据的使用的情况。可以看到如果没有人类对局数据的情况下，数值仅仅为149，但是只要经过了简单的监督学习，对应的数值就可以达到936，当然使用人类初始化后的强化学习可以达到更好的效果，利用强化学习加监督学习的KL Loss的话可以达到接近于完整的利用人类统计量 $Z$ 的效果。可以分析出，AlphaStar中人类对局的数据对于整个model的表现是很重要的，其并没有完全像AlphaGo一样，可以不使用人类数据的情况。\n\n## 关于AlphaStar的总结\n\n### 总结\n\n- AlphaStar设计了一个高度可融合图像、文本、标量等信息的神经网络架构，并且对于网络设计使用了Autoregressive解耦了结构化的action space。\n- 模仿学习和监督学习的内容，例如人类统计量 $Z$ 的计算方法\n- 复杂的DRL方法以及超复杂的训练策略\n- 当然了，大量的计算资源（Each agent was trained using 32 third-generation tensor processing units (TPUs 23 ) over 44 days. During league training almost 900 distinct players were created.）\n\n','2021-12-10 12:43:49','2021-12-19 19:43:13'),
	(62,2,'chapter1','## 我是测试\n测试测试测试','2021-12-10 12:43:49','2021-12-19 13:28:10'),
	(63,2,'chapter2','## 我是测试\n```\nconsole.log(\'hello world!\')\n```','2021-12-10 12:43:49','2021-12-19 19:59:05');

/*!40000 ALTER TABLE `learn_detail` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 learn_tag
# ------------------------------------------------------------

CREATE TABLE `learn_tag` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '标签id',
  `learn_id` int DEFAULT NULL COMMENT '学习id',
  `name` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '标签名称',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='学习标签表';

LOCK TABLES `learn_tag` WRITE;
/*!40000 ALTER TABLE `learn_tag` DISABLE KEYS */;

INSERT INTO `learn_tag` (`id`, `learn_id`, `name`, `create_time`, `modify_time`)
VALUES
	(1,1,'机器学习','2021-12-05 00:00:00','2021-12-19 12:31:03'),
	(2,1,'人工智能','2021-12-05 00:00:00','2021-12-19 12:31:06'),
	(3,2,'python','2021-12-05 00:00:00','2021-12-19 12:31:11'),
	(4,2,'pandas','2021-12-05 00:00:00','2021-12-19 12:31:16'),
	(5,3,'强化学习','2021-12-05 00:00:00','2021-12-19 12:31:19');

/*!40000 ALTER TABLE `learn_tag` ENABLE KEYS */;
UNLOCK TABLES;


# 转储表 learn_video
# ------------------------------------------------------------

CREATE TABLE `learn_video` (
  `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '学习视频id',
  `learn_id` int DEFAULT NULL COMMENT '学习id',
  `title` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT NULL COMMENT '视频标题',
  `video_url` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT '' COMMENT '视频url',
  `image_url` varchar(512) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci DEFAULT '' COMMENT '视频封面url',
  `create_time` timestamp NULL DEFAULT NULL COMMENT '创建时间',
  `modify_time` timestamp NULL DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='学习视频表';

LOCK TABLES `learn_video` WRITE;
/*!40000 ALTER TABLE `learn_video` DISABLE KEYS */;

INSERT INTO `learn_video` (`id`, `learn_id`, `title`, `video_url`, `image_url`, `create_time`, `modify_time`)
VALUES
	(1,1,'第0章-导学','https://www.bilibili.com/video/BV1Mh411e7VU?p=1','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:20'),
	(2,1,'第3章-一元线性回归','https://www.bilibili.com/video/BV1Mh411e7VU?p=2','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:24'),
	(3,1,'第3章-多元线性回归','https://www.bilibili.com/video/BV1Mh411e7VU?p=3','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:28'),
	(4,1,'第3章-对数几率回归','https://www.bilibili.com/video/BV1Mh411e7VU?p=4','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:32'),
	(5,1,'第3章-二分类线性判别分析','https://www.bilibili.com/video/BV1Mh411e7VU?p=5','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:37'),
	(6,1,'第4章-决策树','https://www.bilibili.com/video/BV1Mh411e7VU?p=6','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:41'),
	(7,1,'第5章-神经网络','https://www.bilibili.com/video/BV1Mh411e7VU?p=7','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:14:46'),
	(8,1,'第6章-支持向量机','https://www.bilibili.com/video/BV1Mh411e7VU?p=8','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(9,1,'第6章-软间隔与支持向量回归','https://www.bilibili.com/video/BV1Mh411e7VU?p=9','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(10,1,'第7章-贝叶斯分类器','https://www.bilibili.com/video/BV1Mh411e7VU?p=10','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(11,1,'第8章 - 集成学习（上）','https://www.bilibili.com/video/BV1Mh411e7VU?p=11','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(12,1,'第8章 -  集成学习（下）','https://www.bilibili.com/video/BV1Mh411e7VU?p=12','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(13,1,'第9章 - 聚类','https://www.bilibili.com/video/BV1Mh411e7VU?p=13','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(14,1,'第10章 - 降维和度量学习（上）','https://www.bilibili.com/video/BV1Mh411e7VU?p=14','http://www.shadowingszy.top/images/datawhale-homepage-assets/pumpkin-book-video.jpg','2021-12-10 12:43:49','2021-12-19 13:15:50'),
	(15,2,'ch1-预备知识','https://www.bilibili.com/video/BV1tK4y177AF?p=1','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(16,2,'ch2-pandas基础','https://www.bilibili.com/video/BV1tK4y177AF?p=6','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(17,2,'ch3-索引','https://www.bilibili.com/video/BV1tK4y177AF?p=14','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(18,2,'ch4-分组','https://www.bilibili.com/video/BV1tK4y177AF?p=22','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 20:37:45'),
	(19,2,'ch5-变形','https://www.bilibili.com/video/BV1tK4y177AF?p=26','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(21,2,'ch8-文本数据','https://www.bilibili.com/video/BV1tK4y177AF?p=32','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(22,2,'ch9-分类数据','https://www.bilibili.com/video/BV1tK4y177AF?p=36','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25'),
	(23,2,'ch10-时序数据','https://www.bilibili.com/video/BV1tK4y177AF?p=39','http://www.shadowingszy.top/images/datawhale-homepage-assets/joyful-pandas-video.jpg','2021-12-10 12:43:49','2021-12-19 13:16:25');

/*!40000 ALTER TABLE `learn_video` ENABLE KEYS */;
UNLOCK TABLES;



/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;
/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
